<!DOCTYPE html><html lang="en" data-mode="dark" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="n-step Bootstrapping" /><meta property="og:locale" content="en" /><meta name="description" content="이 포스트에서는 TD method의 확장된 형태인 $n$-step TD methods를 간략히 소개한다." /><meta property="og:description" content="이 포스트에서는 TD method의 확장된 형태인 $n$-step TD methods를 간략히 소개한다." /><link rel="canonical" href="https://devslem.github.io/reinforcement-learning/rl-fundamental/n-step-bootstrapping/" /><meta property="og:url" content="https://devslem.github.io/reinforcement-learning/rl-fundamental/n-step-bootstrapping/" /><meta property="og:site_name" content="DevSlem Blog" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-07-19T00:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="n-step Bootstrapping" /><meta name="google-site-verification" content="SSJwUK1JSPZV3wjQS19f2bXfHYIupbYEFsisH4Nuns4" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-07-21T00:00:00+09:00","datePublished":"2022-07-19T00:00:00+09:00","description":"이 포스트에서는 TD method의 확장된 형태인 $n$-step TD methods를 간략히 소개한다.","headline":"n-step Bootstrapping","mainEntityOfPage":{"@type":"WebPage","@id":"https://devslem.github.io/reinforcement-learning/rl-fundamental/n-step-bootstrapping/"},"url":"https://devslem.github.io/reinforcement-learning/rl-fundamental/n-step-bootstrapping/"}</script><title>n-step Bootstrapping | DevSlem Blog</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="DevSlem Blog"><meta name="application-name" content="DevSlem Blog"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> </a></div><div class="site-title mt-3"> <a href="/">DevSlem Blog</a></div><div class="site-subtitle font-italic">It's a blog where i write posts about AI, RL, Unity, etc.</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/DevSlem" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['devslem12','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper"><div id="topbar" class="container d-flex align-items-center justify-content-between h-100 pl-3 pr-3 pl-md-4 pr-md-4"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>n-step Bootstrapping</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper" class="d-flex justify-content-center"><div id="main" class="container pl-xl-4 pr-xl-4"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pr-xl-4"><div class="post pl-1 pr-1 pl-md-2 pr-md-2"><h1 data-toc-skip>n-step Bootstrapping</h1><div class="post-meta text-muted"> <span> Posted <em class="" data-ts="1658156400" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Jul 19, 2022 </em> </span> <span> Updated <em class="" data-ts="1658361600" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Jul 21, 2022 </em> </span><div class="d-flex justify-content-between"> <span> By <em> <a href="https://github.com/username">DevSlem</a> </em> </span><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="5051 words"> <em>28 min</em> read</span></div></div></div><div class="post-content"><p>이 포스트에서는 TD method의 확장된 형태인 $n$-step TD methods를 간략히 소개한다.</p><h2 id="what-is-n-step-td-method"><span class="mr-2">What is $n$-step TD method</span><a href="#what-is-n-step-td-method" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p><strong><em>$n$-step TD method</em>는 1-step TD method와 Monte Carlo (MC) method를 통합한 방법</strong>이다. $n$-step TD method는 일종의 스펙트럼으로 양 끝단에 각각 1-step TD와 MC method가 존재한다. 실제 1-step TD나 MC method가 항상 좋은 performance를 내는건 아니다. 따라서 보다 일반적인 n-step TD method에 대해 알아둘 필요가 있다.</p><p>이전 포스트에서 TD method는 MC method와 Dynamic Programming (DP)의 아이디어를 결합한 방법이라고 소개했었다. $n$-step method 역시 동일하다. 즉, <strong>sampling과 bootstrapping을 통해 training</strong>이 이루어진다. 다만 1-step TD와의 차이점은 bootstrapping이 이루어지는 time step이 1개가 아니라 여러 개일 뿐이다.</p><h2 id="n-step-td-prediction"><span class="mr-2">$n$-step TD Prediction</span><a href="#n-step-td-prediction" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>MC method는 episode를 완전히 고려하기 때문에 전체 reward를 알고 있어 실제 return을 구할 수 있었다.</p>\[G_t \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots + \gamma^{T-t-1}R_T\]<p>$T$는 episode의 last time step이다. 그런데 1-step TD method는 episode 전체가 아니라 sampling된 단 하나의 transition 고려하기 때문에 next reward $R_{t+1}$만 구할 수 있다. 따라서 아직 sampling 되지 않은 time step들의 discounted reward들을 아래와 같이 <strong>next state에서의 추정치로 근사해 처리</strong>한다.</p>\[G_{t:t+1} \doteq R_{t+1} + \gamma V_t(S_{t+1})\]<p>$G_{t:t+1}$은 time step $t$에서 $t+1$까지의 transition이 발생했을 때의 target으로 1-step return이라고 한다. 참고로 MC method의 target은 실제 return $G_t$이다. 우리는 위 1-step TD method의 target을 $n$-step으로 확장할 것이다. 먼저 아래 backup diagram을 보자.</p><p><img data-src="/assets/images/rl-sutton-n-step-method-backup-diagram.png" alt="" width="60%" data-proofer-ignore> <em>Fig 1. Backup diagrams of $n$-step methods.<br /> (Image source: Sec 7.1 Sutton &amp; Barto (2018).)</em></p><p>위 backup diagram을 보면 알 수 있지만 1-step TD 수행 시 실제 reward는 $R_{t+1}$만 획득할 수 있다. 2-step TD 수행 시 $R_{t+1}$과 $R_{t+2}$만 획득할 수 있다. <strong>획득하지 못한 나머지 reward는 기존에 학습된 추정치로 대체</strong>한다. 즉, 2-step TD 수행 시 $S_{t+2}$에 위치할 것이기 때문에 $V(S_{t+2})$를 사용한다. 아래는 2-step TD method의 target이다.</p>\[G_{t:t+2} \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 V_{t+1}(S_{t+2})\]<p>이를 $n$-step TD method로 일반화하면 아래와 같다.</p>\[G_{t:t+n} \doteq R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1} R_{t+n} + \gamma^n V_{t+n-1}(S_{t+n})\]<p>위 $n$-step TD method의 target을 <em>$n$-step return</em>이라고 한다. 정리하면 $n$-step return은 실제 return의 근사치로, $n$-step 이후 잘려진 뒤 $V_{t+n-1}(S_{t+n})$에 의해 보정된 값이다. 이 때 $t + n \geq T$일 경우, episode의 termination을 넘어갔기 때문에 보정된 값은 0이 되어 실제 return이 된다.</p>\[G_{t:t+n} \doteq G_t, \quad \text{if } t + n \geq T\]<p>$n$-step return은 $n$-step 이후의 $R_{t+n}$과 이전에 계산된 $V_{t+n-1}$이 발견이 되어야만 계산할 수 있다. 따라서 $t+n$ time step 이후에만 $G_{t:t+n}$을 이용할 수 있다. 아래는 $n$-step return을 사용해 state value를 추정하는 prediction update rule이다.</p>\[V_{t+n}(S_t) \doteq V_{t+n-1}(S_t) + \alpha [G_{t:t+n} - V_{t+n-1}(S_t)]\]<p>이 때 $s \neq S_t$인 모든 state의 value는 변하지 않는다. 위 update rule이 <em>$n$-step TD</em>이다. episode의 첫 $n-1$ step까지는 어떤 변화도 발생하지 않는다는 사실을 꼭 기억하길 바란다.</p><blockquote><h5 id="textalgorithm-n-step-td-for-estimating--v-approx-v_pi"><span class="mr-2">$\text{Algorithm: $n$-step TD for estimating } V \approx v_\pi$</span><a href="#textalgorithm-n-step-td-for-estimating--v-approx-v_pi" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5><p>\(\begin{align*} &amp; \textstyle \text{Input: a policy } \pi \\ &amp; \textstyle \text{Algorithm parameters: step size } \alpha \in (0, 1] \text{, a positive integer } n \\ &amp; \textstyle \text{Initialize } V(s) \text{ arbitrarily, for all } s \in \mathcal{S} \\ &amp; \textstyle \text{All store and access operations (for } S_t \text{ and } R_t \text{) can take their index mod } n+1 \\ \\ &amp; \textstyle \text{Loop for each episode:} \\ &amp; \textstyle \qquad \text{Initialize and store } S_0 \neq \text{terminal} \\ &amp; \textstyle \qquad T \leftarrow \infty \\ &amp; \textstyle \qquad \text{Loop for } t = 0, 1, 2, \dotso : \\ &amp; \textstyle \qquad\qquad \text{If } t &lt; T \text{, then:} \\ &amp; \textstyle \qquad\qquad\qquad \text{Take an action according to } \pi(\cdot \vert S_t) \\ &amp; \textstyle \qquad\qquad\qquad \text{Observe and store the next reward as } R_{t+1} \text{ and the next state as } S_{t+1} \\ &amp; \textstyle \qquad\qquad\qquad \text{If } S_{t+1} \text{ is terminal, then } T \leftarrow t + 1 \\ &amp; \textstyle \qquad\qquad \tau \leftarrow t - n + 1 \qquad \text{($\tau$ is the time whose state's estimate is being updated)} \\ &amp; \textstyle \qquad\qquad \text{If $\tau \geq 0$:} \\ &amp; \textstyle \qquad\qquad\qquad G \leftarrow \sum_{i=\tau + 1}^{\min(\tau + n, T)} \gamma^{i - \tau - 1}R_i \\ &amp; \textstyle \qquad\qquad\qquad \text{If } \tau + n &lt; T \text{, then: } G \leftarrow G + \gamma^n V(S_{\tau + n}) \qquad (G_{\tau : \tau + n}) \\ &amp; \textstyle \qquad\qquad\qquad V(S_\tau) \leftarrow V(S_\tau) + \alpha [G - V(S_\tau)] \\ &amp; \textstyle \qquad \text{until } \tau = T - 1 \end{align*}\)</p></blockquote><p>위 알고리즘을 보면 $n$-step transition이 발생할 때는 $t = n-1$일 때로 이 때 부터 value function의 update가 수행된다.</p><h2 id="n-step-sarsa"><span class="mr-2">$n$-step Sarsa</span><a href="#n-step-sarsa" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>이제 $n$-step method의 control에 대해 알아보자. 가장 먼저 알아볼 것은 <em>n-step Sarsa</em>이다. Sarsa는 on-policy method이며 여기서는 단지 1-step이 아닌 $n$-step으로 확장했을 뿐이다.</p><p>핵심 아이디어는 MC나 1-step TD method와 동일하게 <strong>state value 추정을 action value 추정으로 전환</strong>하는 것이다. 이에 따라 모든 시작과 끝은 state가 아니라 action이 된다. 아래는 $n$-step Sarsa에 대한 backup diagram이다.</p><p><img data-src="/assets/images/rl-sutton-n-step-sarsa-backup-diagram.png" alt="" width="70%" data-proofer-ignore> <em>Fig 2. Backup diagrams of $n$-step Sarsa.<br /> (Image source: Sec 7.2 Sutton &amp; Barto (2018).)</em></p><p>이제 $n$-step Sarsa에서 사용하기 위한 $n$-step return을 action value에 관해 정의해보자. 단지, <a href="#n-step-td-prediction">$n$-step TD Prediction</a>의 $n$-step return에서 $V$를 $Q$로 바꿔주기만 하면 된다.</p>\[G_{t:t+n} \doteq R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1} R_{t+n} + \gamma^n Q_{t+n-1}(S_{t+n}, A_{t+n}), \quad n \geq 1, 0 \leq t &lt; T - n\]<p>단, $t + n \geq T$일 때 $G_{t:t+n} \doteq G_t$이다. 위 $n$-step return for Sarsa를 바탕으로 $n$-step Sarsa의 update rule을 정의해보자.</p>\[Q_{t+n}(S_t,A_t) \doteq Q_{t+n-1}(S_t,A_t) + \alpha [G_{t:t+n} - Q_{t+n-1}(S_t, A_t)], \quad 0 \leq t &lt; T\]<p>역시 마찬가지로 $s \neq S_t$ or $a \neq A_t$인 모든 $s, a$에 대한 value function은 변하지 않는다.</p><p>아래는 1-step Sarsa와 $n$-step Sarsa를 비교하는 그림이다. 파란색 화살표는 목표 G에 도달했을 떄 증가하는 action value를 나타낸다.</p><p><img data-src="/assets/images/rl-sutton-n-step-sarsa-example.png" alt="" data-proofer-ignore> <em>Fig 3. Comparison of 1-step and $n$-step Sarsa.<br /> (Image source: Sec 7.2 Sutton &amp; Barto (2018).)</em></p><p>위 그림을 보면 알 수 있지만 목표 G에 도달했을 때 1-step Sarsa는 바로 직전 state에서의 action value만 증가하지만 $n$-step Sarsa는 sequence의 마지막 $n$개의 action만큼 증가한다. 이로 인해 하나의 episode로부터 더 많은 학습이 가능해진다.</p><p>아래는 $n$-step Sarsa의 알고리즘이다.</p><blockquote><h5 id="textalgorithm-n-step-sarsa-for-estimating--q-approx-q_ast-text-or--q_pi"><span class="mr-2">$\text{Algorithm: $n$-step Sarsa for estimating } Q \approx q_\ast \text{ or } q_\pi$</span><a href="#textalgorithm-n-step-sarsa-for-estimating--q-approx-q_ast-text-or--q_pi" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5><p>\(\begin{align*} &amp; \textstyle \text{Initialize } Q(s,a) \text{ arbitrarily, for all } s \in \mathcal{S}, a \in \mathcal{A} \\ &amp; \textstyle \text{Initialize $\pi$ to be $\epsilon$-greedy with respect to $Q$, or to a fixed given policy} \\ &amp; \textstyle \text{Algorithm parameters: step size } \alpha \in (0, 1] \text{, small $\epsilon &gt; 0$, a positive integer $n$} \\ &amp; \textstyle \text{All store and access operations (for $S_t$, $A_t$, and $R_t$) can take their index mod $n+1$} \\ \\ &amp; \textstyle \text{Loop for each episode}: \\ &amp; \textstyle \qquad \text{Initialize and store } S_0 \neq \text{terminal} \\ &amp; \textstyle \qquad \text{Select and store an action } A_0 \sim \pi(\cdot \vert S_0) \\ &amp; \textstyle \qquad T \leftarrow \infty \\ &amp; \textstyle \qquad \text{Loop for } t = 0, 1, 2, \dotso \text{:} \\ &amp; \textstyle \qquad\vert\qquad \text{If $t &lt; T$, then:} \\ &amp; \textstyle \qquad\vert\qquad\qquad \text{Take action } A_t \\ &amp; \textstyle \qquad\vert\qquad\qquad \text{Observe and store the next reward as $R_{t+1}$ and the next state as $S_{t+1}$} \\ &amp; \textstyle \qquad\vert\qquad\qquad \text{If $S_{t+1}$ is terminal, then:} \\ &amp; \textstyle \qquad\vert\qquad\qquad\qquad T \leftarrow t + 1 \\ &amp; \textstyle \qquad\vert\qquad\qquad \text{else:} \\ &amp; \textstyle \qquad\vert\qquad\qquad\qquad \text{Select and store an action } A_{t+1} \sim \pi(\cdot \vert S_{t+1}) \\ &amp; \textstyle \qquad\vert\qquad \tau \leftarrow t - n + 1 \qquad (\tau \text{ is the time whose estimate is being updated}) \\ &amp; \textstyle \qquad\vert\qquad \text{If $\tau \geq 0$:} \\ &amp; \textstyle \qquad\vert\qquad\qquad G \leftarrow \sum_{i = \tau + 1}^{\min(\tau + n, T)} \gamma^{i - \tau - 1}R_i \\ &amp; \textstyle \qquad\vert\qquad\qquad \text{If } \tau + n &lt; T \text{, then } G \leftarrow G + \gamma^n Q(S_{\tau + n}, A_{\tau + n}) \qquad (G_{\tau : \tau + n}) \\ &amp; \textstyle \qquad\vert\qquad\qquad Q(S_\tau, A_\tau) \leftarrow Q(S_\tau, A_\tau) + \alpha [G - Q(S_\tau, A_\tau)] \\ &amp; \textstyle \qquad\vert\qquad\qquad \text{If $\pi$ is being learned, then ensure that $\pi(\cdot \vert S_\tau)$ is $\epsilon$-greedy wrt $Q$} \\ &amp; \textstyle \qquad \text{until } \tau = T - 1 \\ \end{align*}\)</p></blockquote><h2 id="n-step-expected-sarsa"><span class="mr-2">$n$-step Expected Sarsa</span><a href="#n-step-expected-sarsa" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>$n$-step Expected Sarsa 역시 어렵지 않게 구할 수 있다.. Fig.2를 보면 알 수 있지만 Sarsa와 동일하게 진행되나 마지막 $n$-step에서 모든 action value에 대한 expected value를 고려한다. 아래는 임의의 state $s$에서 action value 추정치를 통해 획득할 수 있는 <em>expected approximate value</em>이다.</p>\[\bar{V}_t(s) \doteq \sum_a \pi(a \vert s)Q_t(s,a), \quad \text{for all } s \in \mathcal{S}\]<p>이 때 $\pi$는 target policy이다. 위 수식을 바탕으로 Expected Sarsa에 대한 $n$-step return을 구할 수 있다.</p>\[G_{t:t+n} \doteq R_{t+1} + \cdots + \gamma^{n-1}R_{t+n} + \gamma^n \bar{V}_{t+n-1}(S_{t+n}), \quad t + n &lt; T\]<p>위 수식을 바탕으로 $n$-step Sarsa 알고리즘을 약간만 변형하면 $n$-step Expected Sarsa 알고리즘을 쉽게 구할 수 있다.</p><h2 id="n-step-off-policy-learning"><span class="mr-2">$n$-step Off-policy Learning</span><a href="#n-step-off-policy-learning" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>off-policy learning은 <strong>behavior policy $b$에 따라 sampling 한 뒤, target policy $\pi$에 대한 value function을 학습</strong> 하는 방법이다. off-policy method에서 behavior policy와 target policy의 distribution이 다르기 때문에 importance-sampling 기법을 사용했었다. importance-sampling ratio는 <strong>어떤 trajectory의 action들을 두 policy에 따라 선택할 상대적 확률</strong>이다.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></p>\[\rho_{t:h} \doteq \prod_{k=t}^{\min(h, T-1)} \dfrac{\pi(A_k \vert S_k)}{b(A_k \vert S_k)}\]<p>아래는 $S_t$가 주어져있을 때 $n$-step transition으로 생성된 trajectory이다.</p>\[A_t, S_{t+1}, A_{t+1}, \dots, A_{t+n-1}, S_{t+n}\]<p>이 때 off-policy에서는 아래와 같이 <em>importance-sampling ratio</em> $\rho_{t:t+n-1}$을 통해 $n$-step TD error에 가중치를 부여하여 update를 수행할 수 있다.</p>\[V_{t+n}(S_t) \doteq V_{t+n-1}(S_t) + \alpha \rho_{t:t+n-1}[G_{t:t+n} - V_{t+n-1}(S_t)], \quad 0 \leq t &lt; T\]<p>이를 바탕으로 off-policy $n$-step Sarsa를 정의할 수 있다. 마찬가지로 state value를 action value로 전환하면 된다. 다만 importance-sampling ratio을 고려할 때 주의할 점이 있다. <strong>state value 추정과 action value 추정 시의 trajectory가 달라</strong> importance-sampling ratio가 다르다. action value를 추정할 때는 state action pair $S_t, A_t$가 주어져 있으며, Sarsa이기 때문에 trajectory의 마지막 state에서도 action을 선택한다. 아래는 Sarsa의 $n$-step transition trajectory이다.</p>\[S_{t+1}, A_{t+1}, \dots, A_{t+n-1}, S_{t+n}, A_{t+n}\]<p>따라서 우리가 고려해야할 importance-sampling ratio는 $\rho_{t+1:t+n}$이다. 아래는 off-policy Sarsa의 update rule이다.</p>\[Q_{t+n}(S_t, A_t) \doteq Q_{t+n-1}(S_t, A_t) + \alpha \rho_{t+1:t+n}[G_{t:t+n} - Q_{t+n-1}(S_t, A_t)], \quad 0 \leq t &lt; T\]<p><strong>importance-sampling ratio는 subsequent action에 대해서만 고려</strong>한다는 사실을 꼭 기억하길 바란다. 아래는 off-policy $n$-step Sarsa 알고리즘이다.</p><blockquote><h5 id="textalgorithm-off-policy-n-step-sarsa-for-estimating--q-approx-q_ast-text-or--q_pi"><span class="mr-2">$\text{Algorithm: Off-policy $n$-step Sarsa for estimating } Q \approx q_\ast \text{ or } q_\pi$</span><a href="#textalgorithm-off-policy-n-step-sarsa-for-estimating--q-approx-q_ast-text-or--q_pi" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5><p>\(\begin{align*} &amp; \textstyle \text{Input: an arbitrary behavior policy $b$ such that $b(a \vert s) &gt; 0$, for all } s \in \mathcal{S}, a \in \mathcal{A} \\ &amp; \textstyle \text{Initialize } Q(s,a) \text{ arbitrarily, for all } s \in \mathcal{S}, a \in \mathcal{A} \\ &amp; \textstyle \text{Initialize $\pi$ to be greedy with respect to $Q$, or as a fixed given policy} \\ &amp; \textstyle \text{Algorithm parameters: step size } \alpha \in (0, 1] \text{, a positive integer $n$} \\ &amp; \textstyle \text{All store and access operations (for $S_t$, $A_t$, and $R_t$) can take their index mod $n+1$} \\ \\ &amp; \textstyle \text{Loop for each episode}: \\ &amp; \textstyle \qquad \text{Initialize and store } S_0 \neq \text{terminal} \\ &amp; \textstyle \qquad \text{Select and store an action } A_0 \sim b(\cdot \vert S_0) \\ &amp; \textstyle \qquad T \leftarrow \infty \\ &amp; \textstyle \qquad \text{Loop for } t = 0, 1, 2, \dotso \text{:} \\ &amp; \textstyle \qquad\vert\qquad \text{If $t &lt; T$, then:} \\ &amp; \textstyle \qquad\vert\qquad\qquad \text{Take action } A_t \\ &amp; \textstyle \qquad\vert\qquad\qquad \text{Observe and store the next reward as $R_{t+1}$ and the next state as $S_{t+1}$} \\ &amp; \textstyle \qquad\vert\qquad\qquad \text{If $S_{t+1}$ is terminal, then:} \\ &amp; \textstyle \qquad\vert\qquad\qquad\qquad T \leftarrow t + 1 \\ &amp; \textstyle \qquad\vert\qquad\qquad \text{else:} \\ &amp; \textstyle \qquad\vert\qquad\qquad\qquad \text{Select and store an action } A_{t+1} \sim b(\cdot \vert S_{t+1}) \\ &amp; \textstyle \qquad\vert\qquad \tau \leftarrow t - n + 1 \qquad (\tau \text{ is the time whose estimate is being updated}) \\ &amp; \textstyle \qquad\vert\qquad \text{If $\tau \geq 0$:} \\ &amp; \textstyle \qquad\vert\qquad\qquad \rho \leftarrow \prod_{i = \tau + 1}^{\min(\tau + n, T-1)} \frac{\pi(A_i \vert S_i)}{b(A_i \vert S_i)} \qquad (\rho_{\tau + 1:\tau + n}) \\ &amp; \textstyle \qquad\vert\qquad\qquad G \leftarrow \sum_{i = \tau + 1}^{\min(\tau + n, T)} \gamma^{i - \tau - 1}R_i \\ &amp; \textstyle \qquad\vert\qquad\qquad \text{If } \tau + n &lt; T \text{, then: } G \leftarrow G + \gamma^n Q(S_{\tau + n}, A_{\tau + n}) \qquad (G_{\tau : \tau + n}) \\ &amp; \textstyle \qquad\vert\qquad\qquad Q(S_\tau, A_\tau) \leftarrow Q(S_\tau, A_\tau) + \alpha \rho [G - Q(S_\tau, A_\tau)] \\ &amp; \textstyle \qquad\vert\qquad\qquad \text{If $\pi$ is being learned, then ensure that $\pi(\cdot \vert S_\tau)$ is greedy wrt $Q$} \\ &amp; \textstyle \qquad \text{until } \tau = T - 1 \\ \end{align*}\)</p></blockquote><blockquote class="prompt-info"><div><p>참고로 위 알고리즘은 Reinforcement: An Introduction (2020)에 소개된 알고리즘이다. 2018에 있는 알고리즘과 살짝 다른데 2020 버전이 올바른 알고리즘으로 보인다.</p></div></blockquote><p>off-policy Expected Sarsa의 경우 위 알고리즘에서 importance-sampling ratio $\rho_{t+1:t+n}$ 대신에 $\rho_{t+1:t+n-1}$을 사용하고 $n$-step return Expected Sarsa version을 사용하면 된다. importance-sampling ratio에서 마지막 action을 고려하지 않는 이유는 아래와 같다.</p><blockquote><p>This is because in Expected Sarsa all possible actions are taken into account in the last state; the one actually taken has no e↵ect and does not have to be corrected for.<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup></p></blockquote><h2 id="n-step-tree-backup"><span class="mr-2">$n$-step Tree Backup</span><a href="#n-step-tree-backup" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>importance sampling은 off-policy learning을 가능하게 하지만 <strong>분산이 커질 수 있다는 단점</strong>이 있다. importance sampling을 사용하지 않고 off-policy learning을 가능하게 해주는 방법이 있는데 바로 <em>tree-backup algorithm</em>이다. 아래는 3개의 sample transition과 2개의 sample action을 나타낸 backup diagram이다. 각 state node의 사이드에 달려 있는 action node들은 sampling 시에 선택되지 않은 action을 나타낸다.</p><p><img data-src="/assets/images/rl-sutton-3-step-tree-backup-diagram.png" alt="" width="15%" data-proofer-ignore> <em>Fig 4. 3-step tree-backup diagram.<br /> (Image source: Sec 7.5 Sutton &amp; Barto (2018).)</em></p><p>지금까지 diagram에서 top node의 action value를 추정할 때 아래 node를 쭉 따라 획득한 discounted reward들과 가장 아래 node의 value를 결합한 값을 사용했었다. tree-backup algorithm은 이러한 것들에 추가적으로 위 backup diagram에서 각 state들의 사이드에 달려 있는 action node에 해당하는 value 추정치 또한 고려한다.</p><p>좀 더 구체적으로 보자. 먼저, 각 sample transition에서 선택되지 않은 action $a \neq A_{t+1}$들은 target policy $\pi$에 의해 추정된 action value에 가중치 $\pi(a \vert S_{t+1})$을 부여한다. 실제로 선택된 action $A_{t+1}$은 다음 단계의 모든 action에 $\pi(A_{t+1} \vert S_{t+1})$을 가중치로 부여한다. 이는 재귀적으로 수행된다.</p><p>이제 구체적인 $n$-step tree-backup algorithm에 대한 수식을 보자. 먼저 1-step return은 아래와 같이 정의된다. 이 때 1-step return은 Expected Sarsa의 return과 동일하다.</p>\[G_{t:t+1} \doteq R_{t+1} + \gamma \sum_a \pi(a \vert S_{t+1})Q_t(S_{t+1}, a), \quad t &lt; T-1\]<p>2-step tree-backup return은 아래와 같이 정의된다.</p>\[\begin{align*} G_{t:t+2} &amp;\doteq R_{t+1} + \gamma \sum_{a \neq A_{t+1}} \pi(a \vert S_{t+1})Q_{t+1}(S_{t+1}, a) \\ &amp;+ \gamma \pi (A_{t+1} \vert S_{t+1})\Big(R_{t+2} + \gamma \sum_a \pi(a \vert S_{t+2})Q_{t+1}(S_{t+2},a) \Big) \\ &amp;= R_{t+1} + \gamma \sum_{a \neq A_{t+1}} \pi(a \vert S_{t+1})Q_{t+1}(S_{t+1},a) + \gamma \pi(A_{t+1} \vert S_{t+1})G_{t+1:t+2}, \quad t &lt; T - 2 \end{align*}\]<p>$n$-step tree-backup return (target)은 아래와 같이 정의된다. 단지 위 수식을 $n$-step에 대한 재귀적 형태로 바꿔주면 된다.</p>\[G_{t:t+n} \doteq R_{t+1} + \gamma \sum_{a \neq A_{t+1}} \pi(a \vert S_{t+1})Q_{t+n-1}(S_{t+1}, a) + \gamma \pi(A_{t+1} \vert S_{t+1})G_{t+1:t+n}, \quad t &lt; T - 1, n \geq 2\]<p>참고로 위 수식에서 $t &lt; T-1$인 이유는 $G_{t+1:t+n}$에서 sub-sequence가 처리되기 때문이다. $n=1$인 경우는 $G_{t:t+1}$을 사용하면 된다. 또한 $G_{T-1:t+n} \doteq R_T$로 정의된다. 위에서 정의한 target을 일반적인 $n$-step Sarsa update rule에 적용하면 아래와 같다.</p>\[Q_{t+n}(S_t,A_t) \doteq Q_{t+n-1}(S_t,A_t) + \alpha [G_{t:t+n} - Q_{t+n-1}(S_t,A_t)], \quad 0 \leq t &lt; T\]<p>참고로 위 수식은 on-policy $n$-step Sarsa와 동일한 수식이지만 $n$-step return을 정의하는 방식이 다르다. 지금 보이는 수식은 $n$-step tree-backup return을 사용한 $n$-step tree-backup Sarsa로 off-policy learning이다. 아래는 이에 대한 알고리즘이다.</p><blockquote><h5 id="textalgorithm-n-step-tree-backup-for-estimating--q-approx-q_ast-text-or--q_pi"><span class="mr-2">$\text{Algorithm: $n$-step Tree Backup for estimating } Q \approx q_\ast \text{ or } q_\pi$</span><a href="#textalgorithm-n-step-tree-backup-for-estimating--q-approx-q_ast-text-or--q_pi" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5><p>\(\begin{align*} &amp; \textstyle \text{Initialize } Q(s,a) \text{ arbitrarily, for all } s \in \mathcal{S}, a \in \mathcal{A} \\ &amp; \textstyle \text{Initialize $\pi$ to be greedy with respect to $Q$, or as a fixed given policy} \\ &amp; \textstyle \text{Algorithm parameters: step size } \alpha \in (0, 1] \text{, a positive integer $n$} \\ &amp; \textstyle \text{All store and access operations (for $S_t$, $A_t$, and $R_t$) can take their index mod $n+1$} \\ \\ &amp; \textstyle \text{Loop for each episode}: \\ &amp; \textstyle \qquad \text{Initialize and store } S_0 \neq \text{terminal} \\ &amp; \textstyle \qquad \text{Choose an action $A_0$ arbitrarily as a function of $S_0$; Store $A_0$} \\ &amp; \textstyle \qquad T \leftarrow \infty \\ &amp; \textstyle \qquad \text{Loop for } t = 0, 1, 2, \dotso \text{:} \\ &amp; \textstyle \qquad\vert\qquad \text{If $t &lt; T$:} \\ &amp; \textstyle \qquad\vert\qquad\qquad \text{Take action } A_t \\ &amp; \textstyle \qquad\vert\qquad\qquad \text{Observe and store the next reward as $R_{t+1}$ and the next state as $S_{t+1}$} \\ &amp; \textstyle \qquad\vert\qquad\qquad \text{If $S_{t+1}$ is terminal:} \\ &amp; \textstyle \qquad\vert\qquad\qquad\qquad T \leftarrow t + 1 \\ &amp; \textstyle \qquad\vert\qquad\qquad \text{else:} \\ &amp; \textstyle \qquad\vert\qquad\qquad\qquad \text{Choose an action $A_{t+1}$ arbitrarily as a function of $S_{t+1}$; Store $A_{t+1}$} \\ &amp; \textstyle \qquad\vert\qquad \tau \leftarrow t - n + 1 \qquad (\tau \text{ is the time whose estimate is being updated}) \\ &amp; \textstyle \qquad\vert\qquad \text{If $\tau \geq 0$:} \\ &amp; \textstyle \qquad\vert\qquad\qquad \text{If } t + 1 \geq T \text{:} \\ &amp; \textstyle \qquad\vert\qquad\qquad\qquad G \leftarrow R_T \\ &amp; \textstyle \qquad\vert\qquad\qquad \text{else:} \\ &amp; \textstyle \qquad\vert\qquad\qquad\qquad G \leftarrow R_{t+1} + \gamma \sum_a \pi(a \vert S_{t+1})Q(S_{t+1},a) \\ &amp; \textstyle \qquad\vert\qquad\qquad \text{Loop for } k = \min(t,T-1) \text{ down through $\tau + 1$:} \\ &amp; \textstyle \qquad\vert\qquad\qquad\qquad G \leftarrow R_k + \gamma \sum_{a \neq A_k} \pi(a \vert S_k)Q(S_k,a) + \gamma \pi(A_k \vert S_k)G \\ &amp; \textstyle \qquad\vert\qquad\qquad Q(S_\tau, A_\tau) \leftarrow Q(S_\tau, A_\tau) + \alpha [G - Q(S_\tau, A_\tau)] \\ &amp; \textstyle \qquad\vert\qquad\qquad \text{If $\pi$ is being learned, then ensure that $\pi(\cdot \vert S_\tau)$ is greedy wrt $Q$} \\ &amp; \textstyle \qquad \text{until } \tau = T - 1 \\ \end{align*}\)</p></blockquote><h2 id="a-unifying-algorithm-n-step-qsigma"><span class="mr-2">A Unifying Algorithm: $n$-step $Q(\sigma)$</span><a href="#a-unifying-algorithm-n-step-qsigma" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>지금까지 아래 3개의 backup diagram이 나타내는 방법을 알아봤었다. 4번째 알고리즘 $n$-step $Q(\sigma)$는 이러한 알고리즘들을 통합한 방법이다.</p><p><img data-src="/assets/images/rl-sutton-unified-n-step-backup-diagram.png" alt="" width="65%" data-proofer-ignore> <em>Fig 5. Backup diagram of $n$-step $Q(\sigma)$.<br /> (Image source: Sec 7.6 Sutton &amp; Barto (2018).)</em></p><p>$n$-step $Q(\sigma)$ 알고리즘은 $\sigma = 1$일 떄는 sampling을, $\sigma = 0$일 때는 expectation을 나타낸다. 아래는 $n$-step $Q(\sigma)$의 $n$-step return에 대한 정의이다.</p>\[G_{t:h} \doteq R_{t+1} + \gamma \Big(\sigma_{t+1}\rho_{t+1} + (1 - \sigma_{t+1})\pi(A_{t+1} \vert S_{t+1})\Big)\Big(G_{t+1:h} - Q_{h-1}(S_{t+1},A_{t+1})\Big) + \gamma \bar{V}_{h-1}(S_{t+1}), \quad t &lt; h \leq T\]<p>이 때 $h = t + n$이다. 재귀호출은 $G_{h:h} \doteq Q_{h-1}(S_h,A_h) \text{ if } h &lt; T$이거나 $G_{T-1:T} \doteq R_T \text{ if } h = T$일 때 끝난다. 알고리즘은 아래와 같다.</p><blockquote><h5 id="textalgorithm-off-policy-n-step-qsigma-for-estimating--q-approx-q_ast-text-or--q_pi"><span class="mr-2">$\text{Algorithm: Off-policy $n$-step $Q(\sigma)$ for estimating } Q \approx q_\ast \text{ or } q_\pi$</span><a href="#textalgorithm-off-policy-n-step-qsigma-for-estimating--q-approx-q_ast-text-or--q_pi" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5><p>\(\begin{align*} &amp; \textstyle \text{Input: an arbitrary behavior policy $b$ such that $b(a \vert s) &gt; 0$, for all } s \in \mathcal{S}, a \in \mathcal{A} \\ &amp; \textstyle \text{Initialize } Q(s,a) \text{ arbitrarily, for all } s \in \mathcal{S}, a \in \mathcal{A} \\ &amp; \textstyle \text{Initialize $\pi$ to be greedy with respect to $Q$, or else it is a fixed given policy} \\ &amp; \textstyle \text{Algorithm parameters: step size } \alpha \in (0, 1] \text{, a positive integer $n$} \\ &amp; \textstyle \text{All store and access operations can take their index mod $n+1$} \\ \\ &amp; \textstyle \text{Loop for each episode}: \\ &amp; \textstyle \qquad \text{Initialize and store } S_0 \neq \text{terminal} \\ &amp; \textstyle \qquad \text{Select and store an action } A_0 \sim b(\cdot \vert S_0) \\ &amp; \textstyle \qquad T \leftarrow \infty \\ &amp; \textstyle \qquad \text{Loop for } t = 0, 1, 2, \dotso \text{:} \\ &amp; \textstyle \qquad\vert\qquad \text{If $t &lt; T$:} \\ &amp; \textstyle \qquad\vert\qquad\qquad \text{Take action } A_t \\ &amp; \textstyle \qquad\vert\qquad\qquad \text{Observe and store the next reward as $R_{t+1}$ and the next state as $S_{t+1}$} \\ &amp; \textstyle \qquad\vert\qquad\qquad \text{If $S_{t+1}$ is terminal:} \\ &amp; \textstyle \qquad\vert\qquad\qquad\qquad T \leftarrow t + 1 \\ &amp; \textstyle \qquad\vert\qquad\qquad \text{else:} \\ &amp; \textstyle \qquad\vert\qquad\qquad\qquad \text{Choose and store an action } A_{t+1} \sim b(\cdot \vert S_{t+1}) \\ &amp; \textstyle \qquad\vert\qquad\qquad\qquad \text{Select and store } \sigma_{t+1} \\ &amp; \textstyle \qquad\vert\qquad\qquad\qquad \text{Store } \frac{\pi(A_{t+1} \vert S_{t+1})}{b(A_{t+1 \vert S_{t+1}})} \text{ as } \rho_{t+1} \\ &amp; \textstyle \qquad\vert\qquad \tau \leftarrow t - n + 1 \qquad (\tau \text{ is the time whose estimate is being updated}) \\ &amp; \textstyle \qquad\vert\qquad \text{If $\tau \geq 0$:} \\ &amp; \textstyle \qquad\vert\qquad\qquad \text{If } t + 1 &lt; T \text{:} \\ &amp; \textstyle \qquad\vert\qquad\qquad\qquad G \leftarrow Q(S_{t+1}, A_{t+1}) \\ &amp; \textstyle \qquad\vert\qquad\qquad \text{Loop for } k = \min(t + 1,T) \text{ down through $\tau + 1$:} \\ &amp; \textstyle \qquad\vert\qquad\qquad\qquad \text{if } k = T \text{:} \\ &amp; \textstyle \qquad\vert\qquad\qquad\qquad\qquad G \leftarrow R_T \\ &amp; \textstyle \qquad\vert\qquad\qquad\qquad \text{else:} \\ &amp; \textstyle \qquad\vert\qquad\qquad\qquad\qquad \bar{V} \leftarrow \sum_a \pi(a \vert S_k)Q(S_k,a) \\ &amp; \textstyle \qquad\vert\qquad\qquad\qquad\qquad G \leftarrow R_k + \gamma\big(\sigma_k \rho_k + (1-\sigma_k)\pi(A_k \vert S_k)\big) \big(G - Q(S_k,A_k)\big) + \gamma \bar{V} \\ &amp; \textstyle \qquad\vert\qquad\qquad Q(S_\tau, A_\tau) \leftarrow Q(S_\tau, A_\tau) + \alpha [G - Q(S_\tau, A_\tau)] \\ &amp; \textstyle \qquad\vert\qquad\qquad \text{If $\pi$ is being learned, then ensure that $\pi(\cdot \vert S_\tau)$ is greedy wrt $Q$} \\ &amp; \textstyle \qquad \text{until } \tau = T - 1 \\ \end{align*}\)</p></blockquote><h2 id="summary"><span class="mr-2">Summary</span><a href="#summary" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>지금까지 $n$-step method에 대해 알아보았다. $n$-step method는 1-step TD와 MC method 사이에 해당하는 방법으로 보다 일반적인 TD method이다. 1-step TD나 MC method는 양극단에 있는 방법이기 떄문에 항상 잘 동작하지는 않는다. 특히 $n$-step TD는 1-step TD보다 복잡하고 더 많은 계산과 메모리를 요구하지만, 단 하나의 time step이 지배하는 현상에서 벗어날 수 있다는 점에서 지불할만한 가치가 있다.</p><p>$n$-step method는 $n$-step transition을 고려하는 방법이다. 모든 $n$-step method는 update를 수행하기 위해 $n$ time step만큼 기다려야 한다. $n$-step method 역시 on-policy와 off-policy method가 있다. 특히 off-policy method에는 2가지 접근법이 있다. 하나는 importance sampling을 사용해 처리하는 방법이다. 이 방법은 단순하지만 높은 분산을 가진다. 다른 하나는 tree-backup algorithm이다.</p><h2 id="references"><span class="mr-2">References</span><a href="#references" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>[1] Richard S. Sutton and Andrew G. Barto. <a href="/assets/materials/Reinforcement%20Learning%20An%20Introduction;%202nd%20Edition.%202018.pdf">Reinforcement Learning: An Introduction; 2nd Edition. 2018</a>.<br /> [2] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction; 2nd Edition. 2020.<br /> [3] Towards Data Science. Sagi Shaier. <a href="https://towardsdatascience.com/introduction-to-reinforcement-learning-rl-part-7-n-step-bootstrapping-6c3006a13265">N-step Bootstrapping</a>.</p><h2 id="footnotes"><span class="mr-2">Footnotes</span><a href="#footnotes" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><div class="footnotes" role="doc-endnotes"><ol><li id="fn:1" role="doc-endnote"><p>DevSlem. <a href="../monte-carlo-methods/#importance-sampling">Importance Sampling</a>. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:2" role="doc-endnote"><p>Reinforcement Learning: An Introduction; 2nd Edition. 2018. <a href="https://devslem.github.io/assets/materials/Reinforcement%20Learning%20An%20Introduction;%202nd%20Edition.%202018.pdf#page=172">Sec. 7.3, p.172</a>. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p></ol></div></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/reinforcement-learning/'>Reinforcement Learning</a>, <a href='/categories/rl-fundamental/'>RL Fundamental</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/rl/" class="post-tag no-text-decoration" >RL</a> <a href="/tags/ai/" class="post-tag no-text-decoration" >AI</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=n-step+Bootstrapping+-+DevSlem+Blog&url=https%3A%2F%2Fdevslem.github.io%2Freinforcement-learning%2Frl-fundamental%2Fn-step-bootstrapping%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=n-step+Bootstrapping+-+DevSlem+Blog&u=https%3A%2F%2Fdevslem.github.io%2Freinforcement-learning%2Frl-fundamental%2Fn-step-bootstrapping%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fdevslem.github.io%2Freinforcement-learning%2Frl-fundamental%2Fn-step-bootstrapping%2F&text=n-step+Bootstrapping+-+DevSlem+Blog" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div><script src="https://utteranc.es/client.js" repo="DevSlem/DevSlem.github.io" issue-term="pathname" label="comment" theme="github-dark" crossorigin="anonymous" async> </script></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/reinforcement-learning/drl-methods/dqn/">DQN: Deep Q-Networks</a><li><a href="/reinforcement-learning/drl-methods/dtqn/">DTQN: Deep Transformer Q-Networks for Partially Observable Reinforcement Learning</a><li><a href="/wsl-setup/">Windows Subsystem for Linux (WSL) Setup for Reinforcement Learning</a><li><a href="/algorithm/sorting/merge-sort/">[알고리즘] 합병 정렬</a><li><a href="/algorithm/sorting/shell-sort/">[알고리즘] 쉘 정렬</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/rl/">RL</a> <a class="post-tag" href="/tags/algorithm/">Algorithm</a> <a class="post-tag" href="/tags/unity/">Unity</a> <a class="post-tag" href="/tags/function-approximation-rl/">Function Approximation RL</a> <a class="post-tag" href="/tags/sorting/">Sorting</a> <a class="post-tag" href="/tags/enemy/">Enemy</a> <a class="post-tag" href="/tags/game-development/">Game Development</a> <a class="post-tag" href="/tags/project/">Project</a> <a class="post-tag" href="/tags/attribute/">Attribute</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 pl-3 pr-3 pr-xl-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/reinforcement-learning/rl-fundamental/policy-gradient-methods/"><div class="card-body"> <em class="small" data-ts="1665586800" data-df="ll" > Oct 13, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Policy Gradient Methods</h3><div class="text-muted small"><p> 드디어 긴 장정 끝에 대망의 마지막 챕터로 왔다. 이번 포스트에서는 그 유명한 policy gradient method에 대해 소개하려고 한다. 길고 긴 여정이 드디어 끝났다. Introduction 지금까지 우리는 value-based 기반의 방법을 다뤘었다. policy는 추정된 action value에 기반해 action을 선택했다. 이번에는...</p></div></div></a></div><div class="card"> <a href="/reinforcement-learning/rl-fundamental/multi-armed-bandits/"><div class="card-body"> <em class="small" data-ts="1653058800" data-df="ll" > May 21, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Multi-armed Bandits</h3><div class="text-muted small"><p> 이 포스트에서는 Reinforcement learning (RL)의 기본 내용인 Multi-armed Bandits 환경과 기본적인 아이디어들에 대해 알아본다. Reinforcement learning vs others Reinforcement learning (RL)과 다른 learning의 가장 큰 구별점은 사용하는 정보의 차이에 있다. 다른 ...</p></div></div></a></div><div class="card"> <a href="/reinforcement-learning/rl-fundamental/finite-markov-decision-processes/"><div class="card-body"> <em class="small" data-ts="1653836400" data-df="ll" > May 30, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Finite Markov Decision Processes</h3><div class="text-muted small"><p> 이 포스트에서는 Reinforcement Learning에서 기반이 되는 finite Markov Decision Processes (MDPs)와 finite MDPs 문제를 해결하기 위한 Bellman equations에 대해 소개한다. What is MDPs Markov Decision Processes (MDPs)는 연속적인 의사 결정을 형식...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/reinforcement-learning/rl-fundamental/temporal-difference-learning/" class="btn btn-outline-primary" prompt="Older"><p>Temporal-Difference Learning</p></a> <a href="/reinforcement-learning/rl-fundamental/planning-and-learning-with-tabular-methods/" class="btn btn-outline-primary" prompt="Newer"><p>Planning and Learning with Tabular Methods</p></a></div></div></div><footer class="row pl-3 pr-3"><div class="col-12 d-flex justify-content-between align-items-center text-muted pl-0 pr-0"><div class="footer-left"><p class="mb-0"> © 2024 <a href="https://github.com/username">DevSlem</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/rl/">RL</a> <a class="post-tag" href="/tags/algorithm/">Algorithm</a> <a class="post-tag" href="/tags/unity/">Unity</a> <a class="post-tag" href="/tags/function-approximation-rl/">Function Approximation RL</a> <a class="post-tag" href="/tags/sorting/">Sorting</a> <a class="post-tag" href="/tags/enemy/">Enemy</a> <a class="post-tag" href="/tags/game-development/">Game Development</a> <a class="post-tag" href="/tags/project/">Project</a> <a class="post-tag" href="/tags/attribute/">Attribute</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a><div id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-animation="true" data-autohide="false"><div class="toast-header"> <button type="button" class="ml-2 ml-auto close" data-dismiss="toast" aria-label="Close"> <span aria-hidden="true">&times;</span> </button></div><div class="toast-body text-center pt-0"><p class="pl-2 pr-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></div><script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] }, svg: { scale: 1.1 } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script>
