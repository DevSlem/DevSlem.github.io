<!DOCTYPE html><html lang="en" data-mode="dark" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="Multi-armed Bandits" /><meta property="og:locale" content="en" /><meta name="description" content="강화학습의 기본인 Multi-armed Bandits 문제에 대해 소개한다." /><meta property="og:description" content="강화학습의 기본인 Multi-armed Bandits 문제에 대해 소개한다." /><link rel="canonical" href="https://devslem.github.io/reinforcement-learning/rl-fundamental/multi-armed-bandits/" /><meta property="og:url" content="https://devslem.github.io/reinforcement-learning/rl-fundamental/multi-armed-bandits/" /><meta property="og:site_name" content="DevSlem Blog" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-05-21T00:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Multi-armed Bandits" /><meta name="google-site-verification" content="SSJwUK1JSPZV3wjQS19f2bXfHYIupbYEFsisH4Nuns4" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-05-22T00:00:00+09:00","datePublished":"2022-05-21T00:00:00+09:00","description":"강화학습의 기본인 Multi-armed Bandits 문제에 대해 소개한다.","headline":"Multi-armed Bandits","mainEntityOfPage":{"@type":"WebPage","@id":"https://devslem.github.io/reinforcement-learning/rl-fundamental/multi-armed-bandits/"},"url":"https://devslem.github.io/reinforcement-learning/rl-fundamental/multi-armed-bandits/"}</script><title>Multi-armed Bandits | DevSlem Blog</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="DevSlem Blog"><meta name="application-name" content="DevSlem Blog"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> </a></div><div class="site-title mt-3"> <a href="/">DevSlem Blog</a></div><div class="site-subtitle font-italic">It's a blog where i write posts about AI, RL, Unity, etc.</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/DevSlem" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['devslem12','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper"><div id="topbar" class="container d-flex align-items-center justify-content-between h-100 pl-3 pr-3 pl-md-4 pr-md-4"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Multi-armed Bandits</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper" class="d-flex justify-content-center"><div id="main" class="container pl-xl-4 pr-xl-4"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pr-xl-4"><div class="post pl-1 pr-1 pl-md-2 pr-md-2"><h1 data-toc-skip>Multi-armed Bandits</h1><div class="post-meta text-muted"> <span> Posted <em class="" data-ts="1653058800" data-df="ll" data-toggle="tooltip" data-placement="bottom"> May 21, 2022 </em> </span> <span> Updated <em class="" data-ts="1653177600" data-df="ll" data-toggle="tooltip" data-placement="bottom"> May 22, 2022 </em> </span><div class="d-flex justify-content-between"> <span> By <em> <a href="https://github.com/username">DevSlem</a> </em> </span><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2817 words"> <em>15 min</em> read</span></div></div></div><div class="post-content"><p>이 포스트에서는 Reinforcement learning (RL)의 기본 내용인 Multi-armed Bandits 환경과 기본적인 아이디어들에 대해 알아본다.</p><h2 id="reinforcement-learning-vs-others"><span class="mr-2">Reinforcement learning vs others</span><a href="#reinforcement-learning-vs-others" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Reinforcement learning (RL)과 다른 learning의 가장 큰 구별점은 사용하는 정보의 차이에 있다. 다른 learning은 주로 올바른 action을 나타내는 일종의 정답 label이 존재하는 instructive feedback을 사용하며, 이러한 feedback을 사용하는 learning을 supervised learning이라고 한다. 그러나 RL에서는 evaluative feedback을 사용한다. evaluative feedback은 이것이 얼마나 좋은 action인지를 나타내지만 best action인지 아닌지를 나타내지는 않는다. 이를 unsupervised learning이라고 한다.</p><h2 id="what-is-multi-armed-bandits"><span class="mr-2">What is Multi-armed Bandits</span><a href="#what-is-multi-armed-bandits" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Multi-armed Bandits 환경은 슬롯 머신에서 여러 개의 레버를 당겨 보상을 획득하는 환경이다. 이 때 레버의 개수를 $k$개라고 할 때 <em>$k$-armed bandit problem</em>이라고 하며 아래와 같은 환경으로 정의된다.</p><ul><li>$k$개의 다른 action들을 반복적으로 선택함.<li>각 선택에 대해 stationary probability distribution을 따르는 수치적인 reward를 획득함.<li>일정 기간(time steps) 동안의 expected total reward를 maximized하는게 목적임.</ul><p>stationary probability distribution은 시간이 흐름에도 변하지 않는 정적인 확률 분포를 의미한다.</p><p><img data-src="https://lilianweng.github.io/posts/2018-01-23-multi-armed-bandit/bern_bandit.png" alt="" width="60%" data-proofer-ignore> <em>Fig 1. Multi-armed bandits.<br /> (Image source: <a href="https://lilianweng.github.io/posts/2018-01-23-multi-armed-bandit/">Lil’Log</a>.)</em></p><p>$k$-armed bandit problem과 일반적인 reinforcement learning problem의 가장 큰 차이점은 $k$-armed bandit problem은 어떤 상태에서 선택한 행동으로 즉각적인 보상만 획득할 뿐, <strong>레버를 당기는 action들이 environment의 states와 future rewards를 변경시키지 않는다</strong>. 즉, actions와 states가 연관성이 없으며 이를 <em>nonassociative</em> setting이라고 한다. 반대로 <em>associative</em> setting에서는 선택한 action들이 states를 변경시켜 future rewards에 영향을 미치는 파급효과를 가진다.</p><p>$k$-armed bandit problem에서 각 time step $t$에서 선택한 action을 $A_t$, 획득한 reward을 $R_t$라고 할 때 임의의 action $a$에 대한 value $q_\ast(a)$는 $a$에 대한 expected reward이다.</p>\[q_\ast(a) \doteq \mathbb{E}[R_t \vert A_t = a]\]<p>그러나 <strong>실제 $q_\ast(a)$를 모르기 때문에 우리는 이 값을 추정</strong>해야한다. time step $t$에서 추정된 action $a$의 value를 $Q_t(a)$라고 할 때 우리의 목적은 이 값을 $q_\ast(a)$에 근접시키는 것이다.</p><h2 id="exploitation-vs-exploration"><span class="mr-2">Exploitation vs Exploration</span><a href="#exploitation-vs-exploration" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>action value를 추정하는 과정에서 가장 value가 높은 action을 <em>greedy</em> action이라고 하며 이들에 대한 선택을 <em>exploiting</em>이라고 한다. 그 외의 action을 선택할 때는 <em>exploration</em>이라고 부른다. exploitation은 현재 가진 정보를 기준으로 <strong>즉각적인 최고의 보상을 획득할 수 있는 수단</strong>이다. 그러나 exploration은 단기간 적은 보상을 획득하지만 내가 모르는 정보를 탐색해 현재 greedy action보다 더 나은 action을 발견하여 <strong>더 높은 total reward를 획득할 수 있는 수단</strong>이다. 결국 exploitation과 exploration 사이에 적절한 선택이 필요하며 이는 그 유명한 <em>exploitation vs exploration dilemma</em>로 강화학습의 숙명과도 같은 문제이다.</p><h2 id="action-value-methods"><span class="mr-2">Action-value Methods</span><a href="#action-value-methods" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>action value를 추정하는 가장 간단한 방법은 지금까지 획득한 reward의 평균을 구하는 것이다.</p>\[Q_t(a) \doteq \dfrac{\sum_{i=1}^{t-1}R_i \cdot 𝟙_{A_i=a}}{\sum_{i=1}^{t-1}𝟙_{A_i=a}}\]<p>$𝟙_{predicate}$은 $predicate$이 true이면 1, false이면 0을 반환하는 함수이다.</p><p>action value에 따라 action을 선택하는 가장 간단한 방법은 가장 높게 추정된 action value를 가진 action을 선택하는 것이다. 즉, greedy action을 선택한다.</p>\[A_t \doteq \underset{a}{\arg\max}\ Q_t(a)\]<p>위 방법은 항상 exploitation을 수행하기 때문에 지금보다 더 나은 행동을 발견할 수 없다. 이에 대안 대안으로 대부분은 exploitation을 수행하되 $\epsilon$의 확률로 랜덤하게 action을 선택한다. 이를 $\epsilon$-<em>greedy</em> 방법이라 한다.</p><h2 id="incremental-implementation"><span class="mr-2">Incremental Implementation</span><a href="#incremental-implementation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>어떤 단일 action의 $i$번째 선택 시 획득 한 reward를 $R_i$라고 할 때, 이 action을 $n - 1$번 선택했을 때의 action value의 추정치 $Q_n$을 $n - 1$번 획득한 reward들의 평균으로 추정한다면 아래와 같은 수식으로 표현할 수 있다.</p>\[Q_n \doteq \dfrac{R_1 + R_2 + \cdots + R_{n-1}}{n - 1}\]<p>그러나 위와 같은 수식에서는 그동안 획득한 모든 reward들을 모두 기록해야하며, 새로운 reward를 획득할 때 마다 처음부터 다시 reward들을 모두 더하는 계산을 해야한다는 문제가 있다. 이에 대한 대안으로 평균을 구하는 수식을 incremental한 형태로 변경할 수 있는데 이 경우 위 수식처럼 reward들을 기록할 필요가 없으며 계산 모든 reward들을 합할 필요가 없어진다. 기존 평균 값에 새롭게 획득한 reward의 일정 비중만을 누적하면 되는 원리이다. 기존 action value $Q_n$과 새롭게 획득한 $n$번째 reward $R_n$이 있을 때 action value에 대한 incremental formula는 아래와 같다.</p>\[Q_{n+1} = Q_n + \dfrac{1}{n}[R_n - Q_n]\]<p>위 수식에 대한 일반적인 형태는 아래와 같다.</p>\[\textit{NewEstimate} \leftarrow \textit{OldEstimate} + \textit{StepSize} \Big[\textit{Target} - \textit{OldEstimate} \Big]\]<p>위 수식에서 $\Big[\textit{Target} - \textit{OldEstimate} \Big]$는 추정치에 대한 <em>error</em>이며 이를 바탕으로 점점 <em>Target</em>에 다가간다.</p><h3 id="nonstationary-problem"><span class="mr-2">Nonstationary Problem</span><a href="#nonstationary-problem" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>reward에 대한 확률들이 시간이 지나도 변하지 않는 stationary problem에서는 평균을 구하는 위 방법이 유용할 지 모르지만 nonstationary 환경에서는 그렇지 않다. 이 경우엔 지난 과거의 보상보다 최근 보상에 더 큰 비중을 주는게 합당하다. 이에 대한 하나의 방법으로 step-size를 상수로 사용한다. 아래는 이에 대한 incremental update 수식이다.</p>\[Q_{n+1} \doteq Q_n + \alpha[R_n - Q_n]\]<p>step-size parameter인 $\alpha \in (0,1]$는 상수이다. 다만 $\alpha$ 값을 step에 따라 변경하는게 더 효과적일 때도 있다.</p><h3 id="initial-value"><span class="mr-2">Initial Value</span><a href="#initial-value" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>위 수식은 past rewards와 initial estimate $Q_1$의 weighted average로 표현될 수 있다.</p>\[\begin{aligned} Q_{n+1} &amp;= Q_n + \alpha[R_n - Q_n] \\ &amp;= (1-\alpha)^nQ_1 + \sum_{i=1}^n\alpha(1-\alpha)^{n-i}R_i \end{aligned}\]<p>위 수식을 보면 알겠지만 현재 action value는 initial value인 $Q_1(a)$에 영향을 받는다. 즉 <em>bias</em>가 발생하였다. 몰론 표본평균방법일 경우 모든 action들이 적어도 한번 선택된다면 이러한 bias는 사라지지만 위 수식처럼 step-size $\alpha$가 상수일 경우 bias는 영구적이다. 그러나 $\alpha \in (0, 1]$이기 때문에 시간이 지날 수록 결국 이러한 bias는 작아지게 된다. 그렇기 때문에 이러한 bias는 실제로 그다지 문제가 되지 않는다.</p><h2 id="upper-confidence-bound-action-selection"><span class="mr-2">Upper-Confidence-Bound Action Selection</span><a href="#upper-confidence-bound-action-selection" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>$\epsilon$-greedy 방법은 exploration을 무차별적으로 수행하게 만든다는 문제가 있다. <strong>action value의 추정치가 최대값에 얼마나 가까운지</strong>와 <strong>불확실성은 얼마나 되는지</strong>를 모두 고려해, 실제로 최적이 될 가능성에 따라 non-greedy action들 사이에서 선택하는 것이 조금 더 효과적일 것이다. 이에 대한 대안으로 <em>upper confidence bound</em> (UCB) 방법이 있으며 그 수식은 아래와 같다.</p>\[A_t \doteq \underset{a}{\arg\max}\ \Bigg[Q_t(a) + c \sqrt{\dfrac{\ln t}{N_t(a)}} \ \Bigg]\]<p>$N_t(a)$는 time step $t$ 이전에 action $a$가 선택된 횟수이며, $c &gt; 0$는 exploration을 컨트롤 하는 정도로 신뢰도를 결정한다. square-root 부분은 $a$의 값에 대한 추정에서 불확실성을 나타낸다. 이를 통해 action $a$의 true value에 대한 일종의 upper bound를 설정할 수 있다. action $a$가 선택될 때에는 분자 $\ln t$가 증가하긴 하지만 분모 $N_t(a)$가 증가하기 때문에 불확실성은 대게 감소한다. 그 이유는 분자는 log-scale이지만 분모는 linear-scale이기 때문이다. $a$ 외의 다른 action이 선택될 때는 분자 $\ln t$는 증가하지만 분모 $N_t(a)$는 변하지 않기 때문에 불확실성은 증가한다. 위 수식에 따라 <strong>action value의 추정치 $Q_t(a)$가 너무 낮거나, action $a$가 너무 자주 선택됬을 경우 점점 선택되는 빈도가 줄게 된다</strong>. 어떤 action $a$의 action value $Q_t(a)$가 높아 이 action이 한동안 계속 자주 선택될 경우 이 action에 대한 불확실성은 줄어든다. 반대로 다른 action들은 그동안 선택되지 않았기 때문에 불확실성이 늘어나며 어느 순간 cross가 발생해 다른 action의 upper bound가 더 커져 다른 action을 수행하게 된다. 그러나 $t \rightarrow \infty$일 경우 분자는 log-scale이지만 분모는 linear-scale이기 때문에 결국 0으로 수렴한다. 즉, <strong>time step $t$가 작을 때는 exploration이 활발히 일어나지만 time step $t$가 증가할 수록 전체 action에 대한 불확실성은 낮아지고 결국 action value $Q_t(a)$에 대해서만 action을 선택하는 exploitation을 수행</strong>할 것이다.</p><p>UCB 방법은 $k$-armed bandits에서 $\epsilon$-greedy 보다 좋은 성능을 낸다. 그러나 좀 더 일반적인 RL setting으로 확장하는 것은 상당히 어려우며 실용적이지 못하다. UCB는 nonstationary 문제를 다루는데 어려움이 있으며 large state space에서 function approximation을 사용할 때 어려움이 있다.</p><h2 id="gradient-bandit-algorithms"><span class="mr-2">Gradient Bandit Algorithms</span><a href="#gradient-bandit-algorithms" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>각 action $a$에 대한 numerical <em>preference</em>를 $H_t(a)$를 학습하는 것을 고려해보자. preference가 클 수록 더 자주 action이 선택된다. 여기서 preference는 action value $Q_t(a)$와는 다르게 reward 측면에서 해석되지 않는다. 또한 action을 선택할 때 한 action의 preference와 다른 action들의 preference 사이의 <strong>상대적 비교</strong>로 결정한다. $k$개의 action이 있다고 할 때 각 action을 선택하는 확률은 <em>soft-max distribution</em>을 따르며 그 수식은 아래와 같다.</p>\[\text{Pr}\lbrace A_t = a \rbrace \doteq \dfrac{e^{H_t(a)}}{\sum_{b=1}^k e^{H_t(b)}} \doteq \pi_t(a)\]<p>$\pi_t(a)$는 time step $t$에서 action을 선택하는 확률이다. preference $H_t(a)$를 학습하기 위한 방법 중 하나로 stochastic gradient ascent가 있다. action $A_t$를 선택한 뒤 reward $R_t$를 획득했을 때 action preference들은 아래와 같은 수식으로 업데이트된다.</p>\[H_{t+1}(a) \doteq H_t(a) + \alpha \dfrac{\partial \mathbb{E}[R_t]}{\partial H_t(a)}\]<p>위 수식은 gradient ascent에 대한 기본적인 아이디어이며 이를 바탕으로 아래와 같은 수식을 얻을 수 있다.</p>\[\begin{aligned} &amp;H_{t+1}(A_t) \doteq H_t(A_t) + \alpha(R_t - \bar{R_t})(1 - \pi_t(A_t)) &amp; \text{and} &amp; \\ &amp;H_{t+1}(a) \doteq H_t(a) - \alpha(R_t - \bar{R_t})\pi_t(a) &amp; \text{for all} \; a \neq A_t \end{aligned}\]<p>$\bar{R_t}$는 time step $t$까지의 모든 reward의 평균이다. $\bar{R_t}$는 reward에 대한 baseline으로 획득한 reward $R_t$가 baseline보다 클 경우 $A_t$를 미래에 수행할 확률은 증가하고, baseline보다 작을 경우엔 감소한다. 선택되지 않은 나머지 action들은 $A_t$와 반대로 업데이트된다.</p><h2 id="references"><span class="mr-2">References</span><a href="#references" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>[1] Richard S. Sutton and Andrew G. Barto. <a href="/assets/materials/Reinforcement%20Learning%20An%20Introduction;%202nd%20Edition.%202018.pdf">Reinforcement Learning: An Introduction; 2nd Edition. 2018</a>.<br /> [2] Lil’Log. <a href="https://lilianweng.github.io/posts/2018-01-23-multi-armed-bandit/">The Multi-Armed Bandit Problem and Its Solutions</a>.</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/reinforcement-learning/'>Reinforcement Learning</a>, <a href='/categories/rl-fundamental/'>RL Fundamental</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/rl/" class="post-tag no-text-decoration" >RL</a> <a href="/tags/ai/" class="post-tag no-text-decoration" >AI</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Multi-armed+Bandits+-+DevSlem+Blog&url=https%3A%2F%2Fdevslem.github.io%2Freinforcement-learning%2Frl-fundamental%2Fmulti-armed-bandits%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Multi-armed+Bandits+-+DevSlem+Blog&u=https%3A%2F%2Fdevslem.github.io%2Freinforcement-learning%2Frl-fundamental%2Fmulti-armed-bandits%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fdevslem.github.io%2Freinforcement-learning%2Frl-fundamental%2Fmulti-armed-bandits%2F&text=Multi-armed+Bandits+-+DevSlem+Blog" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div><script src="https://utteranc.es/client.js" repo="DevSlem/DevSlem.github.io" issue-term="pathname" label="comment" theme="github-dark" crossorigin="anonymous" async> </script></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/reinforcement-learning/drl-methods/dqn/">DQN: Deep Q-Networks</a><li><a href="/reinforcement-learning/drl-methods/dtqn/">DTQN: Deep Transformer Q-Networks for Partially Observable Reinforcement Learning</a><li><a href="/wsl-setup/">Windows Subsystem for Linux (WSL) Setup for Reinforcement Learning</a><li><a href="/algorithm/sorting/merge-sort/">[알고리즘] 합병 정렬</a><li><a href="/algorithm/sorting/shell-sort/">[알고리즘] 쉘 정렬</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/rl/">RL</a> <a class="post-tag" href="/tags/algorithm/">Algorithm</a> <a class="post-tag" href="/tags/unity/">Unity</a> <a class="post-tag" href="/tags/function-approximation-rl/">Function Approximation RL</a> <a class="post-tag" href="/tags/sorting/">Sorting</a> <a class="post-tag" href="/tags/enemy/">Enemy</a> <a class="post-tag" href="/tags/game-development/">Game Development</a> <a class="post-tag" href="/tags/project/">Project</a> <a class="post-tag" href="/tags/attribute/">Attribute</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 pl-3 pr-3 pr-xl-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/reinforcement-learning/rl-fundamental/policy-gradient-methods/"><div class="card-body"> <em class="small" data-ts="1665586800" data-df="ll" > Oct 13, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Policy Gradient Methods</h3><div class="text-muted small"><p> 드디어 긴 장정 끝에 대망의 마지막 챕터로 왔다. 이번 포스트에서는 그 유명한 policy gradient method에 대해 소개하려고 한다. 길고 긴 여정이 드디어 끝났다. Introduction 지금까지 우리는 value-based 기반의 방법을 다뤘었다. policy는 추정된 action value에 기반해 action을 선택했다. 이번에는...</p></div></div></a></div><div class="card"> <a href="/reinforcement-learning/rl-fundamental/finite-markov-decision-processes/"><div class="card-body"> <em class="small" data-ts="1653836400" data-df="ll" > May 30, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Finite Markov Decision Processes</h3><div class="text-muted small"><p> 이 포스트에서는 Reinforcement Learning에서 기반이 되는 finite Markov Decision Processes (MDPs)와 finite MDPs 문제를 해결하기 위한 Bellman equations에 대해 소개한다. What is MDPs Markov Decision Processes (MDPs)는 연속적인 의사 결정을 형식...</p></div></div></a></div><div class="card"> <a href="/reinforcement-learning/rl-fundamental/dynamic-programming-in-rl/"><div class="card-body"> <em class="small" data-ts="1656082800" data-df="ll" > Jun 25, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Dynamic Programming in RL</h3><div class="text-muted small"><p> 이 포스트에서는 RL에서 MDP로 environment의 perfect model이 주어졌을 때 optimal policy를 구하는데 사용되는 기초적인 방식인 Dynamic Programming (DP)를 소개한다. Introduction Reinforcement Learning (RL)에서 environment가 perfect model로 env...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/reinforcement-learning/exploration-methods/curiosity-driven-exploration/" class="btn btn-outline-primary" prompt="Older"><p>Curiosity-driven Exploration by Self-supervised Prediction</p></a> <a href="/game-development/game-physics-summary/" class="btn btn-outline-primary" prompt="Newer"><p>게임 물리 요약 (Game Physics Summary)</p></a></div></div></div><footer class="row pl-3 pr-3"><div class="col-12 d-flex justify-content-between align-items-center text-muted pl-0 pr-0"><div class="footer-left"><p class="mb-0"> © 2024 <a href="https://github.com/username">DevSlem</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/rl/">RL</a> <a class="post-tag" href="/tags/algorithm/">Algorithm</a> <a class="post-tag" href="/tags/unity/">Unity</a> <a class="post-tag" href="/tags/function-approximation-rl/">Function Approximation RL</a> <a class="post-tag" href="/tags/sorting/">Sorting</a> <a class="post-tag" href="/tags/enemy/">Enemy</a> <a class="post-tag" href="/tags/game-development/">Game Development</a> <a class="post-tag" href="/tags/project/">Project</a> <a class="post-tag" href="/tags/attribute/">Attribute</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a><div id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-animation="true" data-autohide="false"><div class="toast-header"> <button type="button" class="ml-2 ml-auto close" data-dismiss="toast" aria-label="Close"> <span aria-hidden="true">&times;</span> </button></div><div class="toast-body text-center pt-0"><p class="pl-2 pr-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></div><script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] }, svg: { scale: 1.1 } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script>
