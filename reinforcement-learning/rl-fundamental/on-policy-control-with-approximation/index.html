<!DOCTYPE html><html lang="en" data-mode="dark" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="On-policy Control with Approximation" /><meta property="og:locale" content="en" /><meta name="description" content="이 포스트에서는 function approximation을 사용한 prediction을 control로 확장할 것이다. 이를 위해 state-value function이 아닌 action-value function을 추정한다. 그 후 on-policy GPI의 일반적인 패턴을 따라 학습을 진행하는 방법을 알아본다. 이 포스트에서는 특히 semi-gradient TD(0)의 가장 기본적인 확장인 semi-gradient Sarsa 알고리즘을 다룰 것이다." /><meta property="og:description" content="이 포스트에서는 function approximation을 사용한 prediction을 control로 확장할 것이다. 이를 위해 state-value function이 아닌 action-value function을 추정한다. 그 후 on-policy GPI의 일반적인 패턴을 따라 학습을 진행하는 방법을 알아본다. 이 포스트에서는 특히 semi-gradient TD(0)의 가장 기본적인 확장인 semi-gradient Sarsa 알고리즘을 다룰 것이다." /><link rel="canonical" href="https://devslem.github.io/reinforcement-learning/rl-fundamental/on-policy-control-with-approximation/" /><meta property="og:url" content="https://devslem.github.io/reinforcement-learning/rl-fundamental/on-policy-control-with-approximation/" /><meta property="og:site_name" content="DevSlem Blog" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-08-23T00:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="On-policy Control with Approximation" /><meta name="google-site-verification" content="SSJwUK1JSPZV3wjQS19f2bXfHYIupbYEFsisH4Nuns4" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-08-25T00:00:00+09:00","datePublished":"2022-08-23T00:00:00+09:00","description":"이 포스트에서는 function approximation을 사용한 prediction을 control로 확장할 것이다. 이를 위해 state-value function이 아닌 action-value function을 추정한다. 그 후 on-policy GPI의 일반적인 패턴을 따라 학습을 진행하는 방법을 알아본다. 이 포스트에서는 특히 semi-gradient TD(0)의 가장 기본적인 확장인 semi-gradient Sarsa 알고리즘을 다룰 것이다.","headline":"On-policy Control with Approximation","mainEntityOfPage":{"@type":"WebPage","@id":"https://devslem.github.io/reinforcement-learning/rl-fundamental/on-policy-control-with-approximation/"},"url":"https://devslem.github.io/reinforcement-learning/rl-fundamental/on-policy-control-with-approximation/"}</script><title>On-policy Control with Approximation | DevSlem Blog</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="DevSlem Blog"><meta name="application-name" content="DevSlem Blog"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> </a></div><div class="site-title mt-3"> <a href="/">DevSlem Blog</a></div><div class="site-subtitle font-italic">It's a blog where i write posts about AI, RL, Unity, etc.</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/DevSlem" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['devslem12','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper"><div id="topbar" class="container d-flex align-items-center justify-content-between h-100 pl-3 pr-3 pl-md-4 pr-md-4"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>On-policy Control with Approximation</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper" class="d-flex justify-content-center"><div id="main" class="container pl-xl-4 pr-xl-4"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pr-xl-4"><div class="post pl-1 pr-1 pl-md-2 pr-md-2"><h1 data-toc-skip>On-policy Control with Approximation</h1><div class="post-meta text-muted"> <span> Posted <em class="" data-ts="1661180400" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Aug 23, 2022 </em> </span> <span> Updated <em class="" data-ts="1661385600" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Aug 25, 2022 </em> </span><div class="d-flex justify-content-between"> <span> By <em> <a href="https://github.com/username">DevSlem</a> </em> </span><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2690 words"> <em>14 min</em> read</span></div></div></div><div class="post-content"><p>이 포스트에서는 function approximation을 사용한 prediction을 control로 확장할 것이다. 이를 위해 state-value function이 아닌 action-value function을 추정한다. 그 후 on-policy GPI의 일반적인 패턴을 따라 학습을 진행하는 방법을 알아본다. 이 포스트에서는 특히 semi-gradient TD(0)의 가장 기본적인 확장인 semi-gradient Sarsa 알고리즘을 다룰 것이다.</p><h2 id="episodic-semi-gradient-control"><span class="mr-2">Episodic Semi-gradient Control</span><a href="#episodic-semi-gradient-control" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>function approximation을 통해 매개변수화된 action-value function은 $\hat{q}(s,a,\mathbf{w}) \approx q_\pi(s,a)$이며, 이 때 $\mathbf{w} \in \mathbb{R}^d$는 $d$차원 weight 벡터이다. 또한 $S_t \mapsto U_t$가 아닌 $S_t, A_t \mapsto U_t$ 형식의 training example을 고려한다. $U_t$는 $q_\pi(S_t,A_t)$의 어떤 근사값이든 가능하다. 이를 바탕으로 action-value prediction에 대한 일반적인 gradient-descent update는 아래와 같다.</p>\[\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \Big[ U_t - \hat{q}(S_t,A_t,\mathbf{w}_t) \Big] \nabla \hat{q}(S_t,A_t,\mathbf{w}_t)\]<p>위 update rule을 바탕으로 한 one-step Sarsa의 update rule은 아래와 같다.</p>\[\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \Big[ R_{t+1} + \gamma \hat{q}(S_{t+1},A_{t+1},\mathbf{w}_t) - \hat{q}(S_t,A_t,\mathbf{w}_t) \Big] \nabla \hat{q}(S_t,A_t,\mathbf{w}_t)\]<p>위 방법을 <em>episodic semi-gradient one-step Sarsa</em>라고 부른다. continuing task에는 다른 방법이 사용된다.</p><p>이제 control을 수행하자. 여기서는 continuous action이나 매우 많은 discrete action에 대해서는 다루지 않는다. 이는 아직까지 연구중인 굉장히 어려운 문제이기 때문이다. action이 discrete하고 그리 많지 않다면 $\epsilon$-greedy policy와 같은 방법을 통해 policy improvement를 수행할 수 있다. 아래 박스는 전체 알고리즘이다.</p><blockquote><h5 id="textalgorithm-episodic-semi-gradient-sarsa-for-estimating--hatq-approx-q_ast"><span class="mr-2">$\text{Algorithm: Episodic Semi-gradient Sarsa for Estimating } \hat{q} \approx q_\ast$</span><a href="#textalgorithm-episodic-semi-gradient-sarsa-for-estimating--hatq-approx-q_ast" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5><p>\(\begin{align*} &amp; \textstyle \text{Input: a differentiable action-value function parameterization } \hat{q} : \mathcal{S} \times \mathcal{A} \times \mathbb{R}^d \rightarrow \mathbb{R} \\ &amp; \textstyle \text{Algorithm parameters: step size $\alpha &gt; 0$, small $\epsilon &gt; 0$} \\ &amp; \textstyle \text{Initialize value-function weights $\mathbf{w} \in \mathbb{R}^d$ arbitrarily (e.g., $\mathbf{w} = \mathbf{0}$)} \\ \\ &amp; \textstyle \text{Loop for each episode:} \\ &amp; \textstyle \qquad S,A \leftarrow \text{initial state and action of episode (e.g., $\epsilon$-greedy)} \\ &amp; \textstyle \qquad \text{Loop for each step of episode:} \\ &amp; \textstyle \qquad\qquad \text{Take action $A$, observe } R, S' \\ &amp; \textstyle \qquad\qquad \text{If $S'$ is terminal:} \\ &amp; \textstyle \qquad\qquad\qquad \mathbf{w} \leftarrow \mathbf{w} + \alpha \big[R - \hat{q}(S,A,\mathbf{w}) \big] \nabla \hat{q}(S,A,\mathbf{w}) \\ &amp; \textstyle \qquad\qquad\qquad \text{Go to next episode} \\ &amp; \textstyle \qquad\qquad \text{Choose $A'$ as a function of $\hat{q}(S',\cdot,\mathbf{w})$ (e.g., $\epsilon$-greedy)} \\ &amp; \textstyle \qquad\qquad \mathbf{w} \leftarrow \mathbf{w} + \alpha \big[R + \gamma \hat{q}(S',A',\mathbf{w}) - \hat{q}(S,A,\mathbf{w}) \big] \nabla \hat{q}(S,A,\mathbf{w}) \\ &amp; \textstyle \qquad\qquad S \leftarrow S' \\ &amp; \textstyle \qquad\qquad A \leftarrow A' \\ \end{align*}\)</p></blockquote><h2 id="semi-gradient-n-step-sarsa"><span class="mr-2">Semi-gradient $n$-step Sarsa</span><a href="#semi-gradient-n-step-sarsa" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>episodic semi-gradient Sarsa의 $n$-step version이다. 아래는 update target에 사용되는 $n$-step return으로 function approximation 형식으로 정의되었다.</p>\[G_{t:{t+n}} \doteq R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1}R_{t+n} + \gamma^n \hat{q}(S_{t+n}, A_{t+n}, \mathbf{w}_{t+n-1}), \quad t + n &lt; T\]<p>$t+n \geq T$일 때 $G_{t:t+n} \doteq G_t$이다. $n$-step update rule은 아래와 같다.</p>\[\mathbf{w}_{t+n} \doteq \mathbf{w}_{t+n-1} + \alpha \Big[ G_{t:t+n} - \hat{q}(S_t,A_t,\mathbf{w}_{t+n-1}) \Big] \nabla \hat{q}(S_t,A_t,\mathbf{w}_{t+n-1}), \quad 0 \leq t &lt; T\]<p>아래 박스는 전체 알고리즘이다.</p><blockquote><h5 id="textalgorithm-episodic-semi-gradient-n-step-sarsa-for-estimating--hatq-approx-q_ast-text-or--q_pi"><span class="mr-2">$\text{Algorithm: Episodic semi-gradient $n$-step Sarsa for estimating } \hat{q} \approx q_\ast \text{ or } q_\pi$</span><a href="#textalgorithm-episodic-semi-gradient-n-step-sarsa-for-estimating--hatq-approx-q_ast-text-or--q_pi" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5><p>\(\begin{align*} &amp; \textstyle \text{Input: a differentiable action-value function parameterization } \hat{q}: \mathcal{S} \times \mathcal{A} \times \mathbb{R}^d \rightarrow \mathbb{R} \\ &amp; \textstyle \text{Input: a policy $\pi$ (if estimating $q_\pi$)} \\ &amp; \textstyle \text{Algorithm parameters: step size $\alpha &gt; 0$, small $\epsilon &gt; 0$, a positive integer $n$} \\ &amp; \textstyle \text{Initialize value-function weights $\mathbf{w} \in \mathbb{R}^d$ arbitrarily (e.g., $\mathbf{w} = \mathbf{0}$)} \\ &amp; \textstyle \text{All store and access operations $(S_t, A_t, R_t)$ can take their index mod $n+1$} \\ \\ &amp; \textstyle \text{Loop for each episode:} \\ &amp; \textstyle \qquad \text{Initialize and store $S_0 \neq$ terminal} \\ &amp; \textstyle \qquad \text{Select and store an action $A_0 \sim \pi(\cdot \vert S_0)$ or $\epsilon$-greedy wrt $\hat{q}(S_0,\cdot,\mathbf{w})$} \\ &amp; \textstyle \qquad T \leftarrow \infty \\ &amp; \textstyle \qquad \text{Loop for } t = 0, 1, 2, \ldots : \\ &amp; \textstyle \qquad\vert\qquad \text{If $t &lt; T$, then:} \\ &amp; \textstyle \qquad\vert\qquad\qquad \text{Take action } A_t \\ &amp; \textstyle \qquad\vert\qquad\qquad \text{Observe and store the next reward as $R_{t+1}$ and the next state as $S_{t+1}$} \\ &amp; \textstyle \qquad\vert\qquad\qquad \text{If $S_{t+1}$ is terminal, then:} \\ &amp; \textstyle \qquad\vert\qquad\qquad\qquad T \leftarrow t + 1 \\ &amp; \textstyle \qquad\vert\qquad\qquad \text{else:} \\ &amp; \textstyle \qquad\vert\qquad\qquad\qquad \text{Select and store $A_{t+1} \sim \pi(\cdot \vert S_{t+1})$ or $\epsilon$-greedy wrt $\hat{q}(S_{t+1},\cdot,\mathbf{w})$} \\ &amp; \textstyle \qquad\vert\qquad \tau \leftarrow t - n + 1 \qquad \text{($\tau$ is the time whose estimate is being updated)} \\ &amp; \textstyle \qquad\vert\qquad \text{If $\tau \geq 0$:} \\ &amp; \textstyle \qquad\vert\qquad\qquad G \leftarrow \sum_{i = \tau + 1}^{\min(\tau + n, T)} \gamma^{i - \tau - 1}R_i \\ &amp; \textstyle \qquad\vert\qquad\qquad \text{If $\tau + n &lt; T$, then } G \leftarrow G + \gamma^n \hat{q}(S_{\tau + n}, A_{\tau + n}, \mathbf{w}) \qquad (G_{\tau : \tau + n}) \\ &amp; \textstyle \qquad\vert\qquad\qquad \mathbf{w} \leftarrow \mathbf{w} + \alpha \big[G - \hat{q}(S_\tau, A_\tau, \mathbf{w}) \big] \nabla \hat{q}(S_\tau, A_\tau, \mathbf{w}) \\ &amp; \textstyle \qquad \text{Until } \tau = T - 1 \\ \end{align*}\)</p></blockquote><h2 id="average-reward-a-new-problem-setting-for-continuing-tasks"><span class="mr-2">Average Reward: A New Problem Setting for Continuing Tasks</span><a href="#average-reward-a-new-problem-setting-for-continuing-tasks" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>우리가 그동안 다뤘던 setting은 discounted setting으로 delayed reward에 discount를 적용하는 setting이다. 이 setting은 function approximation을 continuing task에서 사용할 때 문제가 발생한다. 이로 인해 새로운 setting을 도입할 필요가 있다. 사실 tabular 방법을 사용할 때는 오히려 discounted setting이 효과적이다.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></p><h3 id="average-reward"><span class="mr-2">Average Reward</span><a href="#average-reward" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>continuing task에 적용하기 위해 새롭게 도입할 setting은 <em>average reward</em> setting이다. 이 setting은 discounted reward가 존재하지 않으며 즉각적인 reward와 delayed reward를 동일하게 취급한다. 이 setting에서 policy $\pi$에 대한 quality는 <em>average reward</em> $r(\pi)$에 의해 정의된다.</p>\[\begin{align*} r(\pi) &amp;\doteq \lim_{h \rightarrow \infty} \dfrac{1}{h} \sum_{t=1}^h \mathbb{E}[R_t \ \vert \ S_0, A_{0:t-1} \sim \pi] \\ &amp;= \lim_{t \rightarrow \infty} \mathbb{E}[R_t \ \vert \ S_0, A_{0:t-1} \sim \pi] \\ &amp;= \sum_s \mu_\pi(s) \sum_a \pi(a \vert s) \sum_{s',r} p(s',r \vert s,a)r \end{align*}\]<p>기대값은 시작 state $S_0$와 policy $\pi$에 따라 선택된 subsequent action $A_0, A_1, \dots, A_{t-1}$을 조건으로 한다. MDP가 <em>ergodic</em>하다면 위 두번째와 세번째 수식이 성립한다. ergodic하다는게 무슨 의미일까? 이에 대해 알아보자.</p><h3 id="ergodicity"><span class="mr-2">Ergodicity</span><a href="#ergodicity" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>ergodicity는 시간이 충분할 때 선택된 action들에 상관없이 모든 state가 방문됨을 의미한다. 따라서 $S_0$에 독립적인 steady-state distribution $\mu_\pi(s) \doteq \lim_{t \rightarrow \infty} \Pr \{ S_t=s \ \vert \ A_{0:t-1} \sim \pi \}$가 존재한다. distribution $\mu_\pi(s)$가 존재한다는 것은, 모든 state에 대해 어떤 action들이 선택되었든 간에 어떤 state $s$에 방문할 확률이 수렴했음을 의미이다.</p><p>위 정의에 따라 ergodic MDP에서는 시작 state와 초기에 결정된 action들은 오직 일시적인 효과만 가진다. 장기적 관점에서는 어떤 state에 방문할 확률은 오직 policy와 MDP transition probability에 의해 결정된다. ergodicity는 $r(\pi)$의 수렴에 대한 충분조건이지만 필요조건은 아니다.</p><p>steady-state distribution $\mu_\pi$ 하에서 policy $\pi$에 따라 action을 선택할 때, 어떤 state $s’$이 방문될 확률 $\mu_\pi(s’)$에 대해 아래와 같은 수식이 성립한다.</p>\[\sum_s \mu_\pi(s) \sum_a \pi(a \vert s)p(s' \vert s,a) = \mu_\pi(s')\]<p>위 수식이 성립하는 이유는 앞서 언급했듯이 어떤 state에 방문할 확률은 오직 policy $\pi$와 transition probability $p$에 의해 결정되기 때문임을 직관적으로 알 수 있다.</p><h3 id="conversion-to-average-reward-setting"><span class="mr-2">Conversion to Average Reward Setting</span><a href="#conversion-to-average-reward-setting" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>average reward setting에서 return은 reward와 average reward의 차이의 관점에서 정의된다.</p>\[G_t \doteq R_{t+1} - r(\pi) + R_{t+2} - r(\pi) + R_{t+3} - r(\pi) + \cdots\]<p>이 return을 <em>differential return</em>이라고 하며, 이에 대응되는 value function을 <em>differential value function</em>이라고 한다. differential value function은 새로운 return만 사용할 뿐, 기존 value function과 동일한 관점에서 정의된다. 즉, differential value function $v_\pi(s) \doteq \mathbb{E}_ \pi[G_t \vert S_t=s]$이고, $q_\pi(s,a) \doteq \mathbb{E}_\pi[G_t \vert S_t=s, A_t=a]$이다. differential value function 역시 bellman equation을 가지나 약간의 차이가 있다. 먼저 discount rate $\gamma$를 제거하고, 모든 reward를 reward와 실제 average reward와의 차이로 대체한다.</p>\[\begin{align*} &amp; v_\pi(s) = \sum_a \pi(a \vert s) \sum_{r,s'} p(s',r \vert s,a)\Big[r - r(\pi) + v_\pi(s')\Big] \\ &amp; q_\pi(s,a) = \sum_{r,s'}p(s',r \vert s,a)\Big[r - r(\pi) + \sum_{a'} \pi(a' \vert s')q_\pi(s', a')\Big] \\ &amp; v_\ast(s) = \max_a \sum_{r,s'} p(s',r \vert s,a)\Big[r - \max_\pi r(\pi) + v_\ast(s')\Big] \\ &amp; q_\ast(s,a) = \sum_{r,s'} p(s',r \vert s,a) \Big[r - \max_\pi r(\pi) + \max_{a'} q_\ast(s',a')\Big] \end{align*}\]<p>또한 TD error에 대한 differential한 형식 역시 정의할 수 있다.</p>\[\begin{align*} &amp; \delta_t \doteq R_{t+1} - \bar{R}_t + \hat{v}(S_{t+1}, \mathbf{w}_t) - \hat{v}(S_t, \mathbf{w}_t) \\ &amp; \delta_t \doteq R_{t+1} - \bar{R}_t + \hat{q}(S_{t+1}, A_{t+1}, \mathbf{w}_t) - \hat{q}(S_t, A_t, \mathbf{w}_t) \end{align*}\]<p>$\bar{R}_t$는 average reward $r(\pi)$의 time step $t$에서의 추정치이다. 이 정의들을 통해 기존 대부분의 알고리즘과 이론적 결과를 특별한 변화 없이 average reward setting으로 전환할 수 있다. 예를 들어 semi-gradient Sarsa의 average reward version은 단순히 TD error을 differential TD error로 전환하기만 하면 된다.</p>\[\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \delta_t \nabla \hat{q}(S_t, A_t, \mathbf{w}_t)\]<p>$\delta_t$는 action value에 대한 differential TD error이다. differential semi-gradient Sarsa에 대한 전체 알고리즘은 아래 박스와 같다.</p><blockquote><h5 id="textalgorithm-differential-semi-gradient-sarsa-for-estimating--hatq-approx-q_ast"><span class="mr-2">$\text{Algorithm: Differential semi-gradient Sarsa for estimating } \hat{q} \approx q_\ast$</span><a href="#textalgorithm-differential-semi-gradient-sarsa-for-estimating--hatq-approx-q_ast" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5><p>\(\begin{align*} &amp; \textstyle \text{Input: a differentiable action-value function parameterization } \hat{q} : \mathcal{S} \times \mathcal{A} \times \mathbb{R}^d \rightarrow \mathbb{R} \\ &amp; \textstyle \text{Algorithm parameters: step size $\alpha, \beta &gt; 0$, small $\epsilon &gt; 0$} \\ &amp; \textstyle \text{Initialize value-function weights $\mathbf{w} \in \mathbb{R}^d$ arbitrarily (e.g., $\mathbf{w}=\mathbf{0}$)} \\ &amp; \textstyle \text{Initialize average reward estimate $\bar{R} \in \mathbb{R}$ arbitrarily (e.g., $\bar{R} = 0$)} \\ \\ &amp; \textstyle \text{Initialize state $S$, and action $A$} \\ &amp; \textstyle \text{Loop for each step:} \\ &amp; \textstyle \qquad \text{Take action $A$, observe $R, S'$} \\ &amp; \textstyle \qquad \text{Choose $A'$ as a function of $\hat{q}(S', \cdot, \mathbf{w})$ (e.g., $\epsilon$-greedy)} \\ &amp; \textstyle \qquad \delta \leftarrow R - \bar{R} + \hat{q}(S',A',\mathbf{w}) - \hat{q}(S,A,\mathbf{w}) \\ &amp; \textstyle \qquad \bar{R} \leftarrow \bar{R} + \beta \delta \\ &amp; \textstyle \qquad \mathbf{w} \leftarrow \mathbf{w} + \alpha \delta \nabla \hat{q}(S,A,\mathbf{w}) \\ &amp; \textstyle \qquad S \leftarrow S' \\ &amp; \textstyle \qquad A \leftarrow A' \\ \end{align*}\)</p></blockquote><p>위 알고리즘은 differential value가 아닌 differential value에 임의의 offset이 더해진 값으로 수렴한다는 이슈가 있다. 그러나 bellman equation이나 TD error는 모든 값이 같은 양만큼 이동하더라도 (즉, 같은 offset이 더해지더라도) 영향을 받지 않는다. 따라서 실제로는 문제가 되지 않는다.</p><h2 id="deprecating-the-discounted-setting"><span class="mr-2">Deprecating the Discounted Setting</span><a href="#deprecating-the-discounted-setting" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>곧 추가될 예정.</p><h2 id="differential-semi-gradient-n-step-sarsa"><span class="mr-2">Differential Semi-gradient $n$-step Sarsa</span><a href="#differential-semi-gradient-n-step-sarsa" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>$n$-step return을 function approximation이 사용된 differential 형식으로 아래와 같이 바꿀 수 있다.</p>\[G_{t:t+n} \doteq R_{t+1} - \bar{R}_{t+n-1} + \cdots + R_{t+n} - \bar{R}_{t+n-1} + \hat{q}(S_{t+n}, A_{t+n}, \mathbf{w}_{t+n-1})\]<p>$\bar{R}$는 $r(\pi)$의 추정치이며, $n \geq 1$, $t+n &lt; T$이다. $t+n \geq T$이면 $G_{t:t+n} \doteq G_t$이다. differential $n$-step TD error는 아래와 같다.</p>\[\delta_t \doteq G_{t:t+n} - \hat{q}(S_t, A_t, \mathbf{w})\]<p>아래 박스는 전체 알고리즘이다.</p><blockquote><h5 id="textalgorithm-differential-semi-gradient-n-step-sarsa-for-estimating-hatq-approx-q_pi-or-q_ast"><span class="mr-2">$\text{Algorithm: Differential semi-gradient $n$-step Sarsa for estimating $\hat{q} \approx q_\pi$ or $q_\ast$}$</span><a href="#textalgorithm-differential-semi-gradient-n-step-sarsa-for-estimating-hatq-approx-q_pi-or-q_ast" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5><p>\(\begin{align*} &amp; \textstyle \text{Input: a differentiable function $\hat{q} : \mathcal{S} \times \mathcal{A} \times \mathbb{R}^d \rightarrow \mathbb{R}$, a policy $\pi$} \\ &amp; \textstyle \text{Initialize value-function weights $\mathbf{w} \in \mathbb{R}^d$ arbitrarily (e.g., $\mathbf{w} = \mathbf{0}$)} \\ &amp; \textstyle \text{Initialize average-reward estimate $\bar{R} \in \mathbb{R}$ arbitrarily (e.g., $\bar{R} = 0$)} \\ &amp; \textstyle \text{Algorithm parameters: step size $\alpha, \beta &gt; 0$, small $\epsilon &gt; 0$, a positive integer $n$} \\ &amp; \textstyle \text{All store and access operations $(S_t,A_t,R_t)$ can take their index mode $n+1$} \\ \\ &amp; \textstyle \text{Initialize and store $S_0$, and $A_0$} \\ &amp; \textstyle \text{Loop for each step, } t = 0, 1, 2, \ldots : \\ &amp; \textstyle \qquad \text{Take action $A_t$} \\ &amp; \textstyle \qquad \text{Observe and store the next reward as $R_{t+1}$ and the next state as $S_{t+1}$} \\ &amp; \textstyle \qquad \text{Select and store an action $A_{t+1} \sim \pi(\cdot \vert S_{t+1})$, or $\epsilon$-greedy wrt $\hat{q}(S_{t+1}, \cdot, \mathbf{w})$} \\ &amp; \textstyle \qquad \tau \leftarrow t - n + 1 \qquad \text{($\tau$ is the time whose estimate is being updated)} \\ &amp; \textstyle \qquad \text{If $\tau \geq 0$:} \\ &amp; \textstyle \qquad\qquad \delta \leftarrow \sum_{i=\tau+1}^{\tau+n} (R_i - \bar{R}) + \hat{q}(S_{\tau+n}, A_{\tau+n}, \mathbf{w}) - \hat{q}(S_\tau, A_\tau, \mathbf{w}) \\ &amp; \textstyle \qquad\qquad \bar{R} \leftarrow \bar{R} + \beta \delta \\ &amp; \textstyle \qquad\qquad \mathbf{w} \leftarrow \mathbf{w} + \alpha \delta \nabla \hat{q}(S_\tau, A_\tau, \mathbf{w}) \\ \end{align*}\)</p></blockquote><h2 id="references"><span class="mr-2">References</span><a href="#references" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>[1] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction; 2nd Edition. 2020.</p><h2 id="footnotes"><span class="mr-2">Footnotes</span><a href="#footnotes" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><div class="footnotes" role="doc-endnotes"><ol><li id="fn:1" role="doc-endnote"><p>DevSlem. <a href="../finite-markov-decision-processes/#return">Finite Markov Decision Processes. Return</a>. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p></ol></div></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/reinforcement-learning/'>Reinforcement Learning</a>, <a href='/categories/rl-fundamental/'>RL Fundamental</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/rl/" class="post-tag no-text-decoration" >RL</a> <a href="/tags/ai/" class="post-tag no-text-decoration" >AI</a> <a href="/tags/function-approximation-rl/" class="post-tag no-text-decoration" >Function Approximation RL</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=On-policy+Control+with+Approximation+-+DevSlem+Blog&url=https%3A%2F%2Fdevslem.github.io%2Freinforcement-learning%2Frl-fundamental%2Fon-policy-control-with-approximation%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=On-policy+Control+with+Approximation+-+DevSlem+Blog&u=https%3A%2F%2Fdevslem.github.io%2Freinforcement-learning%2Frl-fundamental%2Fon-policy-control-with-approximation%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fdevslem.github.io%2Freinforcement-learning%2Frl-fundamental%2Fon-policy-control-with-approximation%2F&text=On-policy+Control+with+Approximation+-+DevSlem+Blog" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div><script src="https://utteranc.es/client.js" repo="DevSlem/DevSlem.github.io" issue-term="pathname" label="comment" theme="github-dark" crossorigin="anonymous" async> </script></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/reinforcement-learning/drl-methods/dqn/">DQN: Deep Q-Networks</a><li><a href="/reinforcement-learning/drl-methods/dtqn/">DTQN: Deep Transformer Q-Networks for Partially Observable Reinforcement Learning</a><li><a href="/wsl-setup/">Windows Subsystem for Linux (WSL) Setup for Reinforcement Learning</a><li><a href="/algorithm/sorting/merge-sort/">[알고리즘] 합병 정렬</a><li><a href="/algorithm/sorting/shell-sort/">[알고리즘] 쉘 정렬</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/rl/">RL</a> <a class="post-tag" href="/tags/algorithm/">Algorithm</a> <a class="post-tag" href="/tags/unity/">Unity</a> <a class="post-tag" href="/tags/function-approximation-rl/">Function Approximation RL</a> <a class="post-tag" href="/tags/sorting/">Sorting</a> <a class="post-tag" href="/tags/enemy/">Enemy</a> <a class="post-tag" href="/tags/game-development/">Game Development</a> <a class="post-tag" href="/tags/project/">Project</a> <a class="post-tag" href="/tags/attribute/">Attribute</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 pl-3 pr-3 pr-xl-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/reinforcement-learning/rl-fundamental/policy-gradient-methods/"><div class="card-body"> <em class="small" data-ts="1665586800" data-df="ll" > Oct 13, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Policy Gradient Methods</h3><div class="text-muted small"><p> 드디어 긴 장정 끝에 대망의 마지막 챕터로 왔다. 이번 포스트에서는 그 유명한 policy gradient method에 대해 소개하려고 한다. 길고 긴 여정이 드디어 끝났다. Introduction 지금까지 우리는 value-based 기반의 방법을 다뤘었다. policy는 추정된 action value에 기반해 action을 선택했다. 이번에는...</p></div></div></a></div><div class="card"> <a href="/reinforcement-learning/rl-fundamental/on-policy-prediction-with-approximation/"><div class="card-body"> <em class="small" data-ts="1660748400" data-df="ll" > Aug 18, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>On-policy Prediction with Approximation</h3><div class="text-muted small"><p> 이 포스트에서는 reinforcement learning을 수행하는 새로운 방법인 function approximation에 대한 소개와 이를 바탕으로 on-policy method에서 prediction을 수행하는 방법을 소개한다. What is Function Approximation and Why needed? 지금까지 알아본 기존 Reinf...</p></div></div></a></div><div class="card"> <a href="/reinforcement-learning/rl-fundamental/off-policy-methods-with-approximation/"><div class="card-body"> <em class="small" data-ts="1661785200" data-df="ll" > Aug 30, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Off-policy Methods with Approximation</h3><div class="text-muted small"><p> 이 포스트에서는 on-policy function approximation을 off-policy로의 확장과 이로 인해 발생하는 문제들에 대해 다룰 것이다. Introduction off-policy method는 behavior policy $b$에 의해 생성된 experience로부터 target policy $\pi$에 대한 value func...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/reinforcement-learning/rl-fundamental/on-policy-prediction-with-approximation/" class="btn btn-outline-primary" prompt="Older"><p>On-policy Prediction with Approximation</p></a> <a href="/reinforcement-learning/rl-fundamental/off-policy-methods-with-approximation/" class="btn btn-outline-primary" prompt="Newer"><p>Off-policy Methods with Approximation</p></a></div></div></div><footer class="row pl-3 pr-3"><div class="col-12 d-flex justify-content-between align-items-center text-muted pl-0 pr-0"><div class="footer-left"><p class="mb-0"> © 2024 <a href="https://github.com/username">DevSlem</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/rl/">RL</a> <a class="post-tag" href="/tags/algorithm/">Algorithm</a> <a class="post-tag" href="/tags/unity/">Unity</a> <a class="post-tag" href="/tags/function-approximation-rl/">Function Approximation RL</a> <a class="post-tag" href="/tags/sorting/">Sorting</a> <a class="post-tag" href="/tags/enemy/">Enemy</a> <a class="post-tag" href="/tags/game-development/">Game Development</a> <a class="post-tag" href="/tags/project/">Project</a> <a class="post-tag" href="/tags/attribute/">Attribute</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a><div id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-animation="true" data-autohide="false"><div class="toast-header"> <button type="button" class="ml-2 ml-auto close" data-dismiss="toast" aria-label="Close"> <span aria-hidden="true">&times;</span> </button></div><div class="toast-body text-center pt-0"><p class="pl-2 pr-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></div><script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] }, svg: { scale: 1.1 } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script>
