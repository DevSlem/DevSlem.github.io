<!DOCTYPE html><html lang="en" data-mode="dark" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="Off-policy Methods with Approximation" /><meta property="og:locale" content="en" /><meta name="description" content="이 포스트에서는 on-policy function approximation을 off-policy로의 확장과 이로 인해 발생하는 문제들에 대해 다룰 것이다." /><meta property="og:description" content="이 포스트에서는 on-policy function approximation을 off-policy로의 확장과 이로 인해 발생하는 문제들에 대해 다룰 것이다." /><link rel="canonical" href="https://devslem.github.io/reinforcement-learning/rl-fundamental/off-policy-methods-with-approximation/" /><meta property="og:url" content="https://devslem.github.io/reinforcement-learning/rl-fundamental/off-policy-methods-with-approximation/" /><meta property="og:site_name" content="DevSlem Blog" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-08-30T00:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Off-policy Methods with Approximation" /><meta name="google-site-verification" content="SSJwUK1JSPZV3wjQS19f2bXfHYIupbYEFsisH4Nuns4" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-08-31T00:00:00+09:00","datePublished":"2022-08-30T00:00:00+09:00","description":"이 포스트에서는 on-policy function approximation을 off-policy로의 확장과 이로 인해 발생하는 문제들에 대해 다룰 것이다.","headline":"Off-policy Methods with Approximation","mainEntityOfPage":{"@type":"WebPage","@id":"https://devslem.github.io/reinforcement-learning/rl-fundamental/off-policy-methods-with-approximation/"},"url":"https://devslem.github.io/reinforcement-learning/rl-fundamental/off-policy-methods-with-approximation/"}</script><title>Off-policy Methods with Approximation | DevSlem Blog</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="DevSlem Blog"><meta name="application-name" content="DevSlem Blog"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> </a></div><div class="site-title mt-3"> <a href="/">DevSlem Blog</a></div><div class="site-subtitle font-italic">It's a blog where i write posts about AI, RL, Unity, etc.</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/DevSlem" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['devslem12','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper"><div id="topbar" class="container d-flex align-items-center justify-content-between h-100 pl-3 pr-3 pl-md-4 pr-md-4"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Off-policy Methods with Approximation</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper" class="d-flex justify-content-center"><div id="main" class="container pl-xl-4 pr-xl-4"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pr-xl-4"><div class="post pl-1 pr-1 pl-md-2 pr-md-2"><h1 data-toc-skip>Off-policy Methods with Approximation</h1><div class="post-meta text-muted"> <span> Posted <em class="" data-ts="1661785200" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Aug 30, 2022 </em> </span> <span> Updated <em class="" data-ts="1661904000" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Aug 31, 2022 </em> </span><div class="d-flex justify-content-between"> <span> By <em> <a href="https://github.com/username">DevSlem</a> </em> </span><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2033 words"> <em>11 min</em> read</span></div></div></div><div class="post-content"><p>이 포스트에서는 on-policy function approximation을 off-policy로의 확장과 이로 인해 발생하는 문제들에 대해 다룰 것이다.</p><h2 id="introduction"><span class="mr-2">Introduction</span><a href="#introduction" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>off-policy method는 <em>behavior policy</em> $b$에 의해 생성된 experience로부터 target policy $\pi$에 대한 value function을 학습하는 방법이다. 반대로 on-policy method는 behavior policy와 target policy가 동일한 방법이다. off-policy method에서는 일반적으로 target policy $\pi$는 greedy policy이고, behavior policy $b$는 좀 더 효과적으로 exploration을 수행할 수 있는 policy (e.g., $\epsilon$-greedy policy) 이다.</p><p>off-policy method의 문제는 크게 두가지로 나뉜다. 첫 번째는 update target (target policy 아님)과 관련된 이슈로, 이전 tabular off-policy method에서 다뤘었던 importance sampling과 관련되어 있다. 두 번째는 update distribution과 관련된 이슈로, update distribution이 on-policy distribution을 따르지 않는 다는 점이다. on-policy distribution은 semi-gradient method의 안정성에 있어 중요하다. 이 두 이슈에 대해 이번 포스트에서 다룰 것이다.</p><h2 id="semi-gradient-methods"><span class="mr-2">Semi-gradient Methods</span><a href="#semi-gradient-methods" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>이 섹션에서는 off-policy method를 간략히 semi-gradient method로 확장한다. 이 확장된 방법은 off-policy method의 첫 번째 이슈에 대해서만 다루며, 수렴이 안되어 발산할 때도 있다.</p><p>$n$-step tabular method에서 사용했던 방법을 단순히 function approximation을 사용한 weight vector $\mathbf{w}$에 대한 update로 변경한다. off-policy method는 target policy와 behavior policy의 distribution이 다르기 때문에 importance sampling 기법을 사용한다.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> 그 중 off-policy TD(0)와 같은 알고리즘은 per-step importance sampling ratio를 사용한다.</p>\[\rho_t \doteq \rho_{t:t} = \dfrac{\pi(A_t \vert S_t)}{b(A_t \vert S_t)}\]<p>아래는 off-policy semi-gradient TD(0)로 <a href="../on-policy-prediction-with-approximation/#textalgorithm-semi-gradient-td0-for-estimating--hatv-approx-v_pi">semi-gradient on-policy TD(0)</a>에 $\rho_t$만 추가되었다.</p>\[\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \rho_t \delta_t \nabla \hat{v}(S_t, \mathbf{w}_t)\]<p>$\delta_t$는 TD error로 아래는 각각 episodic, discounted setting과 average reward를 사용하는 continuing, undiscounted setting에서의 TD error이다.</p>\[\begin{align*} &amp; \delta_t \doteq R_{t+1} + \gamma \hat{v}(S_{t+1}, \mathbf{w}_t) - \hat{v}(S_t, \mathbf{w}_t) \text{, or} \tag{episodic} \\ &amp; \delta_t \doteq R_{t+1} + \bar{R}_t + \hat{v}(S_{t+1}, \mathbf{w}_t) - \hat{v}(S_t, \mathbf{w}_t) \tag{continuing} \end{align*}\]<p>$\bar{R}_t$는 average reward의 추정치이다.<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup></p><p>action value에 대해서도 쉽게 전환할 수 있다. 아래는 off-policy semi-gradient Expected Sarsa이다.</p>\[\begin{align*} &amp; \mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \delta_t \nabla \hat{q}(S_t, A_t, \mathbf{w_t}) \text{, with} \\ \\ &amp; \delta_t \doteq R_{t+1} + \gamma \sum_a \pi(a \vert S_{t+1}) \hat{q}(S_{t+1}, a, \mathbf{w}_t) - \hat{q}(S_t, A_t, \mathbf{w}_t) \text{, or} \tag{episodic} \\ &amp; \delta_t \doteq R_{t+1} - \bar{R}_t + \sum_a \pi(a \vert S_{t+1}) \hat{q}(S_{t+1}, a, \mathbf{w}_t) - \hat{q}(S_t, A_t, \mathbf{w}_t) \tag{continuing} \end{align*}\]<p>이 알고리즘은 importance sampling을 사용하지 않는다. tabular case에서는 action value에 대한 1-step TD method에 importance sampling을 사용하지 않는 이유가 명확했다.<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup> 그러나 function approximation에서는 명확하지 않다. 그 이유를 Sutton 책에서는 아래와 같이 설명한다. (솔직히 내가 이해 못했음)</p><blockquote><p>With function approximation it is less clear because we might want to weight different state-action pairs differently once they all contribute to the same overall approximation.<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup></p></blockquote><p>아래는 off-policy semi-gradient Sarsa의 $n$-step version이다.</p>\[\begin{align*} &amp; \mathbf{w}_{t+n} \doteq \mathbf{w}_{t+n-1} + \alpha \rho_{t+1} \cdots \rho_{t+n} [G_{t:t+n} - \hat{q}(S_t, A_t, \mathbf{w}_{t+n-1})] \nabla \hat{q}(S_t,A_t,\mathbf{w}_{t+n-1}) \text{, with} \\ \\ &amp; G_{t:t+n} \doteq R_{t+1} + \cdots + \gamma^{n-1}R_{t+n} + \gamma^n \hat{q}(S_{t+n}, A_{t+n}, \mathbf{w}_{t+n-1}) \text{, or} \tag{episodic} \\ &amp; G_{t:t+n} \doteq R_{t+1} - \bar{R}_t + \cdots + R_{t+n} - \bar{R}_{t+n-1} + \hat{q}(S_{t+n}, A_{t+n}, \mathbf{w}_{t+n-1}) \tag{continuing} \end{align*}\]<p>위 첫 번째 수식에서 $\rho_k$는 $k \geq T$ ($T$는 episode의 terminal step) 일 때 $1$이다. $t + n \geq T$일 때 $G_{t:t+n} \doteq G_t$이다.</p><blockquote class="prompt-warning"><div><p>세 번째 수식에서 <a href="../on-policy-control-with-approximation/#differential-semi-gradient-n-step-sarsa">on-policy semi-gradient $n$-step Sarsa</a>의 경우 average reward가 오직 $\bar{R}_{t+n-1}$이었는데 여기서는 다르다. off-policy 여서 다른 건지, 책이 오류인 건지 모르겠다.</p></div></blockquote><h2 id="the-deadly-triad"><span class="mr-2">The Deadly Triad</span><a href="#the-deadly-triad" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>off-policy learning에서 발생하는 불안정성과 발산 이슈는 아래 3가지 요소를 모두 결합할 때 발생한다. 이를 <em>the deadly triad</em>라고 부른다.</p><ol><li>Function approximation<li>Bootstrapping<li>Off-policy learning</ol><p>the deadly triad 중 2가지 요소만 결합한다면 불안정성 이슈를 피할 수 있다. 따라서 위 3개 중 하나를 포기할 수 있다면 포기하는 것이 바람직하다.</p><p>먼저, <em>function approximation</em>은 당연히 포기할 수 없다. function approximation은 매우 큰 state space를 효과적으로 처리할 수 있는 강력한 도구이기 때문이다.</p><p><em>bootstrapping</em> 없이 학습이 가능하다는 것은 이미 알려진 사실이다. 가장 대표적인 게 Monte Carlo (MC) Method이다. 그러나 bootstrapping method는 MC method에 비해 훨씬 강력하다. online으로 학습할 수 있기 때문에 훨씬 빠르고, 데이터를 효율적으로 사용하며, continuing task에 적용 가능하다. 또한 MC method는 일반적인 bootstrapping method에 비해 분산이 훨씬 크다. 따라서 bootstrapping 역시 포기하기는 어렵다.</p><p>마지막으로 <em>off-policy learning</em>이다. 포기할 수 있을까? off-policy method는 target policy와 behavior policy가 다르다. 이는 어디까지나 편리한 거지, 반드시 필요한 것은 아니다. 그러나 더 큰 목표를 가진 강력한 AI를 만들기 위해서는 off-policy learning은 필수적이다. agent가 하나의 behavior policy로부터 선택된 action을 바탕으로 생성된 experience를 통해 여러 개의 target policy를 병렬적으로 학습할 수 있기 때문이다.</p><h2 id="linear-value-function-geometry"><span class="mr-2">Linear Value-function Geometry</span><a href="#linear-value-function-geometry" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>이 섹션에서는 value function approximation을 조금 더 추상적으로 이해해 볼 것이다. 먼저, state-value function $v : \mathcal{S} \rightarrow \mathbb{R}$이 있다고 하자. 또한 function approximation을 사용하는 대부분의 경우, state 개수보다 weight vector $\mathbf{w}$의 weight 개수가 훨씬 작다.</p><p>예를 들어 3개의 state $\mathcal{S} = \{ s_1, s_2, s_3 \}$와 2개의 weight $\mathbf{w} = (w_1, w_2)^\top$가 있다고 하자. 이 경우 value function $v = [v(s_1), v(s_2), v(s_3)]^\top$를 3차원 공간 내의 점들로 표현되며, weight vector는 하위 공간인 2차원 공간으로 표현된다. 아래는 이에 대한 그림이다.</p><p><img data-src="/assets/images/rl-sutton-geometry-of-linear-value-function-approximation.png" alt="" data-proofer-ignore> <em>Fig 1. The geometry of linear value-function approximation.<br /> (Image source: Sec 11.4 Sutton &amp; Barto (2020).)</em></p><p>위 그림에서 true value function $v_\pi$는 weight vector $\mathbf{w}$에 의해 표현되는 2차원 공간보다 더 큰 공간에 (여기서는 3차원) 표현된다.</p><p>어떤 고정된 policy $\pi$를 고려해보자. true value function $v_\pi$는 위 그림처럼 매우 복잡해서 weight vector $\mathbf{w}$로 정확히 근사하는 것은 불가능하다. 즉, $v_\pi$는 하위 공간에 존재하지 않는다. 그렇다면 표현 가능한 value function 중 어떤 것이 true value function에 가까울까?</p><p>먼저, 두 value function 사이의 거리를 측정할 필요가 있다. 두 value function $v_1$, $v_2$가 주어졌졌으며 $v = v_1 - v_2$라고 하자. 또한 특정 state를 조금 더 정확히 추정하기 위해 그 state에 얼마나 집중할 지를 나타내는 state distribution $\mu : \mathcal{S} \rightarrow [0,1]$를 (주로 on-policy distribution이 사용됨) 사용하자. 이 때 value function 사이의 distance를 아래와 같이 norm을 사용해 정의할 수 있다.</p>\[\big\lVert v \big\rVert_\mu^2 \doteq \sum_{s \in \mathcal{S}} \mu(s) v(s)^2\]<p>위 정의를 사용할 때 mean square value error $\overline{\text{VE}}(\mathbf{w}) \doteq \sum_{s \in \mathcal{S}}\mu(s)\Big[v_\pi(s) - \hat{v}(s,\mathbf{w}) \Big]^2$를 $\overline{\text{VE}}(\mathbf{w}) = \lVert v_{\mathbf{w}} - v_\pi \rVert_\mu^2$와 같은 단순한 형태로 나타낼 수 있다.<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup> 어떤 value function의 하위 공간에서의 가장 가까운 표현 가능한 value function은 projection 연산 $\Pi$을 통해 찾을 수 있다.</p>\[\Pi v \doteq v_\mathbf{w} \text{ where } \mathbf{w} = \underset{\mathbf{w} \in \mathbb{R}^d}{\arg\min} \ \big\lVert v - v_\mathbf{w} \big\rVert_\mu^2\]<p>true value function $v_\pi$에 가장 가까운 표현 가능한 value function은 $v_\pi$의 projection $\Pi v_\pi$로 Fig 1에서 확인할 수 있다.</p><p>여기서는 이정도 컨셉만 이해하고 넘어간다. 더 자세한 내용은 여기서 소개하기에는 너무 복잡해 궁금하다면 Sutton 책을 참조하기 바란다.<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">6</a></sup></p><h2 id="references"><span class="mr-2">References</span><a href="#references" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>[1] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction; 2nd Edition. 2020.</p><h2 id="footnotes"><span class="mr-2">Footnotes</span><a href="#footnotes" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><div class="footnotes" role="doc-endnotes"><ol><li id="fn:1" role="doc-endnote"><p>DevSlem. <a href="../monte-carlo-methods/#importance-sampling">Monte Carlo Methods in RL. Importance Sampling</a>. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:2" role="doc-endnote"><p>DevSlem. <a href="../on-policy-control-with-approximation/#conversion-to-average-reward-setting">On-policy Control with Approximation. Average Reward: A New Problem Setting for Continuing Tasks. Conversion to Average Reward Setting</a>. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:3" role="doc-endnote"><p>DevSlem. <a href="../n-step-bootstrapping/#n-step-off-policy-learning">n-step Bootstrapping. $n$-step Off-policy Learning</a>. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:4" role="doc-endnote"><p>Reinforcement Learning: An Introduction; 2nd Edition. 2020. Sec. 11.1, p.259. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:5" role="doc-endnote"><p>DevSlem. <a href="../on-policy-prediction-with-approximation/#the-prediction-objective-overlinetextve">On-policy Prediction with Approximation. The Prediction Objective ($\overline{\text{VE}}$)</a>. <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:6" role="doc-endnote"><p>Reinforcement Learning: An Introduction; 2nd Edition. 2020. Sec. 11.4, p.266. <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p></ol></div></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/reinforcement-learning/'>Reinforcement Learning</a>, <a href='/categories/rl-fundamental/'>RL Fundamental</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/rl/" class="post-tag no-text-decoration" >RL</a> <a href="/tags/ai/" class="post-tag no-text-decoration" >AI</a> <a href="/tags/function-approximation-rl/" class="post-tag no-text-decoration" >Function Approximation RL</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Off-policy+Methods+with+Approximation+-+DevSlem+Blog&url=https%3A%2F%2Fdevslem.github.io%2Freinforcement-learning%2Frl-fundamental%2Foff-policy-methods-with-approximation%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Off-policy+Methods+with+Approximation+-+DevSlem+Blog&u=https%3A%2F%2Fdevslem.github.io%2Freinforcement-learning%2Frl-fundamental%2Foff-policy-methods-with-approximation%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fdevslem.github.io%2Freinforcement-learning%2Frl-fundamental%2Foff-policy-methods-with-approximation%2F&text=Off-policy+Methods+with+Approximation+-+DevSlem+Blog" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div><script src="https://utteranc.es/client.js" repo="DevSlem/DevSlem.github.io" issue-term="pathname" label="comment" theme="github-dark" crossorigin="anonymous" async> </script></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/reinforcement-learning/drl-methods/dqn/">DQN: Deep Q-Networks</a><li><a href="/reinforcement-learning/drl-methods/dtqn/">DTQN: Deep Transformer Q-Networks for Partially Observable Reinforcement Learning</a><li><a href="/wsl-setup/">Windows Subsystem for Linux (WSL) Setup for Reinforcement Learning</a><li><a href="/algorithm/sorting/merge-sort/">[알고리즘] 합병 정렬</a><li><a href="/algorithm/sorting/shell-sort/">[알고리즘] 쉘 정렬</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/rl/">RL</a> <a class="post-tag" href="/tags/algorithm/">Algorithm</a> <a class="post-tag" href="/tags/unity/">Unity</a> <a class="post-tag" href="/tags/function-approximation-rl/">Function Approximation RL</a> <a class="post-tag" href="/tags/sorting/">Sorting</a> <a class="post-tag" href="/tags/enemy/">Enemy</a> <a class="post-tag" href="/tags/game-development/">Game Development</a> <a class="post-tag" href="/tags/project/">Project</a> <a class="post-tag" href="/tags/attribute/">Attribute</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 pl-3 pr-3 pr-xl-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/reinforcement-learning/rl-fundamental/policy-gradient-methods/"><div class="card-body"> <em class="small" data-ts="1665586800" data-df="ll" > Oct 13, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Policy Gradient Methods</h3><div class="text-muted small"><p> 드디어 긴 장정 끝에 대망의 마지막 챕터로 왔다. 이번 포스트에서는 그 유명한 policy gradient method에 대해 소개하려고 한다. 길고 긴 여정이 드디어 끝났다. Introduction 지금까지 우리는 value-based 기반의 방법을 다뤘었다. policy는 추정된 action value에 기반해 action을 선택했다. 이번에는...</p></div></div></a></div><div class="card"> <a href="/reinforcement-learning/rl-fundamental/on-policy-prediction-with-approximation/"><div class="card-body"> <em class="small" data-ts="1660748400" data-df="ll" > Aug 18, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>On-policy Prediction with Approximation</h3><div class="text-muted small"><p> 이 포스트에서는 reinforcement learning을 수행하는 새로운 방법인 function approximation에 대한 소개와 이를 바탕으로 on-policy method에서 prediction을 수행하는 방법을 소개한다. What is Function Approximation and Why needed? 지금까지 알아본 기존 Reinf...</p></div></div></a></div><div class="card"> <a href="/reinforcement-learning/rl-fundamental/on-policy-control-with-approximation/"><div class="card-body"> <em class="small" data-ts="1661180400" data-df="ll" > Aug 23, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>On-policy Control with Approximation</h3><div class="text-muted small"><p> 이 포스트에서는 function approximation을 사용한 prediction을 control로 확장할 것이다. 이를 위해 state-value function이 아닌 action-value function을 추정한다. 그 후 on-policy GPI의 일반적인 패턴을 따라 학습을 진행하는 방법을 알아본다. 이 포스트에서는 특히 semi-gra...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/reinforcement-learning/rl-fundamental/on-policy-control-with-approximation/" class="btn btn-outline-primary" prompt="Older"><p>On-policy Control with Approximation</p></a> <a href="/reinforcement-learning/rl-fundamental/eligibility-traces/" class="btn btn-outline-primary" prompt="Newer"><p>Eligibility Traces</p></a></div></div></div><footer class="row pl-3 pr-3"><div class="col-12 d-flex justify-content-between align-items-center text-muted pl-0 pr-0"><div class="footer-left"><p class="mb-0"> © 2024 <a href="https://github.com/username">DevSlem</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/rl/">RL</a> <a class="post-tag" href="/tags/algorithm/">Algorithm</a> <a class="post-tag" href="/tags/unity/">Unity</a> <a class="post-tag" href="/tags/function-approximation-rl/">Function Approximation RL</a> <a class="post-tag" href="/tags/sorting/">Sorting</a> <a class="post-tag" href="/tags/enemy/">Enemy</a> <a class="post-tag" href="/tags/game-development/">Game Development</a> <a class="post-tag" href="/tags/project/">Project</a> <a class="post-tag" href="/tags/attribute/">Attribute</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a><div id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-animation="true" data-autohide="false"><div class="toast-header"> <button type="button" class="ml-2 ml-auto close" data-dismiss="toast" aria-label="Close"> <span aria-hidden="true">&times;</span> </button></div><div class="toast-body text-center pt-0"><p class="pl-2 pr-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></div><script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] }, svg: { scale: 1.1 } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script>
