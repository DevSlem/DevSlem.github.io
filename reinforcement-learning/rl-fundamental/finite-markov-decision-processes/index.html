<!DOCTYPE html><html lang="en" data-mode="dark" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="Finite Markov Decision Processes" /><meta property="og:locale" content="en" /><meta name="description" content="강화학습의 기반이 되는 finite MDPs와 Bellman equations에 대해 소개한다." /><meta property="og:description" content="강화학습의 기반이 되는 finite MDPs와 Bellman equations에 대해 소개한다." /><link rel="canonical" href="https://devslem.github.io/reinforcement-learning/rl-fundamental/finite-markov-decision-processes/" /><meta property="og:url" content="https://devslem.github.io/reinforcement-learning/rl-fundamental/finite-markov-decision-processes/" /><meta property="og:site_name" content="DevSlem Blog" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-05-30T00:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Finite Markov Decision Processes" /><meta name="google-site-verification" content="SSJwUK1JSPZV3wjQS19f2bXfHYIupbYEFsisH4Nuns4" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-05-30T00:00:00+09:00","datePublished":"2022-05-30T00:00:00+09:00","description":"강화학습의 기반이 되는 finite MDPs와 Bellman equations에 대해 소개한다.","headline":"Finite Markov Decision Processes","mainEntityOfPage":{"@type":"WebPage","@id":"https://devslem.github.io/reinforcement-learning/rl-fundamental/finite-markov-decision-processes/"},"url":"https://devslem.github.io/reinforcement-learning/rl-fundamental/finite-markov-decision-processes/"}</script><title>Finite Markov Decision Processes | DevSlem Blog</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="DevSlem Blog"><meta name="application-name" content="DevSlem Blog"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> </a></div><div class="site-title mt-3"> <a href="/">DevSlem Blog</a></div><div class="site-subtitle font-italic">It's a blog where i write posts about AI, RL, Unity, etc.</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/DevSlem" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['devslem12','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper"><div id="topbar" class="container d-flex align-items-center justify-content-between h-100 pl-3 pr-3 pl-md-4 pr-md-4"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Finite Markov Decision Processes</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper" class="d-flex justify-content-center"><div id="main" class="container pl-xl-4 pr-xl-4"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pr-xl-4"><div class="post pl-1 pr-1 pl-md-2 pr-md-2"><h1 data-toc-skip>Finite Markov Decision Processes</h1><div class="post-meta text-muted"> <span> Posted <em class="" data-ts="1653836400" data-df="ll" data-toggle="tooltip" data-placement="bottom"> May 30, 2022 </em> </span> <span> Updated <em class="" data-ts="1653868800" data-df="ll" data-toggle="tooltip" data-placement="bottom"> May 30, 2022 </em> </span><div class="d-flex justify-content-between"> <span> By <em> <a href="https://github.com/username">DevSlem</a> </em> </span><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="3433 words"> <em>19 min</em> read</span></div></div></div><div class="post-content"><p>이 포스트에서는 Reinforcement Learning에서 기반이 되는 finite Markov Decision Processes (MDPs)와 finite MDPs 문제를 해결하기 위한 Bellman equations에 대해 소개한다.</p><h2 id="what-is-mdps"><span class="mr-2">What is MDPs</span><a href="#what-is-mdps" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p><strong><em>Markov Decision Processes</em> (MDPs)는 연속적인 의사 결정을 형식화</strong>한 프레임이다. MDPs와 Multi-armed bandits 환경의 가장 큰 차이점은 MDPs에서는 선택한 action들이 environment의 states를 변경시켜 future rewards에 영향을 미친다는 점이다. 즉, actions와 states가 연관성이 있는 <em>associative</em> setting이다. MDPs는 $(\mathcal{S}, \mathcal{A}, P, R)$로 구성되며 각 요소는 아래와 같다.</p><ul><li>$\mathcal{S}$ - a set of states<li>$\mathcal{A}$ - a set of actions<li>$P$ - state-transition probability function<li>$R$ - reward function</ul><p>MDPs에서는 앞서 말했듯이 actions가 immediate rewards 뿐만 아니라 이후의 states들과 future rewards에 영향을 미친다. 그렇기 때문에 MDPs에서는 immediate rewards와 future rewards 사이에 tradeoff를 할 필요가 있다.</p><p>MDPs에서 learner이자 decision maker를 <em>agent</em>라고 하며, agent가 상호작용하는 agent 외의 모든 요소를 <em>environment</em>라고 한다. decision making은 agent가 action을 선택하는 행위이다. discrete time steps $t = 0, 1, 2, 3, \dots$이 있을 때 각 time step $t$에서 agent는 environment의 <em>state</em> $S_t \in \mathcal{S}$에서 <em>action</em> $A_t \in \mathcal{A}(s)$를 선택한다. 그러면 새로운 state $S_{t+1}$로 전이되고 environment로부터 numerical <em>reward</em> $R_{t+1} \in \mathcal{R} \subset \mathbb{R}$을 획득한다. 아래는 MDP에 대한 그림이다.</p><p><img data-src="/assets/images/rl-sutton-figure3.1.png" alt="" width="60%" data-proofer-ignore> <em>Fig 1. The agent–environment interaction in a Markov decision process.<br /> (Image source: Sec. 3.1 Sutton &amp; Barto (2018).)</em></p><p>위 과정은 매 time step 마다 끊임없이 반복되며 MDP와 agent는 아래와 같은 sequence 혹은 <em>trajectory</em>를 생성한다.</p>\[S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, \dots\]<p>이러한 <strong>states, actions, rewards ($\mathcal{S}$, $\mathcal{A}$, $\mathcal{R}$) 모두 유한할 때 <em>finite</em> MDP</strong>라고 한다. 이 때 $R_t$와 $S_t$는 잘 정의된 discrete probability distribution으로 이전의 모든 state나 action들이 아닌 <strong>오직 직전 state $S_{t-1}$과 action $A_{t-1}$에만 의존</strong>하며 $S_{t-1}$과 $A_{t-1}$이 주어졌을 때 $S_t$와 $R_t$가 발생할 확률 $p$를 정의할 수 있다.</p>\[p(s', r \vert s, a) \doteq \text{Pr}\lbrace S_t = s', R_t = r \ \vert \ S_{t-1} = s, A_{t-1} = a \rbrace\]<p>위 function $p$는 MDP의 <em>dynamics</em>를 정의한다. state는 미래에 대한 차이를 만들어내는 과거의 agent-environment interaction에 대한 모든 측면의 정보를 포함해야하며, 이 때 state는 <em>Markov property</em>를 가진다고 말한다.</p><p>위에서 정의한 dynamics $p$로부터 <em>state-transition probability</em>를 유도할 수 있다.</p>\[p(s' \vert s, a) \doteq \text{Pr}\lbrace S_t = s' \ \vert \ S_{t-1} = s, A_{t-1} = a \rbrace = \sum_{r \in \mathcal{R}}p(s', r \vert s, a)\]<p>또한 state-action pair에 대한 expected reward를 계산할 수 있다.</p>\[r(s, a) \doteq \mathbb{E}[R_t \ \vert \ S_{t-1} = s, A_{t-1} = a] = \sum_{r \in \mathcal{R}}r\sum_{s' \in \mathcal{S}}p(s', r \vert s, a)\]<p>state-action-next-state에 대한 expected reward 역시 계산할 수 있다.</p>\[r(s, a, s') \doteq \mathbb{E}[R_t \ \vert \ S_{t-1} = s, A_{t-1} = a, S_t = s'] = \sum_{r \in \mathcal{R}}r\dfrac{p(s', r \vert s, a)}{p(s' \vert s, a)}\]<p>이 포스트에서는 위 수식 중 dynamics $p(s’, r \vert s, a)$를 주로 사용하였다.</p><h2 id="goals-in-rl"><span class="mr-2">Goals in RL</span><a href="#goals-in-rl" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Reinforcement Learning (RL)에서 agent는 immediate reward가 아닌 오랜 기간에 걸친 <strong>cumulative reward를 maximize하는 것을 목표</strong>로 한다. reward는 우리가 달성하고자 하는 것을 나타내는 중요한 지표이다. 주의할 점은 이 reward signal을 설정할 때 <em>how</em>가 아닌 <em>what</em>의 관점으로 설정해야한다. 달성하고자 하는 목표가 무엇인지에 초점을 맞추되 이것을 달성하기 위한 지식을 제시해서는 안된다.</p><h2 id="episode"><span class="mr-2">Episode</span><a href="#episode" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>배틀그라운드라는 게임을 생각해보자. 이 게임은 배틀로얄 장르로 매치 시작 시 비행기에서 낙하 후 총기를 비롯한 아이템을 파밍해 전투를 펼치는 게임이다. 각 매치는 싱글플레이 기준 매치 도중 사망하거나 적이 전부 사망해 홀로 생존 시 종료되며 다시 매치 시작 시 이전 매치에서 획득한 총기, 아이템 등은 전부 초기화된다. 각 매치는 사망 혹은 홀로 생존과 같이 <em>terminal state</em>가 존재하는데 게임 내 모든 상호작용을 하나의 sequence라고 볼 때 매치 단위의 subsequence로 쪼갤 수 있다. 이러한 subsequence를 <em>episode</em>라고 하는데 episode는 앞서 언급한 terminal state에 종료된다. terminal state는 주로 게임에서의 승리나 패배와 같다. episode가 terminal state에 도달해 종료되면 다시 처음 state로 초기화되고 새로운 episode가 시작된다. 새로운 episode는 이전 episode와 독립적인 관계이다. 이러한 종류의 episodes를 가진 tasks를 <em>episodic tasks</em>라고 부른다.</p><h2 id="return"><span class="mr-2">Return</span><a href="#return" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>cumulative reward를 수학적으로 정의한 것이 <em>expected return</em> $G_t$이며 이는 time step $t$ 이후에 획득한 rewards sequence $R_{t+1}, R_{t+2}, R_{t+3}, \dots ,$에 대한 함수이다. $G_t$를 구할 때 단순히 rewards sequence의 합으로 구할 수 있지만 이는 episodic tasks에서만 유효하다. terminal state가 존재하지 않는 <em>continuing tasks</em>에서는 무한한 time steps에서 rewards를 획득하기 때문에 $G_t \rightarrow \infty$가 될 것이다. 따라서 episodic tasks 뿐만 아니라 continuing tasks에서도 expected return $G_t$를 구하기 위해 일반적으로 <strong>미래가 고려된 discounted rewards sequence의 합</strong>으로 구한다. 이를 <em>discounted return</em>이라고 하며 수식은 아래와 같다.</p>\[G_t \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots = \sum_{k=t+1}^T \gamma^{k-t-1}R_k\]<p>$T$는 termination time으로 continuing task면 $T = \infty$이다. $0 \leq \gamma \leq 1$는 <em>discount rate</em>로 더 먼 미래에 획득한 reward일수록 더 많이 discount한다.</p><p>위 수식을 연속적인 time steps에서의 return 형태로 변경할 수 있다. 즉, <strong>재귀적</strong>으로 변경할 수 있는데 이는 강화학습 전반에서 굉장히 중요한 수식이다.</p>\[\begin{aligned} G_t &amp;\doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} \cdots \\ &amp;= R_{t+1} + \gamma(R_{t+2} + \gamma R_{t+3} + \gamma^2 R_{t+4} \cdots) \\ &amp;= R_{t+1} + \gamma G_{t+1} \end{aligned}\]<h2 id="value-function"><span class="mr-2">Value Function</span><a href="#value-function" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p><strong><em>value function</em>은 states 혹은 state-action pairs가 얼마나 좋은지를 추정</strong>하는 함수이다. “얼마나 좋은가”는 보통 expected return의 관점에서 정의된다. 미래에 획득할 rewards는 agent가 행동하는 방식에 의존하기 때문에 이에 대한 value functions는 agent의 행동 방식인 policy에 영향을 받는다. <strong><em>policy</em>는 각 state에서 가능한 각 행동들의 선택 확률</strong>로 agent가 time step $t$에서 policy $\pi$를 따를 때, $\pi(a \vert s)$는 $S_t = s$일 때 $A_t = a$일 확률이다.</p><p>agent가 state $s$에 있을 때 agent에게 얼마나 좋은지를 추정하는 <strong><em>state-value function</em> $v_\pi(s)$는 state $s$에서 시작하고 policy $\pi$를 따를 때 얻을 수 있는 expected return</strong>이다. MDPs에서는 $v_\pi$를 아래와 같이 정의할 수 있다.</p>\[v_\pi(s) \doteq \mathbb{E}_\pi[G_t \ \vert \ S_t = s]\]<p>주의할 점은 terminal state의 value는 항상 0이다.</p><p>위와 비슷하게 policy $\pi$에 대한 <strong><em>action-value function</em> $q_\pi$는 state $s$에서 policy $\pi$에 따라 action $a$를 선택했을 때 얻을 수 있는 expected return</strong>이다.</p>\[q_\pi(s, a) \doteq \mathbb{E}_\pi[G_t \ \vert \ S_t = s, A_t = a]\]<p>value function $v_\pi$와 $q_\pi$는 경험으로부터 추정된다. 경험이란 agent가 직접 states에 방문해보고 actions를 선택함으로써 얻게 되는 return과 같은 정보들을 말한다.</p><h2 id="bellman-expectation-equation"><span class="mr-2">Bellman Expectation Equation</span><a href="#bellman-expectation-equation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>state-value function은 expected return $G_t \doteq R_{t+1} + \gamma G_{t+1}$와 같이 <strong>재귀적 관계</strong>를 만족한다. 즉, $v_\pi$를 현재 state value와 후속 state value 사이의 관계로 나타낼 수 있으며 이를 <strong>$v_\pi$에 대한 <em>Bellman expectation equation</em></strong>이라고 한다. 수식은 아래와 같으며 복잡한 증명은 생략한다.</p>\[\begin{aligned} v_\pi(s) &amp;\doteq \mathbb{E}_\pi[G_t \ \vert \ S_t = s] \\ &amp; = \mathbb{E}_\pi[R_{t+1} + \gamma v_\pi(S_{t+1}) \ \vert \ S_t = s] \end{aligned}\]<p>action-value function 역시 위와 마찬가지로 <strong>재귀적 관계</strong>를 나타내는 <strong>$q_\pi$에 대한 Bellman expectation equation</strong>으로 나타낼 수 있다.</p>\[q_\pi(s, a) \doteq \mathbb{E}_\pi[R_{t+1} + \gamma q_\pi(S_{t+1}, A_{t+1}) \ \vert \ S_t = s, A_t = a]\]<p>$v_\pi$에 대한 Bellman expectation equation을 아래와 같은 <em>backup diagram</em>으로 나타낼 수 있다. 참고로 backup diagram인 이유는 후속 states에서의 value로부터 역으로 현재 state에서의 value를 구하는 <em>backup</em> operation 관계를 표현하고 있기 때문이다.</p><p><img data-src="/assets/images/rl-sutton-backup-diagram-for-v.png" alt="" width="40%" data-proofer-ignore> <em>Fig 2. Backup diagram for state-value.<br /> (Image source: Sec. 3.5 Sutton &amp; Barto (2018).)</em></p><p>비어있는 circle은 $v_\pi$, 검은색 circle은 $q_\pi$를 나타낸다. 가장 위의 node는 $v_\pi(s)$로 바로 아래의 $q_\pi(s, a)$를 가리킨다. 즉, $v_\pi$에 대한 Bellman expectation equation은 <strong>어떤 state에서 선택 가능한 각 action들의 action-value $q_\pi$들에 대한 expectation</strong>이다. 따라서 $v_\pi$를 아래와 같은 수식으로 나타낼 수 있다.</p>\[v_\pi(s) \doteq \sum_a\pi(a \vert s)q_\pi(s, a)\]<p>$q_\pi$에 대한 Bellman expectation equation의 backup diagram은 아래와 같다.</p><p><img data-src="/assets/images/rl-sutton-backup-diagram-for-q.png" alt="" width="35%" data-proofer-ignore> <em>Fig 3. Backup diagram for action-value.<br /> (Image source: Sec. 3.5 Sutton &amp; Barto (2018).)</em></p><p>$p$는 MDP의 dynamics로 state $s$에서 action $a$를 선택했을 때 reward $r$과 next state $s’$이 발생할 확률이다. 가장 위의 node는 $q_\pi(s, a)$로 바로 아래의 $v_\pi(s’)$을 가리킨다. 즉, $q_\pi(s, a)$는 <strong>어떤 state $s$에서 action $a$를 선택했을 때 획득한 return들의 expectation</strong>으로 나타낼 수 있다.</p>\[q_\pi(s, a) \doteq \sum_{s', r} p(s', r \vert s, a) \Big[r + \gamma v_\pi(s') \Big]\]<p>위 두 Bellman expectation equation을 바탕으로 $v_\pi$를 아래와 같이 정의할 수 있다.</p>\[v_\pi(s) \doteq \sum_a \pi(a \vert s) \sum_{s', r}p(s', r \vert s, a)\Big[r + \gamma v_\pi(s') \Big]\]<p>지금까지 알아본 내용은 특정 policy $\pi$를 따를 때 value를 추정하는 방법이다. 그러나 이것은 MDP에서 할 수 있는 최적의 방식이 아니다. 어디까지나 특정 policy $\pi$에 대한 value 추정일 뿐이다. 이제 MDP의 문제를 해결하는 방법을 알아보자.</p><h2 id="optimal-value-function"><span class="mr-2">Optimal Value Function</span><a href="#optimal-value-function" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>RL은 cumulative reward를 maximize하는 것이 목표로 한다. 위 Bellman equation의 수식을 보면 policy $\pi$에 따라 expected return을 나타내는 state value가 달라짐을 알 수 있다. 즉, cumulative reward는 policy $\pi$에 의존한다. 따라서 cumulative reward를 maximize하는 <em>optimal policy</em> $\pi_\ast$를 찾는 것이 목적이며 state-value function이 optimal policy를 따를 때 <em>optimal state-value function</em> $v_\ast$라 한다. 이때 $v_\ast$는 <strong>모든 policy에 대해 가장 큰 state-value function</strong>이다.</p>\[v_\ast(s) \doteq \max_\pi v_\pi(s)\]<p>state-value는 expected return을 나타내기 때문에 optimal policy $\pi_\ast$를 따르는 optimal state-value 역시 maximize된다.</p><p>위와 마찬가지로 <em>optimal action-value function</em> $q_\ast$는 <strong>모든 policy에 대해 가장 큰 action-value function</strong>이다.</p>\[q_\ast(s, a) \doteq \max_\pi q_\pi(s, a)\]<h2 id="optimal-policy"><span class="mr-2">Optimal policy</span><a href="#optimal-policy" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>그렇다면 optimal policy $\pi_\ast$를 어떻게 찾을 수 있을까? 어떤 한 policy $\pi$와 다른 policy $\pi’$이 있다고 할 때 모든 states에 대한 $\pi$를 따르는 value function이 모든 states에 대한 $\pi’$을 따르는 value function보다 크거나 같을 때 더 좋은 policy라고 판단할 수 있다. 이를 수식으로 나타내면 아래와 같다.</p>\[\pi \geq \pi' \ \text{if} \ v_\pi(s) \geq v_{\pi'}(s) \ \text{for all} \ s \in \mathcal{S}\]<h2 id="bellman-optimality-equation"><span class="mr-2">Bellman Optimality Equation</span><a href="#bellman-optimality-equation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p><strong><em>Bellman optimality equation</em></strong>은 Bellman expectation equation과 비슷하나 value들에 대한 expectation이 아닌 maximum value만을 고려한다는 차이가 있다. 아래는 optimal state-value function $v_\ast$에 대한 backup diagram이다.</p><p><img data-src="/assets/images/rl-sutton-backup-diagram-for-bellman-optim-v.png" alt="" width="40%" data-proofer-ignore> <em>Fig 4. Backup diagram for optimal state-value.<br /> (Image source: Sec. 3.6 Sutton &amp; Barto (2018).)</em></p><p>가장 위의 node $v_\ast(s)$는 action-value에 대해 maximum value를 선택한다. 즉, $v_\ast$에 대한 Bellman optimality equation은 <strong>optimal policy를 따르는 state value가 그 state에서의 best action에 대한 expected return 혹은 action value와 동일</strong>하다.</p>\[\begin{aligned} v_\ast(s) &amp;= \max_{a \in \mathcal{A}(s)} q_{\pi_\ast}(s, a) \\ \end{aligned}\]<p>아래는 optimal action-value function function $q_\ast$에 대한 backup diagram이다.</p><p><img data-src="/assets/images/rl-sutton-backup-diagram-for-bellman-optim-q.png" alt="" width="40%" data-proofer-ignore> <em>Fig 5. Backup diagram for optimal action-value.<br /> (Image source: Sec. 3.6 Sutton &amp; Barto (2018).)</em></p><p>가장 위의 node는 $q_\ast(s, a)$로 바로 아래의 $v_\ast(s’)$을 가리킨다. $q_\ast$에 대한 Bellman optimality equation은 Bellman expectation equation과 같이 여전히 기댓값을 취하지만 유일한 차이점은 이미 각 states에 대한 optimal values를 알고 있다는 점이다.</p>\[q_\ast(s, a) \doteq \mathbb{E}[R_{t+1} + \gamma v_\ast(S_{t+1}) \ \vert \ S_t = s, A_t = a]\]<p>위 내용을 바탕으로 $v_\ast$와 $q_\ast$에 대한 Bellman optimality equation을 다시 정의할 수 있다. 아래는 $v_\ast$에 대한 Bellman optimality equation을 현재 state와 후속 state 사이의 재귀적 관계로 표현한 수식이다.</p>\[\begin{aligned} v_\ast(s) &amp;= \max_a \mathbb{E}[R_{t+1} + \gamma v_\ast(S_{t+1}) \ \vert \ S_t = s, A_t = a] \\ &amp;= \max_a \sum_{s', r}p(s', r \vert s, a)\Big[r + \gamma v_\ast(s') \Big] \end{aligned}\]<p>마찬가지로 $q_\ast$에 대한 Bellman optimality equation 역시 현재 state-action pair와 후속 state-action pair 사이의 재귀적 관계로 표현할 수 있다.</p>\[\begin{aligned} q_\ast(s, a) &amp;= \mathbb{E} \Big[R_{t+1} + \gamma \max_{a'} q_\ast(S_{t+1}, a') \ \Big\vert \ S_t = s, A_t = a \Big] \\ &amp;= \sum_{s', r}p(s', r \vert s, a) \Big[r + \gamma \max_{a'}q_\ast(s', a') \Big] \end{aligned}\]<p>Bellman optimality equation을 풀면 RL의 목적인 optimal policy를 찾을 수 있다. 그러나 이 방법은 실제로 유용하지 않다. Bellman optimality equation을 푸는 행위는 exhaustive search와 유사한 행위이다. RL에서는 environment의 가능한 states가 계산이 불가능한 영역 수준으로 많다. 가장 대표적인 예시가 그 유명한 AlphaGo의 바둑이다. 바둑의 경우의 수는 계산 불가능의 영역이다. 그럼에도 AlphaGo가 성공했던 이유는 RL의 기반인 Bellman optimality equation을 근사적으로 잘 풀어냈기 때문이다.</p><h2 id="references"><span class="mr-2">References</span><a href="#references" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>[1] Richard S. Sutton and Andrew G. Barto. <a href="/assets/materials/Reinforcement%20Learning%20An%20Introduction;%202nd%20Edition.%202018.pdf">Reinforcement Learning: An Introduction; 2nd Edition. 2018</a>.<br /> [2] Towards Data Science. blackburn. <a href="https://towardsdatascience.com/reinforcement-learning-markov-decision-process-part-2-96837c936ec3">Reinforcement Learning: Bellman Equation and Optimality (Part 2)</a>.<br /> [3] Wikipedia. <a href="https://en.wikipedia.org/wiki/Markov_decision_process">Markov decision process</a>.</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/reinforcement-learning/'>Reinforcement Learning</a>, <a href='/categories/rl-fundamental/'>RL Fundamental</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/rl/" class="post-tag no-text-decoration" >RL</a> <a href="/tags/ai/" class="post-tag no-text-decoration" >AI</a> <a href="/tags/mdp/" class="post-tag no-text-decoration" >MDP</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Finite+Markov+Decision+Processes+-+DevSlem+Blog&url=https%3A%2F%2Fdevslem.github.io%2Freinforcement-learning%2Frl-fundamental%2Ffinite-markov-decision-processes%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Finite+Markov+Decision+Processes+-+DevSlem+Blog&u=https%3A%2F%2Fdevslem.github.io%2Freinforcement-learning%2Frl-fundamental%2Ffinite-markov-decision-processes%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fdevslem.github.io%2Freinforcement-learning%2Frl-fundamental%2Ffinite-markov-decision-processes%2F&text=Finite+Markov+Decision+Processes+-+DevSlem+Blog" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div><script src="https://utteranc.es/client.js" repo="DevSlem/DevSlem.github.io" issue-term="pathname" label="comment" theme="github-dark" crossorigin="anonymous" async> </script></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/reinforcement-learning/drl-methods/dqn/">DQN: Deep Q-Networks</a><li><a href="/reinforcement-learning/drl-methods/dtqn/">DTQN: Deep Transformer Q-Networks for Partially Observable Reinforcement Learning</a><li><a href="/wsl-setup/">Windows Subsystem for Linux (WSL) Setup for Reinforcement Learning</a><li><a href="/algorithm/sorting/merge-sort/">[알고리즘] 합병 정렬</a><li><a href="/algorithm/sorting/shell-sort/">[알고리즘] 쉘 정렬</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/rl/">RL</a> <a class="post-tag" href="/tags/algorithm/">Algorithm</a> <a class="post-tag" href="/tags/unity/">Unity</a> <a class="post-tag" href="/tags/function-approximation-rl/">Function Approximation RL</a> <a class="post-tag" href="/tags/sorting/">Sorting</a> <a class="post-tag" href="/tags/enemy/">Enemy</a> <a class="post-tag" href="/tags/game-development/">Game Development</a> <a class="post-tag" href="/tags/project/">Project</a> <a class="post-tag" href="/tags/attribute/">Attribute</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 pl-3 pr-3 pr-xl-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/reinforcement-learning/rl-fundamental/policy-gradient-methods/"><div class="card-body"> <em class="small" data-ts="1665586800" data-df="ll" > Oct 13, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Policy Gradient Methods</h3><div class="text-muted small"><p> 드디어 긴 장정 끝에 대망의 마지막 챕터로 왔다. 이번 포스트에서는 그 유명한 policy gradient method에 대해 소개하려고 한다. 길고 긴 여정이 드디어 끝났다. Introduction 지금까지 우리는 value-based 기반의 방법을 다뤘었다. policy는 추정된 action value에 기반해 action을 선택했다. 이번에는...</p></div></div></a></div><div class="card"> <a href="/reinforcement-learning/rl-fundamental/multi-armed-bandits/"><div class="card-body"> <em class="small" data-ts="1653058800" data-df="ll" > May 21, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Multi-armed Bandits</h3><div class="text-muted small"><p> 이 포스트에서는 Reinforcement learning (RL)의 기본 내용인 Multi-armed Bandits 환경과 기본적인 아이디어들에 대해 알아본다. Reinforcement learning vs others Reinforcement learning (RL)과 다른 learning의 가장 큰 구별점은 사용하는 정보의 차이에 있다. 다른 ...</p></div></div></a></div><div class="card"> <a href="/reinforcement-learning/rl-fundamental/dynamic-programming-in-rl/"><div class="card-body"> <em class="small" data-ts="1656082800" data-df="ll" > Jun 25, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Dynamic Programming in RL</h3><div class="text-muted small"><p> 이 포스트에서는 RL에서 MDP로 environment의 perfect model이 주어졌을 때 optimal policy를 구하는데 사용되는 기초적인 방식인 Dynamic Programming (DP)를 소개한다. Introduction Reinforcement Learning (RL)에서 environment가 perfect model로 env...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/game-development/game-physics-summary/" class="btn btn-outline-primary" prompt="Older"><p>게임 물리 요약 (Game Physics Summary)</p></a> <a href="/reinforcement-learning/rl-fundamental/dynamic-programming-in-rl/" class="btn btn-outline-primary" prompt="Newer"><p>Dynamic Programming in RL</p></a></div></div></div><footer class="row pl-3 pr-3"><div class="col-12 d-flex justify-content-between align-items-center text-muted pl-0 pr-0"><div class="footer-left"><p class="mb-0"> © 2024 <a href="https://github.com/username">DevSlem</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/rl/">RL</a> <a class="post-tag" href="/tags/algorithm/">Algorithm</a> <a class="post-tag" href="/tags/unity/">Unity</a> <a class="post-tag" href="/tags/function-approximation-rl/">Function Approximation RL</a> <a class="post-tag" href="/tags/sorting/">Sorting</a> <a class="post-tag" href="/tags/enemy/">Enemy</a> <a class="post-tag" href="/tags/game-development/">Game Development</a> <a class="post-tag" href="/tags/project/">Project</a> <a class="post-tag" href="/tags/attribute/">Attribute</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a><div id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-animation="true" data-autohide="false"><div class="toast-header"> <button type="button" class="ml-2 ml-auto close" data-dismiss="toast" aria-label="Close"> <span aria-hidden="true">&times;</span> </button></div><div class="toast-body text-center pt-0"><p class="pl-2 pr-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></div><script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] }, svg: { scale: 1.1 } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script>
