<!DOCTYPE html><html lang="en" data-mode="dark" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="Policy Gradient Methods" /><meta property="og:locale" content="en" /><meta name="description" content="드디어 긴 장정 끝에 대망의 마지막 챕터로 왔다. 이번 포스트에서는 그 유명한 policy gradient method에 대해 소개하려고 한다. 길고 긴 여정이 드디어 끝났다." /><meta property="og:description" content="드디어 긴 장정 끝에 대망의 마지막 챕터로 왔다. 이번 포스트에서는 그 유명한 policy gradient method에 대해 소개하려고 한다. 길고 긴 여정이 드디어 끝났다." /><link rel="canonical" href="https://devslem.github.io/reinforcement-learning/rl-fundamental/policy-gradient-methods/" /><meta property="og:url" content="https://devslem.github.io/reinforcement-learning/rl-fundamental/policy-gradient-methods/" /><meta property="og:site_name" content="DevSlem Blog" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-10-13T00:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Policy Gradient Methods" /><meta name="google-site-verification" content="SSJwUK1JSPZV3wjQS19f2bXfHYIupbYEFsisH4Nuns4" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-10-14T00:00:00+09:00","datePublished":"2022-10-13T00:00:00+09:00","description":"드디어 긴 장정 끝에 대망의 마지막 챕터로 왔다. 이번 포스트에서는 그 유명한 policy gradient method에 대해 소개하려고 한다. 길고 긴 여정이 드디어 끝났다.","headline":"Policy Gradient Methods","mainEntityOfPage":{"@type":"WebPage","@id":"https://devslem.github.io/reinforcement-learning/rl-fundamental/policy-gradient-methods/"},"url":"https://devslem.github.io/reinforcement-learning/rl-fundamental/policy-gradient-methods/"}</script><title>Policy Gradient Methods | DevSlem Blog</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="DevSlem Blog"><meta name="application-name" content="DevSlem Blog"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> </a></div><div class="site-title mt-3"> <a href="/">DevSlem Blog</a></div><div class="site-subtitle font-italic">It's a blog where i write posts about AI, RL, Unity, etc.</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/DevSlem" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['devslem12','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper"><div id="topbar" class="container d-flex align-items-center justify-content-between h-100 pl-3 pr-3 pl-md-4 pr-md-4"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Policy Gradient Methods</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper" class="d-flex justify-content-center"><div id="main" class="container pl-xl-4 pr-xl-4"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pr-xl-4"><div class="post pl-1 pr-1 pl-md-2 pr-md-2"><h1 data-toc-skip>Policy Gradient Methods</h1><div class="post-meta text-muted"> <span> Posted <em class="" data-ts="1665586800" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Oct 13, 2022 </em> </span> <span> Updated <em class="" data-ts="1665705600" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Oct 14, 2022 </em> </span><div class="d-flex justify-content-between"> <span> By <em> <a href="https://github.com/username">DevSlem</a> </em> </span><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="3988 words"> <em>22 min</em> read</span></div></div></div><div class="post-content"><p>드디어 긴 장정 끝에 대망의 마지막 챕터로 왔다. 이번 포스트에서는 그 유명한 policy gradient method에 대해 소개하려고 한다. 길고 긴 여정이 드디어 끝났다.</p><h2 id="introduction"><span class="mr-2">Introduction</span><a href="#introduction" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>지금까지 우리는 value-based 기반의 방법을 다뤘었다. policy는 추정된 action value에 기반해 action을 선택했다. 이번에는 <strong>policy 자체를 매개변수화된 함수로써 학습</strong>하는 방법을 볼 것이다. policy parameter vector를 $\mathbf{\theta} \in \mathbb{R}^{d’}$라고 할 때 policy를 parameter $\mathbf{\theta}$가 주어졌을 때 action $a$를 선택하는 확률로써 정의할 수 있다.</p>\[\pi(a \vert s, \mathbf{\theta}) = \Pr\{A_t = a \ \vert \ S_t = s, \mathbf{\theta}_t = \mathbf{\theta} \}\]<p>policy parameter에 관한 scalar 성능 수치 $J(\mathbf{\theta})$가 있다고 하자. policy를 이 수치에 대한 gradient에 기반해 policy parameter를 학습할 것이다. 이 성능 수치를 maximize하는 것이 목적이며 이에 따라 $J$에 대해 <em>gradient ascent</em>를 수행한다.</p>\[\mathbf{\theta}_{t+1} + \mathbf{\theta}_t + \alpha \widehat{\nabla J(\mathbf{\theta}_t)}\]<p>$\widehat{\nabla J(\mathbf{\theta}_t)} \in \mathbb{R}^{d’}$는 stochastic 추정치로, 이것의 기대값은 성능 수치의 gradient를 근사화한다. 이러한 일반적인 schema를 따르는 모든 방법들을 <em>policy gradient methods</em>라고 부른다.</p><h2 id="policy-approximation-and-its-advantages"><span class="mr-2">Policy Approximation and its Advantages</span><a href="#policy-approximation-and-its-advantages" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>policy gradient method에서는 $\pi(a \vert s, \mathbf{\theta})$가 미분 가능하다. 또한 exploration을 보장하기 위해 일반적으로 <strong>policy는 절대 deterministic하지 않으며 즉, stochastic</strong> (i.e., $\pi(a \vert s, \mathbf{\theta}) \in (0, 1), \text{ for all } s, a, \mathbf{\theta}$) 하다. policy-based method는 discrete action space 뿐만 아니라 continuous action space에서도 쉽게 적용 가능하다는 엄청난 장점이 있다.</p><p>action space가 discrete이고 너무 크지 않다고 하자. 이때 state-action pair에 대한 매개변수화된 수치적인 선호도를 $h(s, a, \mathbf{\theta})$라고 한다면, 아래와 같이 policy $\pi$를 soft-max distribution으로 정의할 수 있다.</p>\[\pi(a \vert s, \mathbf{\theta}) \doteq \dfrac{e^{h(s,a,\mathbf{\theta})}}{\sum_b e^{h(s,b,\mathbf{\theta})}}\]<p>이러한 방식이 가지는 이점은 크게 아래와 같다.</p><ol><li>policy가 점점 deterministic policy로 다가가도록 근사화됨<li>임의의 확률을 가진 action 선택을 가능하게 함<li>policy가 종종 근사화하기에 더 단순한 함수이며 학습 속도가 빠름<li>사전 지식을 주입하기에 더 좋음</ol><p>1번의 경우 어떤 state에서의 optimal policy가 deterministic할 때 policy-based method는 deterministic 하도록 근사화하지만, action-value method는 $\epsilon$-greedy policy를 사용할 때 항상 $\epsilon$의 확률로 랜덤하게 action을 선택해야 한다.</p><blockquote class="prompt-danger"><div><p>$\epsilon$을 0으로 설정하면 다른 state에서도 deterministic하게 되므로 결코 바람직 하지 않음</p></div></blockquote><p>2번의 경우 카드게임 같은 확률 게임을 생각해볼 수 있다. 이러한 게임들은 optimal policy가 stochastic하다. 아래 그림을 보면 왜 $\epsilon$-greedy에 비해 훨씬 우월한지 알 수 있다.</p><p><img data-src="/assets/images/rl-sutton-ex13.1.png" alt="" width="80%" data-proofer-ignore> <em>Fig 1. When optimal policy is stochastic.<br /> (Image source: Sec 13.1 Sutton &amp; Barto (2020).)</em></p><p>위 그림은 $\epsilon = 0.1$일 때의 상황으로 optimal (약 0.59의 확률)에 비해 획득한 value가 훨씬 낮다.</p><h2 id="the-policy-gradient-theorem"><span class="mr-2">The Policy Gradient Theorem</span><a href="#the-policy-gradient-theorem" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>policy 매개변수화에 대한 중요한 이론적인 이점이 하나 더 있다. <strong>연속적인 policy 매개변수화는 action 선택 확률을 학습된 parameter의 함수로써 부드럽게 변화시킨다.</strong> 반면 $\epsilon$-greedy는 매우 작은 변화로 maximum action value를 가지는 action이 변경되면 확률이 급격히 변경된다. 따라서 policy-gradient method는 수렴성이 더 강력히 보장된다.</p><p>성능 수치 $J(\mathbf{\theta})$는 episodic과 continuing task에서 다르게 정의되긴 한다. 이 포스트에서는 episodic case에 대해서만 다룬다. 성능 수치를 episode의 시작 state에서의 value로 정의하자. 모든 episode는 특정한 state $s_0$에서 시작한다고 할 때 아래와 같이 정의된다.</p>\[J(\mathbf{\theta}) \doteq v_{\pi_\mathbf{\theta}}(s_0)\]<p>$v_{\pi_\mathbf{\theta}}$는 $\pi_\mathbf{\theta}$에 대한 true value function이다. 여기서는 no discounting ($\gamma = 1$)을 가정한다.</p><p>성능 개선을 보장하는 방향으로 policy parameter를 변경해야 한다. 문제는 성능이 action 선택과 state distribution에 의존한다는 점이다. action 선택은 별 문제되지 않지만 state distribution은 다르다. 우리는 일반적으로 state distribution을 모른다. 이것은 environment의 함수이기 때문이다.</p><p><em>policy gradient theorem</em>을 통해 이 문제를 쉽게 해결할 수 있다. state distribution에 대한 미분을 포함하지 않고, policy parameter에 관한 성능의 gradient에 대한 analytic 표현을 제공한다. 아래는 episodic case에 대한 policy gradient theorem이다.</p>\[\nabla J(\mathbf{\theta}) \propto \sum_s \mu(s) \sum_a q_\pi(s, a) \nabla \pi(a \vert s, \mathbf{\theta})\]<p>$\propto$는 “비례한다”의 의미이다. distribution $\mu$는 $\pi$하에서의 on-policy distribution이다.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></p><p>이제 policy gradient theorem 증명 과정을 살펴보자. notation의 단순화를 위해 $\pi$는 $\mathbf{\theta}$의 함수이며, 모든 gradient는 $\mathbf{\theta}$에 관한 것임을 암시적으로 나타낸다. 먼저 시작은 state-value function의 gradient를 action-value function에 관해 나타내는 것으로 시작한다.</p>\[\begin{align} \nabla v_\pi(s) &amp;= \nabla \bigg[ \sum_a \pi(a \vert s) q_\pi(s,a) \bigg], \quad \text{for all $s \in \mathcal{S}$} \\ &amp;= \sum_a \Big[ \nabla \pi(a \vert s) q_\pi(s,a) + \pi(a \vert s) \nabla q_\pi(s,a) \Big] \quad \text{(product rule of calculus)} \\ &amp;= \sum_a \Big[ \nabla \pi(a \vert s) q_\pi(s,a) + \pi(a \vert s) \nabla \sum_{s',r}p(s',r \vert s,a)\big(r + v_\pi(s')\big) \Big] \quad (\because q_\pi(s, a) \doteq \sum_{s', r} p(s', r \vert s, a) \Big[r + \gamma v_\pi(s') \Big]) \\ &amp;= \sum_a \Big[ \nabla \pi(a \vert s) q_\pi(s,a) + \pi(a \vert s) \sum_{s'}p(s' \vert s,a) \nabla v_\pi(s') \Big] \quad (\because p(s' \vert s, a) \doteq \sum_{r \in \mathcal{R}}p(s', r \vert s, a)) \\ &amp;= \sum_a \Big[ \nabla \pi(a \vert s) q_\pi(s,a) + \pi(a \vert s) \sum_{s'}p(s' \vert s,a) \quad \text{(unrolling)} \\ &amp;\quad\quad \sum_{a'} \big[\nabla \pi(a' \vert s') q_\pi(s',a') + \pi(a' \vert s')\sum_{s''}p(s'' \vert s',a') \nabla v_\pi(s'') \big] \Big] \\ &amp;= \sum_{x \in \mathcal{S}} \sum_{k=0}^\infty \Pr(s \rightarrow x, k, \pi) \sum_a \nabla \pi(a \vert x) q_\pi(x,a) \end{align}\]<p>$p(s’ \vert s, a)$<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>와 $q_\pi(s, a)$<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>의 변환 원리는 MDP에 관한 포스트를 참고하기 바란다. unrolling을 반복한 이후의 $\Pr(s \rightarrow x, k, \pi)$는 policy $\pi$를 따를 때 $k$ time step에서의 state $s$에서 $x$로 transition이 발생할 확률이다.</p><p>앞서 우리의 목적함수 $J(\mathbf{\theta}) = v_\pi(s_0)$였다. 위에서 구한 $\nabla v_\pi(s)$를 통해 $\nabla v_\pi(s_0)$를 구해보자.</p>\[\begin{align} \nabla J(\mathbf{\theta}) &amp;= \nabla v_\pi(s_0) \\ &amp;= \sum_s \Bigg( \sum_{k=0}^\infty \Pr(s_0 \rightarrow s, k, \pi) \Bigg) \sum_a \nabla \pi(a \vert s) q_\pi(s,a) \\ &amp;= \sum_s \eta(s) \sum_a \nabla \pi(a \vert s) q_\pi(s,a) \quad (\text{$\eta(s)$ is the expected number of visits}) \\ &amp;= \sum_{s'} \eta(s') \sum_s \dfrac{\eta(s)}{\sum_{s'}\eta(s')} \sum_a \nabla \pi(a \vert s) q_\pi(s,a) \\ &amp;= \sum_{s'} \eta(s') \sum_s \mu(s) \sum_a \nabla \pi(a \vert s) q_\pi(s, a) \\ &amp;\propto \sum_s \mu(s) \sum_a \nabla \pi(a \vert s) q_\pi(s, a) \quad (\text{Q.E.D.}) \end{align}\]<p>드디어 모든 증명이 끝났다. 기대 방문 횟수 $\eta$와 state distribution $\mu$의 관계는 sutton 책에 자세히 기술되어 있으니 참고하기 바란다.<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup></p><h2 id="reinforce-monte-carlo-policy-gradient"><span class="mr-2">REINFORCE: Monte Carlo Policy Gradient</span><a href="#reinforce-monte-carlo-policy-gradient" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>자, 이제 본격적으로 실제 사용되는 policy gradient method를 알아보려고 한다. policy gradient theorem의 우측항은 target policy $\pi$를 따를 때 얼마나 자주 state들이 나타나는지에 의한 weighted sum이다. 따라서 이를 <strong>실제 state $S_t$에서의 $\pi$에 관한 기대값으로 표현</strong>할 수 있다.</p>\[\begin{align} \nabla J(\mathbf{\theta}) &amp;\propto \sum_s \mu(s) \sum_a q_\pi(s,a) \nabla \pi(a \vert s, \mathbf{\theta}) \\ &amp;= \mathbb{E}_\pi\Bigg[\sum_a q_\pi(S_t,a) \nabla \pi(a \vert S_t, \mathbf{\theta}) \Bigg] \end{align}\]<p>이를 통해 stochastic gradient-ascent를 수행할 수 있다.</p>\[\mathbf{\theta}_{t+1} \doteq \mathbf{\theta}_t + \alpha \sum_a \hat{q}(S_t, a, \mathbf{w}) \nabla \pi(a \vert S_t, \mathbf{\theta})\]<p>$\hat{q}$은 $q_\pi$의 학습된 근사치이다. 위 방법은 모든 action을 포함하기 때문에 <em>all-actions</em> method라고 부른다.</p><p>그러나 우리는 모든 action을 고려하기 보다는, <strong>실제 선택된 action에 대해 고려</strong>하고 싶다. 이를 수행하는 가장 간단하면서도 유명한 REINFORCE 알고리즘이 있다. 아래는 위 수식으로부터 REINFORCE 알고리즘을 유도하는 과정이다.</p>\[\begin{align} \nabla J(\mathbf{\theta}) &amp;\propto \mathbb{E}_\pi \Bigg[ \sum_a \pi(a \vert S_t, \mathbf{\theta}) q_\pi(S_t, a) \dfrac{\nabla \pi(a \vert S_t, \mathbf{\theta})}{\pi(a \vert S_t, \mathbf{\theta})} \Bigg] \\ &amp;= \mathbb{E}_\pi \Big[ q_\pi(S_t, A_t) \dfrac{\nabla \pi(A_t \vert S_t, \mathbf{\theta})}{\pi(A_t \vert S_t, \mathbf{\theta})} \Big] \quad \text{(replacing $a$ by the sample $A_t \sim \pi$)} \\ &amp;= \mathbb{E}_\pi \Big[G_t \dfrac{\nabla \pi(A_t \vert S_t, \mathbf{\theta})}{\pi(A_t \vert S_t, \mathbf{\theta})}] \quad \text{(because $\mathbb{E}_\pi[G_t \vert S_t, A_t] = q_\pi(S_t, A_t)$)} \\ &amp;= \mathbb{E}_\pi \Big[G_t \nabla \ln \pi(A_t \vert S_t, \mathbf{\theta}) \Big] \quad \text{(logarithmic derivative)} \end{align}\]<p>위 과정을 차례차례 보자. action $a$에 대한 합을, policy $\pi$를 따를 때의 기대값에 의해 실제 선택된 action $A_t$로 대체하고 싶다. 그러기 위해서는 action $a$를 policy $\pi(a \vert S_t, \mathbf{\theta})$에 의해 가중치를 부여하면 된다. 이렇게 되면 policy $\pi$를 따를 때 실제 선택된 action에 대한 기대값과 동일하다. 수식의 동등성을 유지하기 위해 $\pi(a \vert S_t, \mathbf{\theta})$를 곱하고 나눈다.</p><p>마지막 라인이 우리가 얻고 싶었던 수식이다. $G_t$는 일반적인 return이며, $G_t \nabla \ln \pi(A_t \vert S_t, \mathbf{\theta})$는 각 time step에서 sampling된 값으로, 이것의 기대값은 gradient에 비례한다. 이를 통해 stochastic gradient ascent 알고리즘, REINFORCE 업데이트를 수행한다.</p>\[\mathbf{\theta}_{t+1} \doteq \mathbf{\theta}_t + \alpha G_t \nabla \ln \pi(A_t \vert S_t, \mathbf{\theta}_t)\]<p><strong>REINFORCE는 time step $t$에서의 실제 return을 사용한다.</strong> 따라서 REINFORCE는 Monte Carlo (MC) method로, episode가 종료되어야만 모든 업데이트가 수행된다. 아래는 REINFORCE 알고리즘이다.</p><blockquote><h5 id="textalgorithm-reinforce-monte-carlo-policy-gradient-control-episodic-for-pi_ast"><span class="mr-2">$\text{Algorithm: REINFORCE: Monte-Carlo Policy-Gradient Control (episodic) for $\pi_\ast$}$</span><a href="#textalgorithm-reinforce-monte-carlo-policy-gradient-control-episodic-for-pi_ast" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5><p>\(\begin{align*} &amp; \textstyle \text{Input: a differentiable policy parameterization $\pi(a \vert s, \mathbf{\theta})$} \\ &amp; \textstyle \text{Algorithm parameter: step size $\alpha &gt; 0$} \\ &amp; \textstyle \text{Initialize policy parameter $\mathbf{\theta} \in \mathbb{R}^{d'}$ (e.g., to $\mathbf{0}$)} \\ \\ &amp; \textstyle \text{Loop forever (for each episode):} \\ &amp; \textstyle \qquad \text{Generate an episode $S_0, A_0, R_1, \dots, S_{T-1}, A_{T-1}, R_T$, following $\pi(\cdot \vert \cdot, \mathbf{\theta})$} \\ &amp; \textstyle \qquad \text{Loop for each step of the episode $t = 0, 1, \dots, T - 1$:} \\ &amp; \textstyle \qquad\qquad G \leftarrow \sum_{k = t+1}^T \gamma^{k-t-1}R_k \\ &amp; \textstyle \qquad\qquad \mathbf{\theta} \leftarrow \mathbf{\theta} + \alpha \gamma^t G \nabla \ln \pi(A_t \vert S_t, \mathbf{\theta}) \\ \end{align*}\)</p></blockquote><p>한 가지 차이점은, REINFORCE 업데이트 시의 discount factor $\gamma^t$이다. 앞서 policy gradient theorem에서 non-discounted case ($\gamma=1$)을 가정했었지만, 여기서는 일반적인 discounted setting이기 때문에 추가되었다.</p><p>REINFORCE는 $\alpha$가 시간에 따라 감소한다고 할 때 local optimum으로의 수렴성이 보장된다. 그러나 MC method 특성 상 분산이 크고, 학습 속도가 느리다.</p><h2 id="reinforce-with-baseline"><span class="mr-2">REINFORCE with Baseline</span><a href="#reinforce-with-baseline" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>policy gradient theorem은 임의의 action value와 <em>baseline</em> $b(s)$의 비교를 포함하도록 일반화 될 수 있다.</p>\[\nabla J(\mathbf{\theta}) \propto \sum_s \mu(s) \sum_a \Big(q_\pi(s,a) - b(s) \Big) \nabla \pi(a \vert, \mathbf{\theta})\]<p><strong>baseline은 어떤 것도 가능</strong>하며, 심지어 난수도 가능하다. 위 수식이 성립하는 이유는 baseline에 의해 빼는 값이 0이기 때문이다.</p>\[\sum_a b(s) \nabla \pi(a \vert s, \mathbf{\theta}) = b(s) \nabla \sum_a \pi(a \vert s, \mathbf{\theta}) = b(s) \nabla 1 = 0\]<p>위에 따라 REINFORCE에도 baseline을 적용하는 것이 가능하다.</p>\[\mathbf{\theta}_{t+1} \doteq \mathbf{\theta}_t + \alpha \Big(G_t - b(S_t) \Big) \nabla \ln \pi(A_t \vert S_t, \mathbf{\theta}_t)\]<p>baseline은 업데이트의 기대값을 변화시키지는 않는다. 하지만 분산에는 큰 영향을 미친다. 어떤 state들에서는 모든 action들이 높은 값을 가지고, 다른 어떤 state들에서는 모든 action들이 낮은 값을 가질 수 있다. 이 때 baseline을 적절히 사용하면 모든 action들의 value와 baseline의 차이의 평균을 0으로 조정해 분산을 낮출 수 있다.</p><h2 id="actor-critic-methods"><span class="mr-2">Actor-Critic Methods</span><a href="#actor-critic-methods" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>REINFORCE는 앞서 봤듯이 MC method이기 때문에 여러 문제점을 가지고 있다. 우리는 이미 tabular method에서 봤듯이 MC method를 TD method로 개선할 수 있음을 알고 있다. 여기서도 마찬가지로 적용할 수 있다.</p><p>REINFORCE의 실제 return을 bootstrapping이 사용된 one-step return $G_{t:t+1}$으로 대체한다. one-step return은 획득한 reward와 next state에서의 discounted state value를 더한 값이다. baseline은 현재 state value로 설정한다. action을 평가하는데 state-value function이 이러한 방식으로 사용될 때 이를 <em>critic</em>이라고 부르며, policy 부분은 <em>actor</em>라고 부른다. 따라서 이러한 policy gradient method를 <em>actor-critic</em> method라고 부른다.</p><p>one-step actor-critic method는 one-step return과 baseline으로 학습된 state-value function을 통해 REINFORCE를 아래와 같이 완전히 대체할 수 있다.</p>\[\begin{align} \mathbf{\theta}_{t+1} &amp;\doteq \mathbf{\theta}_t + \alpha \Big( G_{t:t+1} - \hat{v}(S_t, \mathbf{w}) \Big) \nabla \ln \pi(A_t \vert S_t, \mathbf{\theta}_t) \\ &amp;= \mathbf{\theta}_t + \alpha \Big( R_{t+1} + \gamma \hat{v}(S_{t+1}, \mathbf{w}) - \hat{v}(S_t, \mathbf{w}) \Big) \nabla \ln \pi(A_t \vert S_t, \mathbf{\theta}_t) \end{align}\]<p>state-value function을 학습하는 가장 자연스러운 방법은 semi-gradient TD(0)이다. <strong>actor-critic은 REINFORCE와 달리 bootstrapping을 이용하기 때문에 완전히 online으로 학습할 수 있다.</strong> 아래는 one-step actor-critic 알고리즘이다.</p><blockquote><h5 id="textalgorithm-one-step-actor-critic-episodic-for-estimating-pi_mathbftheta-approx-pi_ast"><span class="mr-2">$\text{Algorithm: One-step Actor-Critic (episodic), for estimating $\pi_{\mathbf{\theta}} \approx \pi_\ast$}$</span><a href="#textalgorithm-one-step-actor-critic-episodic-for-estimating-pi_mathbftheta-approx-pi_ast" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5><p>\(\begin{align*} &amp; \textstyle \text{Input: a differentiable policy parameterization $\pi(a \vert s, \mathbf{\theta})$} \\ &amp; \textstyle \text{Input: a differentiable state-value function parameterization $\hat{v}(s, \mathbf{w})$} \\ &amp; \textstyle \text{Parameters: step sizes $\alpha^\mathbf{\theta} &gt; 0$, $\alpha^\mathbf{w} &gt; 0$} \\ &amp; \textstyle \text{Initialize policy parameters $\mathbf{\theta} \in \mathbb{R}^{d'}$ and state-value weights $\mathbf{w} \in \mathbb{R}^d$ (e.g., to $\mathbf{0}$)} \\ \\ &amp; \textstyle \text{Loop forever (for each episode)} \\ &amp; \textstyle \qquad \text{Initialize $S$ (first state of episode)} \\ &amp; \textstyle \qquad I \leftarrow 1 \\ &amp; \textstyle \qquad \text{Loop while $S$ is not terminal (for each time step):} \\ &amp; \textstyle \qquad\qquad A \sim \pi(\cdot \vert S, \mathbf{\theta}) \\ &amp; \textstyle \qquad\qquad \text{Take action $A$, observe $S', R$} \\ &amp; \textstyle \qquad\qquad \delta \leftarrow R + \gamma \hat{v}(S', \mathbf{w}) - \hat{v}(S, \mathbf{w}) \qquad \text{(if $S'$ is terminal, then $\hat{v}(S', \mathbf{w}) \doteq 0$)} \\ &amp; \textstyle \qquad\qquad \mathbf{w} \leftarrow \mathbf{w} + \alpha^\mathbf{w} \delta \nabla \hat{v}(S, \mathbf{w}) \\ &amp; \textstyle \qquad\qquad \mathbf{\theta} \leftarrow \mathbf{\theta} + \alpha^\mathbf{\theta} I \delta \nabla \ln \pi(A \vert S, \mathbf{\theta}) \\ &amp; \textstyle \qquad\qquad I \leftarrow \gamma I \\ &amp; \textstyle \qquad\qquad S \leftarrow S' \\ \end{align*}\)</p></blockquote><h2 id="policy-parameterization-for-continuous-actions"><span class="mr-2">Policy Parameterization for Continuous Actions</span><a href="#policy-parameterization-for-continuous-actions" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>policy-based method는 큰 action space를 다루기에 적합하며 심지어 action이 무한개인 continuous action space를 다룰 수 있다. 수많은 action들의 각각의 확률을 학습하기 보다는, 확률 분포를 학습한다. 예를 들면 action이 실수 집합에서 정의될 때 action을 정규분포로부터 sampling 할 수 있다.</p><p>정규분포의 확률밀도함수는 일반적으로 아래와 같다.</p>\[p(x) \doteq \dfrac{1}{\sigma \sqrt{2\pi}} \exp \bigg(- \dfrac{(x - \mu)^2}{2 \sigma^2} \bigg)\]<p>$\mu$는 평균, $\sigma$는 표준편차이며, 여기서 $\pi$는 당연하지만 실수 $\pi \approx 3.14159$이다. $p(x)$는 확률이 아닌 $x$에서의 확률밀도이며, $x$가 어떤 범위 안에 있을 확률은 확률밀도함수의 integral이다. 아래 그림은 $\mu$와 $\sigma$의 값에 따른 정규분포 확률밀도함수이다.</p><p><img data-src="/assets/images/rl-sutton-normal-dist.png" alt="" width="60%" data-proofer-ignore> <em>Fig 2. Normal distribution.<br /> (Image source: Sec 13.7 Sutton &amp; Barto (2020).)</em></p><p>policy를 정규분포에 대해 매개변수화하기 위해 평균 $\mu$와 표준편차 $\sigma$를 아래와 같이 매개변수화한다.</p>\[\pi(a \vert s, \mathbf{\theta}) \doteq \dfrac{1}{\sigma(s, \mathbf{\theta}) \sqrt{2\pi}} \exp \bigg(- \dfrac{(a - \mu(s, \mathbf{\theta}))^2}{2 \sigma(s, \mathbf{\theta})^2} \bigg)\]<p>$\mu : \mathcal{S} \times \mathbb{R}^{d’} \rightarrow \mathbb{R}$와 $\sigma : \mathcal{S} \times \mathbb{R}^{d’} \rightarrow \mathbb{R}^+$는 매개변수화된 function approximator이다. 이를 위해 policy의 parameter vector를 $\mathbf{\theta} = [\mathbf{\theta} _\mu, \mathbf{\theta} _\sigma]^\top$와 같이 두 파트로 나누어야한다.</p><h2 id="summary"><span class="mr-2">Summary</span><a href="#summary" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>길고 긴 여정이 끝이 났다. 강화학습의 기초부터 policy gradient method까지 오는데 많은 시간이 걸렸다. 이 summary를 끝으로 <strong>RL Fundamental</strong>은 끝이다.</p><ul><li>policy gradient method는 policy 자체를 매개변수화하는 방법<li>action을 선택하는 구체적인 확률을 학습할 수 있음<li>discrete action 뿐만 아니라 continuous action에도 적용 가능<li>policy gradient theorem에 의해 state distribution의 미분을 포함하지 않고 policy parameter에 의해 얼마나 성능이 영향을 받는지에 대한 기술이 가능<li>policy gradient method에 baseline을 추가하면 분산을 낮추는데 상당히 도움이 됨<li>REINFORCE는 Monte Carlo policy gradient method<li>Actor-Critic은 bootstrapping을 통해 online 학습이 가능<li>actor는 policy, critic은 state-value function을 학습</ul><h2 id="references"><span class="mr-2">References</span><a href="#references" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>[1] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction; 2nd Edition. 2020.</p><h2 id="footnotes"><span class="mr-2">Footnotes</span><a href="#footnotes" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><div class="footnotes" role="doc-endnotes"><ol><li id="fn:1" role="doc-endnote"><p>DevSlem. <a href="../on-policy-prediction-with-approximation/#the-prediction-objective-overlinetextve">On-policy Prediction with Approximation. The Prediction Objective ($\overline{\text{VE}}$)</a>. <a href="../on-policy-control-with-approximation/#ergodicity">On-policy Control with Approximation. Average Reward: A New Problem Setting for Continuing Tasks. Ergodicity</a>. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:2" role="doc-endnote"><p>DevSlem. <a href="../finite-markov-decision-processes/#what-is-mdps">Finite Markov Decision Processes. What is MDPs</a>. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:3" role="doc-endnote"><p>DevSlem. <a href="../finite-markov-decision-processes/#bellman-expectation-equation">Finite Markov Decision Processes. Bellman Expectation Equation</a>. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:4" role="doc-endnote"><p>Reinforcement Learning: An Introduction; 2nd Edition. 2020. Sec. 9.2, p.199. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p></ol></div></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/reinforcement-learning/'>Reinforcement Learning</a>, <a href='/categories/rl-fundamental/'>RL Fundamental</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/rl/" class="post-tag no-text-decoration" >RL</a> <a href="/tags/ai/" class="post-tag no-text-decoration" >AI</a> <a href="/tags/function-approximation-rl/" class="post-tag no-text-decoration" >Function Approximation RL</a> <a href="/tags/policy-gradient/" class="post-tag no-text-decoration" >Policy Gradient</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Policy+Gradient+Methods+-+DevSlem+Blog&url=https%3A%2F%2Fdevslem.github.io%2Freinforcement-learning%2Frl-fundamental%2Fpolicy-gradient-methods%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Policy+Gradient+Methods+-+DevSlem+Blog&u=https%3A%2F%2Fdevslem.github.io%2Freinforcement-learning%2Frl-fundamental%2Fpolicy-gradient-methods%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fdevslem.github.io%2Freinforcement-learning%2Frl-fundamental%2Fpolicy-gradient-methods%2F&text=Policy+Gradient+Methods+-+DevSlem+Blog" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div><script src="https://utteranc.es/client.js" repo="DevSlem/DevSlem.github.io" issue-term="pathname" label="comment" theme="github-dark" crossorigin="anonymous" async> </script></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/reinforcement-learning/drl-methods/dqn/">DQN: Deep Q-Networks</a><li><a href="/reinforcement-learning/drl-methods/dtqn/">DTQN: Deep Transformer Q-Networks for Partially Observable Reinforcement Learning</a><li><a href="/wsl-setup/">Windows Subsystem for Linux (WSL) Setup for Reinforcement Learning</a><li><a href="/algorithm/sorting/merge-sort/">[알고리즘] 합병 정렬</a><li><a href="/algorithm/sorting/shell-sort/">[알고리즘] 쉘 정렬</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/rl/">RL</a> <a class="post-tag" href="/tags/algorithm/">Algorithm</a> <a class="post-tag" href="/tags/unity/">Unity</a> <a class="post-tag" href="/tags/function-approximation-rl/">Function Approximation RL</a> <a class="post-tag" href="/tags/sorting/">Sorting</a> <a class="post-tag" href="/tags/enemy/">Enemy</a> <a class="post-tag" href="/tags/game-development/">Game Development</a> <a class="post-tag" href="/tags/project/">Project</a> <a class="post-tag" href="/tags/attribute/">Attribute</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 pl-3 pr-3 pr-xl-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/reinforcement-learning/rl-fundamental/on-policy-prediction-with-approximation/"><div class="card-body"> <em class="small" data-ts="1660748400" data-df="ll" > Aug 18, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>On-policy Prediction with Approximation</h3><div class="text-muted small"><p> 이 포스트에서는 reinforcement learning을 수행하는 새로운 방법인 function approximation에 대한 소개와 이를 바탕으로 on-policy method에서 prediction을 수행하는 방법을 소개한다. What is Function Approximation and Why needed? 지금까지 알아본 기존 Reinf...</p></div></div></a></div><div class="card"> <a href="/reinforcement-learning/rl-fundamental/on-policy-control-with-approximation/"><div class="card-body"> <em class="small" data-ts="1661180400" data-df="ll" > Aug 23, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>On-policy Control with Approximation</h3><div class="text-muted small"><p> 이 포스트에서는 function approximation을 사용한 prediction을 control로 확장할 것이다. 이를 위해 state-value function이 아닌 action-value function을 추정한다. 그 후 on-policy GPI의 일반적인 패턴을 따라 학습을 진행하는 방법을 알아본다. 이 포스트에서는 특히 semi-gra...</p></div></div></a></div><div class="card"> <a href="/reinforcement-learning/rl-fundamental/off-policy-methods-with-approximation/"><div class="card-body"> <em class="small" data-ts="1661785200" data-df="ll" > Aug 30, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Off-policy Methods with Approximation</h3><div class="text-muted small"><p> 이 포스트에서는 on-policy function approximation을 off-policy로의 확장과 이로 인해 발생하는 문제들에 대해 다룰 것이다. Introduction off-policy method는 behavior policy $b$에 의해 생성된 experience로부터 target policy $\pi$에 대한 value func...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/reinforcement-learning/exploration-methods/exploration-by-random-network-distillation/" class="btn btn-outline-primary" prompt="Older"><p>Exploration by Random Network Distillation</p></a> <a href="/algorithm/sorting/bubble-sort/" class="btn btn-outline-primary" prompt="Newer"><p>[알고리즘] 버블 정렬</p></a></div></div></div><footer class="row pl-3 pr-3"><div class="col-12 d-flex justify-content-between align-items-center text-muted pl-0 pr-0"><div class="footer-left"><p class="mb-0"> © 2024 <a href="https://github.com/username">DevSlem</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/rl/">RL</a> <a class="post-tag" href="/tags/algorithm/">Algorithm</a> <a class="post-tag" href="/tags/unity/">Unity</a> <a class="post-tag" href="/tags/function-approximation-rl/">Function Approximation RL</a> <a class="post-tag" href="/tags/sorting/">Sorting</a> <a class="post-tag" href="/tags/enemy/">Enemy</a> <a class="post-tag" href="/tags/game-development/">Game Development</a> <a class="post-tag" href="/tags/project/">Project</a> <a class="post-tag" href="/tags/attribute/">Attribute</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a><div id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-animation="true" data-autohide="false"><div class="toast-header"> <button type="button" class="ml-2 ml-auto close" data-dismiss="toast" aria-label="Close"> <span aria-hidden="true">&times;</span> </button></div><div class="toast-body text-center pt-0"><p class="pl-2 pr-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></div><script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] }, svg: { scale: 1.1 } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script>
