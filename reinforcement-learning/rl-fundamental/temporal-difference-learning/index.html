<!DOCTYPE html><html lang="en" data-mode="dark" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="Temporal-Difference Learning" /><meta property="og:locale" content="en" /><meta name="description" content="이 포스트에서는 RL에서 반드시 알아야 하는 RL의 핵심인 Temporal-Difference learning method를 소개한다." /><meta property="og:description" content="이 포스트에서는 RL에서 반드시 알아야 하는 RL의 핵심인 Temporal-Difference learning method를 소개한다." /><link rel="canonical" href="https://devslem.github.io/reinforcement-learning/rl-fundamental/temporal-difference-learning/" /><meta property="og:url" content="https://devslem.github.io/reinforcement-learning/rl-fundamental/temporal-difference-learning/" /><meta property="og:site_name" content="DevSlem Blog" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-07-07T00:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Temporal-Difference Learning" /><meta name="google-site-verification" content="SSJwUK1JSPZV3wjQS19f2bXfHYIupbYEFsisH4Nuns4" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-07-18T00:00:00+09:00","datePublished":"2022-07-07T00:00:00+09:00","description":"이 포스트에서는 RL에서 반드시 알아야 하는 RL의 핵심인 Temporal-Difference learning method를 소개한다.","headline":"Temporal-Difference Learning","mainEntityOfPage":{"@type":"WebPage","@id":"https://devslem.github.io/reinforcement-learning/rl-fundamental/temporal-difference-learning/"},"url":"https://devslem.github.io/reinforcement-learning/rl-fundamental/temporal-difference-learning/"}</script><title>Temporal-Difference Learning | DevSlem Blog</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="DevSlem Blog"><meta name="application-name" content="DevSlem Blog"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> </a></div><div class="site-title mt-3"> <a href="/">DevSlem Blog</a></div><div class="site-subtitle font-italic">It's a blog where i write posts about AI, RL, Unity, etc.</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/DevSlem" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['devslem12','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper"><div id="topbar" class="container d-flex align-items-center justify-content-between h-100 pl-3 pr-3 pl-md-4 pr-md-4"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Temporal-Difference Learning</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper" class="d-flex justify-content-center"><div id="main" class="container pl-xl-4 pr-xl-4"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pr-xl-4"><div class="post pl-1 pr-1 pl-md-2 pr-md-2"><h1 data-toc-skip>Temporal-Difference Learning</h1><div class="post-meta text-muted"> <span> Posted <em class="" data-ts="1657119600" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Jul 7, 2022 </em> </span> <span> Updated <em class="" data-ts="1658102400" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Jul 18, 2022 </em> </span><div class="d-flex justify-content-between"> <span> By <em> <a href="https://github.com/username">DevSlem</a> </em> </span><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="4472 words"> <em>24 min</em> read</span></div></div></div><div class="post-content"><p>이 포스트에서는 RL에서 반드시 알아야 하는 RL의 핵심인 Temporal-Difference learning method를 소개한다.</p><h2 id="what-is-td-learning"><span class="mr-2">What is TD learning</span><a href="#what-is-td-learning" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p><em>Temporal-Difference</em> (TD) learning method는 <strong>Monte Carlo (MC) method와 Dynamic Programming (DP)의 아이디어를 결합</strong>한 방법이다. TD method는 아래와 같은 특징을 가지고 있다.</p><ul><li>MC method처럼 model 없이 experience로부터 학습이 가능하다. (agent는 environment dynamics $p(s’,r \vert s,a)$를 모른다.)<li>DP처럼 다른 학습된 추정치를 기반으로 추정치를 update한다. 즉, bootstrap이다.<li>Generalized Policy Iteration (GPI)를 따른다.</ul><p>TD method는 DP와 MC method의 치명적 단점들을 극복한 방법이다. GPI의 <em>control</em>에서는 약간의 차이만 있을 뿐 거의 비슷하다. 핵심은 GPI의 <em>prediction</em> 부분이며 여기서 큰 차이를 보인다. TD method가 어떻게 prediction을 다루는지 알아보자.</p><h2 id="td-prediction"><span class="mr-2">TD Prediction</span><a href="#td-prediction" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>TD와 MC method의 공통점은 prediction problem을 해결하기 위해 <strong>sample experience를 사용</strong>한다는 점이다. MC method의 가장 큰 단점은 value function을 추정하기 위해 return을 구해야 했기 때문에 episode의 종료를 기다려야 한다는 문제가 있었다. every-visit MC method의 value function을 추정하는 단순한 update rule은 아래와 같다.</p>\[\begin{align} V(S_t) &amp;\leftarrow V(S_t) + \alpha \Big[G_t - V(S_t) \Big] \\ V(S_t) &amp;\leftarrow (1 - \alpha)V(S_t) + \alpha G_t \end{align}\]<p>$G_t$는 time step $t$에 대한 return으로 MC method의 <em>target</em>이다. $\alpha$는 step-size parameter 혹은 weight이다. 위 update rule을 <em>constant</em>-$\alpha$ MC라고도 부른다.</p><blockquote class="prompt-info"><div><p>참고로 이 포스트에서 일반적인 value function은 소문자로 (e.g. $v$) 표기하며, value function의 추정치임을 명확하게 나타낼 때는 대문자로 (e.g. $V$) 표기한다.</p></div></blockquote><p>위의 첫 번째 update rule은 incremental한 형식으로 일반적인 형태는 아래와 같다.</p>\[\textit{NewEstimate} \leftarrow \textit{OldEstimate} + \textit{StepSize} \Big[\textit{Target} - \textit{OldEstimate} \Big]\]<p>TD method는 앞서 언급했듯이 DP의 <strong>bootstrap</strong> 속성을 가져왔다. TD method는 MC method처럼 episode의 종료를 기다릴 필요 없이 <strong>next time step까지만 기다리면 된다</strong>. next time step $t+1$에서 TD method는 즉시 target을 형성할 수 있다. 아래는 TD method의 간단한 update rule이다.</p>\[V(S_t) \leftarrow V(S_t) + \alpha \Big[R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \Big]\]<p>$R_{t+1}$은 획득한 reward, $\gamma$는 discount factor이다. TD method는 reward $R_{t+1}$과 이미 존재하는 next state value 추정치 $V(S_{t+1})$을 통해 현재 state의 value function을 즉시 업데이트 한다. 특히 업데이트에 사용되는 TD method의 target을 <em>TD target</em>이라고 하고, TD target과 현재 state의 value 추정치와의 차이를 <em>TD error</em>라고 한다. 특히 TD error는 RL에서 중요한 형식으로 RL 전반에 걸쳐 다양한 형태로 나타난다.</p>\[\textit{TD target} \doteq R_{t+1} + \gamma V(S_{t+1})\] \[\textit{TD error } \delta_t \doteq R_{t+1} + \gamma V(S_{t+1}) - V(S_t)\]<p>이러한 TD method를 <em>TD(0)</em> 혹은 <em>one-step TD</em>라고 하는데 TD($\lambda$)나 $n$-step TD method의 특수 case이다. 아래는 TD(0)에 대한 알고리즘이다.</p><blockquote><h5 id="textalgorithm-tabular-td0-for-estimating--v_pi"><span class="mr-2">$\text{Algorithm: Tabular TD(0) for estimating } v_\pi$</span><a href="#textalgorithm-tabular-td0-for-estimating--v_pi" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5><p>\(\begin{align*} &amp; \textstyle \text{Input: the policy } \pi \text{ to be evaluated} \\ &amp; \textstyle \text{Algorithm parameter: step size } \alpha \in (0, 1] \\ &amp; \textstyle \text{Initialize } V(s) \text{, for all } s \in \mathcal{S}^+ \text{, arbitrarily except that } V(\textit{terminal}) = 0 \\ \\ &amp; \textstyle \text{Loop for each episode: } \\ &amp; \textstyle \qquad \text{Initialize } S \\ &amp; \textstyle \qquad \text{Loop for each step of episode:} \\ &amp; \textstyle \qquad\qquad A \leftarrow \text{action given by } \pi \text{ for } S \\ &amp; \textstyle \qquad\qquad \text{Take action } A \text{, observe } R, S' \\ &amp; \textstyle \qquad\qquad V(S) \leftarrow V(S) + \alpha[R + \gamma V(S') - V(S)] \\ &amp; \textstyle \qquad\qquad S \leftarrow S' \\ &amp; \textstyle \qquad \text{until } S \text{ is terminal} \\ \end{align*}\)</p></blockquote><p>아래는 TD(0)에 대한 backup diagram이다.</p><p><img data-src="/assets/images/rl-td-backup-diagram.png" alt="" width="43%" data-proofer-ignore> <em>Fig 1. TD(0) backup diagram.<br /> (Image source: Robotic Sea Bass. <a href="https://roboticseabass.com/2020/08/02/an-intuitive-guide-to-reinforcement-learning/">An Intuitive Guide to Reinforcement Learning</a>.)</em></p><p>backup diagram의 맨 위 state node에 대한 value 추정치는 next state로의 <strong>딱 1번의 sample transition</strong>을 통해 즉각적으로 update된다.</p><h2 id="td-control"><span class="mr-2">TD Control</span><a href="#td-control" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>우리는 앞서 TD prediction을 통해 value function을 추정하는 방법을 알아보았다. 이제 GPI에 따라 TD control을 통해 policy를 update할 것이다. MC method와 마찬가지로 sampling을 통해 학습하기 때문에 exploration과 exploitation에 대한 trade off 관계를 고려해야 하며, 이를 수행하는 방법으로 TD method 역시 on-policy와 off-policy method가 있다.</p><p>MC method에서 state value $v_\pi$를 추정할 경우 environment에 대한 지식인 state transition probability distribution을 알아야 policy improvement를 수행할 수 있었다.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> 이는 TD method에도 동일하게 적용된다. 다행이 이러한 문제는 $v_\pi$ 대신 action value $q_\pi$를 directly하게 추정하면 해결할 수 있다. $q_\pi$를 추정하는 것의 장점은 environment에 대한 지식이 필요가 없어지는 것 뿐만 아니라 <a href="#td-prediction">TD Prediction</a>에서의 state value 추정과 본질적으로 같기 때문에 아래 그림과 같이 <strong>단지 state에서 state-action pair sequence로 대체</strong>하기만 하면 된다.</p><p><img data-src="/assets/images/rl-sutton-state-action-sequence.png" alt="" data-proofer-ignore> <em>Fig 2. State-action pair sequence.<br /> (Image source: Sec 6.4 Sutton &amp; Barto (2018).)</em></p><p>앞으로 알아볼 TD method algorithm은 모두 action value $q_\pi$를 추정한다. 이 때 TD method는 bootstrap하기 때문에 다른 학습된 next state에서의 action value 추정치를 기반으로 현재 state-action pair의 $Q(s,a)$를 추정한다. 따라서 TD method algorithm들은 <strong>다른 학습된 action value 추정치를 고려하는 방식에 따라 구분</strong>된다. 조금 더 구체적으로 얘기하자면, TD method를 target policy와 behavior policy 관점에서 볼 때 현재 update하려는 state-action pair는 behavior policy에 의해 선택되고, <strong>다른 학습된 action value 추정치에 대한 선택은 target policy에 의해 이루어진다</strong>. 이 target policy를 어떻게 설정하느냐에 따라 algorithm들이 구분된다.</p><h2 id="sarsa"><span class="mr-2">Sarsa</span><a href="#sarsa" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Sarsa는 가장 기본적인 TD on-policy method이다. 현재 state-action pair의 action value $Q(S_t,A_t)$를 추정할 때 다른 학습된 next state-action pair에 대한 $Q(S_{t+1},A_{t+1})$을 <strong>현재 policy $\pi$에 따라 선택</strong>한다. 이에 대한 update rule은 아래와 같다.</p>\[Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \Big[R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \Big]\]<p>당연하지만 $S_{t+1}$이 terminal state일 경우 $Q(S_{t+1}, A_{t+1})$은 0이다. 위 update rule을 <a href="#td-prediction">TD Prediction</a>에서 보았던 TD method의 state value에 대한 update rule과 비교해볼 때 단지 state value 추정치 $V(S)$를 action value $Q(S, A)$로 대체했을 뿐임을 확인할 수 있다.</p><p>Sarsa를 target policy와 behavior policy 관점에서 살펴보자. Sarsa는 on-policy method이기 때문에 <strong>target policy와 behavior policy가 동일</strong>하다.<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup> 따라서 next state-action pair에 대한 $Q(S_{t+1}, A_{t+1})$을 현재 behavior policy $\pi$에 따라 선택한다. 원래 behavior policy는 $b$로 나타내지만 on-policy method이기 떄문에 $\pi = b$이다. 아래는 위 update rule의 backup diagram이다.</p><p><img data-src="/assets/images/rl-sutton-sarsa-backup-diagram.png" alt="" data-proofer-ignore> <em>Fig 3. Sarsa backup diagram.<br /> (Image source: Sec 6.4 Sutton &amp; Barto (2018).)</em></p><p>모든 on-policy method에서는 experience 생성에 사용된 behavior policy $\pi$에 대한 $q_\pi$를 추정함과 동시에, 추정된 $q_\pi$에 관해 behavior policy $\pi$를 greedy한 방향으로 update한다. Sarsa가 수렴하기 위해서는 exploration이 잘 수행되어야 하기 때문에 주로 $\epsilon$-soft policy류의 방법을 사용한다. 아래는 Sarsa algorithm이다.</p><blockquote><h5 id="textalgorithm-sarsa-on-policy-td-control-for-estimating--q-approx-q_ast"><span class="mr-2">$\text{Algorithm: Sarsa (on-policy TD control) for estimating } Q \approx q_\ast$</span><a href="#textalgorithm-sarsa-on-policy-td-control-for-estimating--q-approx-q_ast" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5><p>\(\begin{align*} &amp; \textstyle \text{Algorithm parameters: step size } \alpha \in (0,1], \text{ small } \epsilon &gt; 0 \\ &amp; \textstyle \text{Initialize } Q(s,a) \text{, for all } s \in \mathcal{S}^+, a \in \mathcal{A}(s) \text{, arbitrarily except that } Q(\textit{terminal},\cdot) = 0 \\ \\ &amp; \textstyle \text{Loop for each episode:} \\ &amp; \textstyle \qquad \text{Initialize } S \\ &amp; \textstyle \qquad \text{Choose } A \text{ from } S \text{ using policy derived from } Q \text{ (e.g., } \epsilon \text{-greedy)} \\ &amp; \textstyle \qquad \text{Loop for each step of episode:} \\ &amp; \textstyle \qquad\qquad \text{Take action } A \text{, observe } R, S' \\ &amp; \textstyle \qquad\qquad \text{Choose } A' \text{ from } S' \text{ using policy derive from } Q \text{ (e.g., } \epsilon \text{-greedy)} \\ &amp; \textstyle \qquad\qquad Q(S,A) \leftarrow Q(S,A) + \alpha [R + \gamma Q(S',A') - Q(S,A)] \\ &amp; \textstyle \qquad\qquad S \leftarrow S'; \ A \leftarrow A'; \\ &amp; \textstyle \qquad \text{until } S \text{ is terminal} \end{align*}\)</p></blockquote><p>아래는 위 algorithm을 구현한 소스 코드이다.</p><blockquote class="prompt-info"><div><p>Windy Gridworld<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup> training with Sarsa: <a href="https://github.com/DevSlem/rl-algorithm/blob/main/trainings/windy_gridworld_with_sarsa.ipynb">DevSlem/rl-algorithm (Github)</a></p></div></blockquote><h2 id="q-learning"><span class="mr-2">Q-learning</span><a href="#q-learning" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Q-learning은 RL에서 가장 기본적이면서도 가장 중요한 알고리즘 중 하나이다. Q-learning은 off-policy TD method이며 아래와 같이 정의된다.</p>\[Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \Big[R_{t+1} + \gamma \max_a Q(S_{t+1},a) - Q(S_t,A_t) \Big]\]<p>Q-learning과 Sarsa의 가장 주요한 차이는 TD error를 구성할 때 next state-action pair의 value function $Q$를 선택하는 기준이다. Sarsa는 next state $S_{t+1}$에서 현재 policy $\pi$를 따라 action value를 선택했다면, Q-learning은 현재 policy와 상관 없이 <strong>next state에서의 maximum action value $\max_a Q(S_{t+1},a)$를 선택</strong>한다. Q-learning의 backup diagram은 아래와 같으며 Sarsa의 backup diagram과 비교해보길 바란다.</p><p><img data-src="/assets/images/rl-sutton-q-learning-backup-diagram.png" alt="" width="25%" data-proofer-ignore> <em>Fig 4. Q-learning backup diagram.<br /> (Image source: Sec 6.5 Sutton &amp; Barto (2018).)</em></p><p>위 backup diagram에서 화살표 사이를 이어주는 선은 greedy selection을 의미한다.</p><p>Q-learning을 target policy와 behavior policy 관점에서 살펴보자. Q-learning은 off-policy method로 target policy와 behavior policy가 분리된다.<sup id="fnref:2:1" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup> Q-learning에서 next state-action pair에 대한 value function $Q$를 고려할 때 greedy하게 고려하기 때문에 <strong>target policy는 greedy policy</strong>이다. behavior policy는 exploration을 충분히 수행할 수 있는 임의의 policy (e.g. $\epsilon$-soft policy)이다. 아래는 Q-learning algorithm이다.</p><blockquote><h5 id="textalgorithm-q-learning-off-policy-td-control-for-estimating--pi-approx-pi_ast"><span class="mr-2">$\text{Algorithm: Q-learning (off-policy TD control) for estimating } \pi \approx \pi_\ast$</span><a href="#textalgorithm-q-learning-off-policy-td-control-for-estimating--pi-approx-pi_ast" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5><p>\(\begin{align*} &amp; \textstyle \text{Algorithm parameters: step size } \alpha \in (0,1] \text{, small } \epsilon &gt; 0 \\ &amp; \textstyle \text{Initialize } Q(s,a) \text{, for all } s \in \mathcal{S}^+, a \in \mathcal{A}(s) \text{, arbitrarily except that } Q(\textit{terminal}, \cdot) = 0 \\ \\ &amp; \textstyle \text{Loop for each episode:} \\ &amp; \textstyle \qquad \text{Initialize } S \\ &amp; \textstyle \qquad \text{Loop for each step of episode:} \\ &amp; \textstyle \qquad\qquad \text{Choose } A \text{ from } S \text{ using policy derived from } Q \text{ (e.g., } \epsilon \text{-greedy)} \\ &amp; \textstyle \qquad\qquad \text{Take action, observe } R,S' \\ &amp; \textstyle \qquad\qquad Q(S,A) \leftarrow Q(S,A) + \alpha [R + \gamma \max_a Q(S',a) - Q(S,A)] \\ &amp; \textstyle \qquad\qquad S \leftarrow S' \\ &amp; \textstyle \qquad \text{until } S \text{ is terminal} \end{align*}\)</p></blockquote><p>아래는 Q-learning과 Sarsa algorithm을 구현한 뒤 비교하는 소스 코드이다.</p><blockquote class="prompt-info"><div><p>Cliff Walking<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup> training with both Q-learning and Sarsa: <a href="https://github.com/DevSlem/rl-algorithm/blob/main/trainings/train_cliff_walking.ipynb">DevSlem/rl-algorithm (Github)</a></p></div></blockquote><h2 id="expected-sarsa"><span class="mr-2">Expected Sarsa</span><a href="#expected-sarsa" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Expected Sarsa는 Q-learning과 유사한 알고리즘이다. Q-learning이 TD error를 구성할 때 next state-action pair들의 maximum action value를 고려했다면 Expected Sarsa는 <strong>target policy $\pi$를 따랐을 때의 next state-action pair에 대한 expected value를 고려</strong>한다. 아래 update rule을 보면 조금 더 쉽게 이해할 수 있다.</p>\[\begin{align} Q(S_t, A_t) &amp;\leftarrow Q(S_t, A_t) + \alpha \Big[R_{t+1} + \gamma \mathbb{E}_\pi[Q(S_{t+1},A_{t+1}) \ \vert \ S_{t+1}] - Q(S_t,A_t) \Big] \\ &amp;\leftarrow Q(S_t, A_t) + \alpha \Big[R_{t+1} + \gamma \sum_a \pi(a \vert S_{t+1}) Q(S_{t+1},a) - Q(S_t,A_t) \Big] \end{align}\]<p>Expected Sarsa는 on-policy일까 off-policy method일까? 정답은 둘다 될 수 있다이다. target policy $\pi$를 어떻게 설정하느냐에 따라 달라진다. target policy와 behavior policy가 다르면 off-policy이고 같으면 on-policy method이다. 단지 그 뿐이다. 예를 들면 behavior policy $b$가 $\epsilon$-greedy policy라고 할 때 target policy $\pi$도 $\epsilon$-greedy policy이면 $\pi = b$인 on-policy method이며 expected value는 $\epsilon$-greedy policy에 관해 계산된다. 반대로 $\pi \neq b$인 off-policy method이며 target policy가 greedy policy라면 어떨까? greedy action을 제외한 나머지 action들의 확률은 0이기 때문에 expected value는 next state-action pair에 대한 maximum action value이다. 즉, Q-learning과 동일해진다. 이처럼 Expected Sarsa는 flexible하다는 것을 알 수 있으며 모든 action들을 고려하는 expected value이기 떄문에 Sarsa에 비해 분산이 작다.</p><p>아래는 Expected Sarsa의 backup diagram이다. Sarsa와 비교했을 때 모든 action을 고려하고 있음을 확인할 수 있다.</p><p><img data-src="/assets/images/rl-sutton-expected-sarsa-backup-diagram.png" alt="" width="30%" data-proofer-ignore> <em>Fig 5. Expected Sarsa backup diagram.<br /> (Image source: Sec 6.6 Sutton &amp; Barto (2018).)</em></p><p>Expected Sarsa는 위 Q-learning과 거의 구조가 동일하기 때문에 따로 algorithm을 올리지는 않겠다. 대신 아래에 소스코드를 첨부한다. 여기서는 target policy와 behavior policy가 동일한 on-policy Expected Sarsa를 구현했다. Expected Sarsa의 update rule은 <code class="language-plaintext highlighter-rouge">update()</code> 메서드에 구현되어 있다.</p><blockquote class="prompt-info"><div><p>Expected Sarsa source code: <a href="https://github.com/DevSlem/rl-algorithm/blob/main/rl/rl_algorithm/expected_sarsa.py">DevSlem/rl-algorithm (Github)</a></p></div></blockquote><h2 id="double-q-learning"><span class="mr-2">Double Q-learning</span><a href="#double-q-learning" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>기존 Q-learning의 가장 큰 문제는 biased하다는 점이다. 이로 인해 특히 stochastic한 environment에서 action value들에 대한 overestimation으로 인해 매우 나쁜 performance를 보인다. 아래 stochastic environment에 대한 예제를 먼저 살펴보자.</p><p><img data-src="/assets/images/rl-sutton-doubleq-stochastic-env.png" alt="" width="60%" data-proofer-ignore> <em>Fig 6. Simple stochastic environment.<br /> (Image source: Sec 6.7 Sutton &amp; Barto (2018).)</em></p><p>agent는 항상 state A에서 시작한다. A에서 right action을 선택하면 reward 0과 함께 즉시 episode는 종료된다. left action을 선택하면 reward 0과 함께 state B로 전이된다. state B에서는 episode를 즉시 종료할 수 있는 수 많은 action들이 존재한다. 이 때 각 action들을 선택함으로써 얻게 되는 reward는 normal distribution $N(-0.1,1)$을 따른다. 즉, stochastic한 environment이다.</p><p>그렇다면 위 environment에서 왜 Q-learning은 매우 나쁜 performance를 보일까? state B에서 획득할 수 있는 reward는 $N(-0.1, 1)$을 따르기 떄문에 state A에서 left action을 선택했을 때의 expected return은 -0.1이 될 것이다. 반대로 right action의 expected return은 0이다. 그런데 Q-learning의 training 초기에 state B에서 action을 선택했을 떄 $N(-0.1, 1)$에 따라 reward를 어떤 큰 양수 (e.g. $+2$) 값으로 주로 획득했다면 어떻게 될까? training 초기이기 때문에 아직 sampling된 data가 부족해 획득된 reward의 expected value는 양수일 것이다. Q-learning은 maximum next state-action pair value를 선택한다. 이는 training 초기에 state A에서 실제 optimal action인 right가 아닌 left action을 선택하도록 유도할 것이다. 즉, training 속도는 저하될 것이다.</p><p>위 문제를 해결하도록 고안된 것이 Double Q-learning algorithm이다. 기존 Q-learning과 다르게 Double Q-learning은 action value를 2개로 나누어 추정한다. 즉, $Q_1, Q_2$를 추정하는 방법이다. 아래는 Q-learning과 Double Q-learning의 performance 비교이다. state A에서 left action을 선택하는 비율이 $y$값이며 이 값이 작을 수록 optimal하다.</p><p><img data-src="/assets/images/rl-sutton-q-vs-doubleq.png" alt="" width="80%" data-proofer-ignore> <em>Fig 7. Comparison of Q-learning and Double Q-learning.<br /> (Image source: Sec 6.7 Sutton &amp; Barto (2018).)</em></p><p>위 그림을 보면 알겠지만 Q-learning은 training 초기에 left action을 overestimation하여 left action 쪽으로 편향된 모습을 확인할 수 있다. 반대로 Double Q-learning은 training 초기부터 안정적이며 Q-learning에 비해 훨씬 빠르게 optimal에 도달한다. 이에 대한 자세한 직관적 설명은 첨부된 블로그를<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup>, 수식적 증명은 논문<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">6</a></sup>을 찾아보길 바란다.</p><p>앞서 Double Q-learning은 두 개의 action value $Q_1, Q_2$를 추정한다고 언급했다. 이 둘은 일종의 <strong>서로를 보완하는 역할</strong>을 한다. $Q_1$을 update하고 싶다고 할 때 TD error를 구성하기 위해 next state-action pair value가 필요하다. Double Q-learning 역시 Q-learning이기 때문에 target policy는 greedy policy로, next state에서 고려할 action은 $Q_1$에 대한 greedy action $A^\ast = \arg\max_aQ_1(S_{t+1},a)$이다. 기존 Q-learning에서는 TD error를 구성할 때 $A^\ast$에 대한 action value $Q_1(S_{t+1}, A^\ast)$를 고려했었다. Double Q-learning에서는 $A^\ast$에 대해 $Q_1$이 아닌 $Q_2(S_{t+1},A^\ast)$를 고려한다. 즉, <strong>$Q_1$을 update하기 위해서 $Q_2$ 추정치를 고려</strong>한다. $Q_2$를 update할 때는 반대이다. 이를 정리한 $Q_1$에 대한 update rule은 아래와 같다.</p>\[Q_1(S_t,A_t) \leftarrow Q_1(S_t,A_t) + \alpha \Big[R_{t+1} + \gamma Q_2\big(S_{t+1}, \underset{a}{\arg\max} \ Q_1(S_{t+1},a) \big) - Q_1(S_t,A_t)]\]<p>$Q_2$를 update할 때는 위 update rule에서 $Q_1$과 $Q_2$를 서로 바꿔주기만 하면 된다. $Q_1$과 $Q_2$는 당연하지만 둘이 같은 값을 가지도록 수렴할 것이다. behavior policy는 보통 $Q_1$과 $Q_2$를 모두 고려한다. 가장 간단한 방법은 behavior policy가 $Q_1 + Q_2$에 대해 action을 선택하는 것이다. $Q_1$과 $Q_2$의 update 역시 여러 가지 방법이 있겠지만 가장 간단한 방법은 각 episode의 time step $t$마다 0.5의 확률로 랜덤하게 update하는 것이다. 이에 대한 algorithm은 아래와 같다.</p><blockquote><h5 id="textalgorithm-double-q-learning-for-estimating--q_1-approx-q_2-approx-q_ast"><span class="mr-2">$\text{Algorithm: Double Q-learning, for estimating } Q_1 \approx Q_2 \approx q_\ast$</span><a href="#textalgorithm-double-q-learning-for-estimating--q_1-approx-q_2-approx-q_ast" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5><p>\(\begin{align*} &amp; \textstyle \text{Algorithm parameters: step size } \alpha \in (0,1] \text{, small } \epsilon &gt; 0 \\ &amp; \textstyle \text{Initialize } Q_1(s,a) \text{ and } Q_2(s,a) \text{, for all } s \in \mathcal{S}^+, a \in \mathcal{A}(s) \text{, such that } Q(\textit{terminal}, \cdot) = 0 \\ \\ &amp; \textstyle \text{Loop for each episode:} \\ &amp; \textstyle \qquad \text{Initialize } S \\ &amp; \textstyle \qquad \text{Loop for each step of episode:} \\ &amp; \textstyle \qquad\qquad \text{Choose } A \text{ from } S \text{ using the policy } \epsilon \text{-greedy in } Q_1 + Q_2 \\ &amp; \textstyle \qquad\qquad \text{Take action } A \text{, observe } R, S' \\ &amp; \textstyle \qquad\qquad \text{With 0.5 probability:} \\ &amp; \textstyle \qquad\qquad\qquad Q_1(S,A) \leftarrow Q_1(S,A) + \alpha \Big(R + \gamma Q_2 \big(S', \arg\max_a Q_1(S',a) \big) - Q_1(S,A) \Big) \\ &amp; \textstyle \qquad\qquad \text{else:} \\ &amp; \textstyle \qquad\qquad\qquad Q_2(S,A) \leftarrow Q_2(S,A) + \alpha \Big(R + \gamma Q_1 \big(S', \arg\max_a Q_2(S',a) \big) - Q_2(S,A) \Big) \\ &amp; \textstyle \qquad\qquad S \leftarrow S' \\ &amp; \textstyle \qquad \text{until } S \text{ is terminal} \end{align*}\)</p></blockquote><p>아래는 위 update rule을 구현한 소스 코드이다. <code class="language-plaintext highlighter-rouge">update()</code> 메서드에 구현되어있다.</p><blockquote class="prompt-info"><div><p>Double Q-learning source code: <a href="https://github.com/DevSlem/rl-algorithm/blob/main/rl/rl_algorithm/double_q_learning.py">DevSlem/rl-algorithm (Github)</a></p></div></blockquote><h2 id="summary"><span class="mr-2">Summary</span><a href="#summary" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>지금까지 TD method에 대해 알아보았다. TD method는 MC method와 DP의 아이디어를 결합한 방식이다. MC method 처럼 environment에 대한 지식 없이 sampling을 통해 학습하며 DP와 같이 bootstrap한 속성을 가진다. TD method 역시 GPI를 따른다. TD prediction에서 TD error는 굉장히 중요한 수식으로 RL 전반에 걸쳐 등장한다. TD method 역시 on-policy와 off-policy로 구분되며 on-policy에는 Sarsa, off-policy에는 Q-learning이 있다. 특히 Q-learning은 굉장히 중요한 algorithm이다. 그 외에도 위 두 algorithm을 개선한 Expected Sarsa, Double Q-learning을 알아보았다.</p><h2 id="references"><span class="mr-2">References</span><a href="#references" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>[1] Richard S. Sutton and Andrew G. Barto. <a href="/assets/materials/Reinforcement%20Learning%20An%20Introduction;%202nd%20Edition.%202018.pdf">Reinforcement Learning: An Introduction; 2nd Edition. 2018</a>.</p><h2 id="footnotes"><span class="mr-2">Footnotes</span><a href="#footnotes" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><div class="footnotes" role="doc-endnotes"><ol><li id="fn:1" role="doc-endnote"><p>DevSlem. <a href="../monte-carlo-methods/#monte-carlo-estimation-of-action-values">Monte Carlo Estimation of Action Values</a>. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:2" role="doc-endnote"><p>DevSlem. <a href="../monte-carlo-methods/#off-policy-methods">Off-policy methods</a>. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:2:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p><li id="fn:3" role="doc-endnote"><p>Reinforcement Learning: An Introduction; 2nd Edition. 2018. <a href="https://devslem.github.io/assets/materials/Reinforcement%20Learning%20An%20Introduction;%202nd%20Edition.%202018.pdf#page=152">Sec. 6.4, p.152; Example 6.5: Windy Gridworld</a>. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:4" role="doc-endnote"><p>Reinforcement Learning: An Introduction; 2nd Edition. 2018. <a href="https://devslem.github.io/assets/materials/Reinforcement%20Learning%20An%20Introduction;%202nd%20Edition.%202018.pdf#page=154">Sec. 6.5, p.154; Example 6.6: Cliff Walking</a>. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:5" role="doc-endnote"><p>Towards Data Science. Ziad SALLOUM. <a href="https://towardsdatascience.com/double-q-learning-the-easy-way-a924c4085ec3">Double Q-Learning, the Easy Way</a>. <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:6" role="doc-endnote"><p>Hado van Hasselt. <a href="https://proceedings.neurips.cc/paper/2010/file/091d584fced301b442654dd8c23b3fc9-Paper.pdf">Double Q-learning</a>. <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p></ol></div></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/reinforcement-learning/'>Reinforcement Learning</a>, <a href='/categories/rl-fundamental/'>RL Fundamental</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/rl/" class="post-tag no-text-decoration" >RL</a> <a href="/tags/ai/" class="post-tag no-text-decoration" >AI</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Temporal-Difference+Learning+-+DevSlem+Blog&url=https%3A%2F%2Fdevslem.github.io%2Freinforcement-learning%2Frl-fundamental%2Ftemporal-difference-learning%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Temporal-Difference+Learning+-+DevSlem+Blog&u=https%3A%2F%2Fdevslem.github.io%2Freinforcement-learning%2Frl-fundamental%2Ftemporal-difference-learning%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fdevslem.github.io%2Freinforcement-learning%2Frl-fundamental%2Ftemporal-difference-learning%2F&text=Temporal-Difference+Learning+-+DevSlem+Blog" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div><script src="https://utteranc.es/client.js" repo="DevSlem/DevSlem.github.io" issue-term="pathname" label="comment" theme="github-dark" crossorigin="anonymous" async> </script></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/reinforcement-learning/drl-methods/dqn/">DQN: Deep Q-Networks</a><li><a href="/reinforcement-learning/drl-methods/dtqn/">DTQN: Deep Transformer Q-Networks for Partially Observable Reinforcement Learning</a><li><a href="/wsl-setup/">Windows Subsystem for Linux (WSL) Setup for Reinforcement Learning</a><li><a href="/algorithm/sorting/merge-sort/">[알고리즘] 합병 정렬</a><li><a href="/algorithm/sorting/shell-sort/">[알고리즘] 쉘 정렬</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/rl/">RL</a> <a class="post-tag" href="/tags/algorithm/">Algorithm</a> <a class="post-tag" href="/tags/unity/">Unity</a> <a class="post-tag" href="/tags/function-approximation-rl/">Function Approximation RL</a> <a class="post-tag" href="/tags/sorting/">Sorting</a> <a class="post-tag" href="/tags/enemy/">Enemy</a> <a class="post-tag" href="/tags/game-development/">Game Development</a> <a class="post-tag" href="/tags/project/">Project</a> <a class="post-tag" href="/tags/attribute/">Attribute</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 pl-3 pr-3 pr-xl-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/reinforcement-learning/rl-fundamental/policy-gradient-methods/"><div class="card-body"> <em class="small" data-ts="1665586800" data-df="ll" > Oct 13, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Policy Gradient Methods</h3><div class="text-muted small"><p> 드디어 긴 장정 끝에 대망의 마지막 챕터로 왔다. 이번 포스트에서는 그 유명한 policy gradient method에 대해 소개하려고 한다. 길고 긴 여정이 드디어 끝났다. Introduction 지금까지 우리는 value-based 기반의 방법을 다뤘었다. policy는 추정된 action value에 기반해 action을 선택했다. 이번에는...</p></div></div></a></div><div class="card"> <a href="/reinforcement-learning/rl-fundamental/multi-armed-bandits/"><div class="card-body"> <em class="small" data-ts="1653058800" data-df="ll" > May 21, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Multi-armed Bandits</h3><div class="text-muted small"><p> 이 포스트에서는 Reinforcement learning (RL)의 기본 내용인 Multi-armed Bandits 환경과 기본적인 아이디어들에 대해 알아본다. Reinforcement learning vs others Reinforcement learning (RL)과 다른 learning의 가장 큰 구별점은 사용하는 정보의 차이에 있다. 다른 ...</p></div></div></a></div><div class="card"> <a href="/reinforcement-learning/rl-fundamental/finite-markov-decision-processes/"><div class="card-body"> <em class="small" data-ts="1653836400" data-df="ll" > May 30, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Finite Markov Decision Processes</h3><div class="text-muted small"><p> 이 포스트에서는 Reinforcement Learning에서 기반이 되는 finite Markov Decision Processes (MDPs)와 finite MDPs 문제를 해결하기 위한 Bellman equations에 대해 소개한다. What is MDPs Markov Decision Processes (MDPs)는 연속적인 의사 결정을 형식...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/reinforcement-learning/rl-fundamental/monte-carlo-methods/" class="btn btn-outline-primary" prompt="Older"><p>Monte Carlo Methods in RL</p></a> <a href="/reinforcement-learning/rl-fundamental/n-step-bootstrapping/" class="btn btn-outline-primary" prompt="Newer"><p>n-step Bootstrapping</p></a></div></div></div><footer class="row pl-3 pr-3"><div class="col-12 d-flex justify-content-between align-items-center text-muted pl-0 pr-0"><div class="footer-left"><p class="mb-0"> © 2024 <a href="https://github.com/username">DevSlem</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/rl/">RL</a> <a class="post-tag" href="/tags/algorithm/">Algorithm</a> <a class="post-tag" href="/tags/unity/">Unity</a> <a class="post-tag" href="/tags/function-approximation-rl/">Function Approximation RL</a> <a class="post-tag" href="/tags/sorting/">Sorting</a> <a class="post-tag" href="/tags/enemy/">Enemy</a> <a class="post-tag" href="/tags/game-development/">Game Development</a> <a class="post-tag" href="/tags/project/">Project</a> <a class="post-tag" href="/tags/attribute/">Attribute</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a><div id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-animation="true" data-autohide="false"><div class="toast-header"> <button type="button" class="ml-2 ml-auto close" data-dismiss="toast" aria-label="Close"> <span aria-hidden="true">&times;</span> </button></div><div class="toast-body text-center pt-0"><p class="pl-2 pr-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></div><script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] }, svg: { scale: 1.1 } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script>
