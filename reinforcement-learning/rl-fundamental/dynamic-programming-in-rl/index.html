<!DOCTYPE html><html lang="en" data-mode="dark" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="Dynamic Programming in RL" /><meta property="og:locale" content="en" /><meta name="description" content="RL에서 optimal policy를 구하는데 사용되는 DP를 소개한다." /><meta property="og:description" content="RL에서 optimal policy를 구하는데 사용되는 DP를 소개한다." /><link rel="canonical" href="https://devslem.github.io/reinforcement-learning/rl-fundamental/dynamic-programming-in-rl/" /><meta property="og:url" content="https://devslem.github.io/reinforcement-learning/rl-fundamental/dynamic-programming-in-rl/" /><meta property="og:site_name" content="DevSlem Blog" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-06-25T00:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Dynamic Programming in RL" /><meta name="google-site-verification" content="SSJwUK1JSPZV3wjQS19f2bXfHYIupbYEFsisH4Nuns4" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-07-02T00:00:00+09:00","datePublished":"2022-06-25T00:00:00+09:00","description":"RL에서 optimal policy를 구하는데 사용되는 DP를 소개한다.","headline":"Dynamic Programming in RL","mainEntityOfPage":{"@type":"WebPage","@id":"https://devslem.github.io/reinforcement-learning/rl-fundamental/dynamic-programming-in-rl/"},"url":"https://devslem.github.io/reinforcement-learning/rl-fundamental/dynamic-programming-in-rl/"}</script><title>Dynamic Programming in RL | DevSlem Blog</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="DevSlem Blog"><meta name="application-name" content="DevSlem Blog"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> </a></div><div class="site-title mt-3"> <a href="/">DevSlem Blog</a></div><div class="site-subtitle font-italic">It's a blog where i write posts about AI, RL, Unity, etc.</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/DevSlem" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['devslem12','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper"><div id="topbar" class="container d-flex align-items-center justify-content-between h-100 pl-3 pr-3 pl-md-4 pr-md-4"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Dynamic Programming in RL</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper" class="d-flex justify-content-center"><div id="main" class="container pl-xl-4 pr-xl-4"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pr-xl-4"><div class="post pl-1 pr-1 pl-md-2 pr-md-2"><h1 data-toc-skip>Dynamic Programming in RL</h1><div class="post-meta text-muted"> <span> Posted <em class="" data-ts="1656082800" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Jun 25, 2022 </em> </span> <span> Updated <em class="" data-ts="1656720000" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Jul 2, 2022 </em> </span><div class="d-flex justify-content-between"> <span> By <em> <a href="https://github.com/username">DevSlem</a> </em> </span><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="3613 words"> <em>20 min</em> read</span></div></div></div><div class="post-content"><p>이 포스트에서는 RL에서 MDP로 environment의 perfect model이 주어졌을 때 optimal policy를 구하는데 사용되는 기초적인 방식인 Dynamic Programming (DP)를 소개한다.</p><h2 id="introduction"><span class="mr-2">Introduction</span><a href="#introduction" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p><em>Reinforcement Learning</em> (RL)에서 environment가 perfect model로 environment의 지식 (일반적으로 MDP)을 완전히 알 수 있다면 <em>Dynamic Programming</em> (DP)를 활용해 optimal policy를 계산할 수 있다. 즉, 모든 가능한 transition의 probability distribution을 완전히 알고 있어야 한다. 이는 굉장히 특수한 상황이다. 우리는 보통 어떤 state에서 action을 선택해 next state로의 transition이 발생했을 때 next state로 transition되었다는 결과만 안다. next state가 얼마나 존재하고 각 next state로 transition될 확률이 어느정도인지 알지 못한다. 이때는 sampling으로 획득한 environment에 대한 experience를 통해 RL 문제를 해결한다.</p><p>그렇다면 DP란 무엇일까? DP는 복잡한 문제를 간단한 여러 개의 문제로 나누어 푸는 최적화 기법이다. DP는 일반적으로 아래와 같은 2가지 유형의 문제에 적용된다.</p><ol><li><a href="https://www.geeksforgeeks.org/overlapping-subproblems-property-in-dynamic-programming-dp-1/">Overlapping Subproblems</a><li><a href="https://www.geeksforgeeks.org/optimal-substructure-property-in-dynamic-programming-dp-2/">Optimal Substructure</a></ol><p>Overlapping Subproblems는 동일한 sub problem들이 반복적으로 요구될 때 연산 결과를 저장했다가 사용할 수 있음을 의미한다. Optimal Substructure은 주어진 문제를 sub problem들로 쪼갠 뒤 각각의 sub problem들의 최적해를 사용하여 원래 문제의 최적해를 구할 수 있음을 의미한다.</p><p>RL에 적용되는 DP의 핵심 아이디어는 위 2가지 특성을 모두 반영해 <strong>Bellman equation을 update rule로 전환</strong>하는 것이다. 이를 통해 value function을 근사시켜 RL 문제를 해결할 수 있다. 이 과정이 어떻게 진행되는지 알아보자.</p><h2 id="generalized-policy-iteration"><span class="mr-2">Generalized Policy Iteration</span><a href="#generalized-policy-iteration" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p><em>Generalized Policy Iteration</em> (GPI)는 RL 문제를 풀 때 사용되는 일반적인 접근 방법이다. GPI에는 다음과 같은 2가지 과정이 존재한다.</p><ol><li>Policy Evaluation - policy $\pi$를 따를 때 value function $v_\pi$를 계산<li>Policy Improvement - 계산된 현재 value function을 통해 policy $\pi$를 개선</ol><p>위 policy evaluation과 policy improvement는 번갈아 가며 수행된다. 아래 다이어그램을 보면 조금 더 직관적으로 이해할 수 있다.</p><p><img data-src="/assets/images/rl-sutton-gpi-diagram.png" alt="" width="30%" data-proofer-ignore> <em>Fig 1. GPI diagram.<br /> (Image source: Sec. 4.6 Sutton &amp; Barto (2018).)</em></p><p>policy evaluation과 policy improvement는 서로 경쟁하면서 협력하는 관계로 볼 수 있다. 이 둘은 서로를 반대 방향으로 잡아 당긴다. policy evaluation에서는 현재 policy $\pi$에 관해 value function $v_\pi$가 계산된다. 즉, value function을 policy 쪽으로 끌어당긴 셈이다. 반대로 policy improvement에서는 value function $v_\pi$에 관해 policy $\pi$가 개선되기 때문에 policy를 value function 쪽으로 끌어 당겼다. 이렇게 서로 끌어당기다 보면 어느 시점에 한 지점에 도달하게 되고 이 때가 바로 optimal value function과 policy이다. 이러한 관계를 나타내는 그림은 아래와 같다.</p><p><img data-src="/assets/images/rl-sutton-gpi-relationship.png" alt="" width="50%" data-proofer-ignore> <em>Fig 2. GPI relationship.<br /> (Image source: Sec. 4.6 Sutton &amp; Barto (2018).)</em></p><p>몰론 실제로는 엄청나게 복잡한 과정이 내부에서 발생하지만 직관적으로 위와 같이 GPI를 이해할 수 있다. GPI에 대해 알아보았으니 이제 DP에서 GPI가 어떻게 적용되는지 알아보자.</p><h2 id="policy-evaluation-prediction"><span class="mr-2">Policy Evaluation (Prediction)</span><a href="#policy-evaluation-prediction" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>DP에서의 <em>policy evaluation</em>에 대해 알아보자. policy evaluation은 <em>prediction</em>으로 불리기도 한다. 먼저 Bellman equation을 리마인드하자.</p>\[\begin{align} v_\pi(s) &amp;\doteq \mathbb{E}_\pi[R_{t+1} + \gamma v_\pi(S_{t+1}) \ \vert \ S_t = s] \\ &amp;= \sum_a\pi(a \vert s) \sum_{s',r}p(s',r \vert s,a) \Big[r + \gamma v_\pi(s') \Big] \tag{1} \end{align}\]<p>DP에서는 위 Bellman equation을 아래와 같은 연속적인 update rule로 적용해 value function $v(s)$를 풀 수 있다.</p>\[\begin{align} v_{k+1}(s) &amp;\doteq \mathbb{E}_\pi[R_{t+1} + \gamma v_k(S_{t+1}) \ \vert \ S_t = s] \\ &amp;= \sum_a\pi(a \vert s) \sum_{s',r}p(s',r \vert s,a) \Big[r + \gamma v_\pi(s') \Big] \tag{2} \end{align}\]<p>DP에서는 위 update rule이 모든 state $s \in \mathcal{S}$에 대해 수행되며 $v_k$는 일반적으로 $k \rightarrow \infty$이면 수렴한다. 이 때 <strong>terminal state의 value는 항상 0</strong>이어야 한다. 이렇게 iterative하게 value function을 구하는 알고리즘을 <em>iterative policy evaluation</em>이라고 부른다.</p><p>iterative policy evaluation이 DP인 이유는 update rule을 수행하는 방식 때문이다. 기존 Bellman equation (1)을 Exhaustive search를 통해 푼다고 생각해보자. 현재 state $s$의 value $v_\pi(s)$는 transition된 next state $s’$의 $v_\pi(s’)$으로부터 계산된다. $v_\pi(s’)$은 다시 transition된 s’의 후속 state $s’‘$의 $v_\pi(s’’)$로부터 계산된다. 즉, 재귀적 관계로 연속된 모든 후속 MDP를 고려해야하는 굉장히 비효율적인 방식이다.</p><p>DP에서는 기존 Bellman equation을 update rule로 전환해 iterative하게 만들었다. 이때 연속된 모든 후속 state를 관찰하는게 아니라 현재 state $s$에서 가능한 next state들에 대한 <strong>one-step transition</strong>만을 고려한다. 현재 state의 new value $v_{k+1}(s)$는 후속 state $s’$의 old value $v_k(s’)$로부터 계산되며 다시 재귀적으로 계산하지 않고 <strong>지금까지 계산된 $v_k(s’)$을 그대로 사용</strong>하겠다는 것이다. 즉, optimal policy를 찾기 위해 각 state별로 쪼개 optimal state value를 구하며 이때 지금까지 구한 state value를 저장해 놓았다가 다음 iteration에서 연산 시 사용하기 때문에 DP의 2가지 특성을 모두 지니고 있다. 아래 그림을 보자.</p><p><img data-src="/assets/images/rl-dp-backup-diagram.png" alt="" width="40%" data-proofer-ignore> <em>Fig 3. DP backup diagram.<br /> (Image source: Robotic Sea Bass. <a href="https://roboticseabass.com/2020/08/02/an-intuitive-guide-to-reinforcement-learning/">An Intuitive Guide to Reinforcement Learning</a>.)</em></p><p>위 그림은 DP의 backup diagram으로 어떤 과정을 거쳐 state value $v_{k+1}(s)$가 계산되는지를 보여준다. 이때 흰색 원은 state, 검은색 원은 action이다. Exhaustive search로 Bellman equation을 푼다면 위 backup diagram의 아래 모든 후속 state들을 고려해야 한다. 그러나 DP에서는 현재 state에서의 one-step transition만을 고려하기 때문에 위 backup diagram에 표시된 빨간색 영역만을 고려한다.</p><p>iterative policy evaluation의 각 iteration은 모든 state에 대해 한번에 이러한 update rule을 수행한다. 따라서 iterative policy evaluation을 수행하기 위해서는 일반적으로 old value를 저장한 array와 new value를 계산하기 위한 array가 각각 필요하다. 그러나 실제로는 한개의 array로 old value의 보관과 new value의 계산을 동시에 수행한다. 이때는 new value가 계산되면 기존 old value를 실시간으로 덮어쓴다. 따라서 각 state에 대한 value 계산 시 old value가 참조될 수도 있고, 이미 계산이 완료된 new value가 참조될 수도 있다. 2-array든 1-array 방식이든 수렴성은 보장되며 1-array 방식이 수렴속도가 일반적으로 더 빠르다. 다만 1-array 방식은 state value를 update하는 순서에 따라 수렴 속도가 변한다.</p><p>지금까지 DP를 통해 state value $v_\pi$를 계산하는 방법을 알아보았다. 이제 계산된 state value를 통해 policy $\pi$를 개선해보자.</p><h2 id="policy-improvement"><span class="mr-2">Policy Improvement</span><a href="#policy-improvement" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>임의의 deterministic policy $\pi$를 따를 때의 state value $v_\pi$가 결정되었다고 가정하자. deterministic policy의 의미는 policy $\pi$의 결과가 action들의 확률 분포가 아닌 action 그 자체인 경우를 말한다. 이 때 어떻게 기존 policy를 개선할 수 있을까? 이는 action value $q_\pi$를 통해 수행할 수 있다. 먼저 action value $q_\pi$를 remind 하자.</p>\[\begin{align} q_\pi(s,a) &amp;\doteq \mathbb{E}[R_{t+1} + \gamma v_\pi(S_{t+1}) \ \vert \ S_t = s, A_t = a] \\ &amp;= \sum_{s',r}p(s',r \vert s,a)\Big[r + \gamma v_\pi(s') \Big] \tag{3} \end{align}\]<p>모든 state $s \in \mathcal{S}$에 대해 action value가 아래와 같은 관계를 만족한다고 하자.</p>\[q_\pi(s, \pi'(s)) \geq v_\pi(s) \tag{4}\]<p>이 경우 모든 state로부터 아래와 같이 더 많거나 같은 expected return을 얻을 수 있다.</p>\[v_\pi'(s) \geq v_\pi(s) \tag{5}\]<p>이를 <em>policy improvement theorem</em>이라고 한다. 그렇다면 어떻게 수식 (4)를 만족할 수 있을까? 가장 간단한 방법은 각 state에서 action value $q_\pi(s,a)$가 최대인 action을 선택하는 것이다. $v_\pi(s)$는 action value들의 expectation이다. 따라서 최대 action value보다 작거나 같다. 이러한 방식을 <em>greedy</em> policy라고 하며 new policy $\pi’$은 아래와 같다.</p>\[\pi'(s) \doteq \underset{a}{\arg\max} \ q_\pi(s, a) \tag{6}\]<p>개선된 정책 $\pi’(s)$는 기존 $\pi(s)$에 의한 action과 같을 수도 있고 다를 수도 있지만 무엇이든 간에 분명히 기존 policy만큼 좋거나 더 나을것이다. 기존 policy에 의한 value function에 관해 greedy하게 선택하는 과정을 <em>policy improvement</em>라고 한다.</p><p>만약 new greedy policy $\pi’$이 기존 policy $\pi$와 동일하면 어떨까? 이 경우 기존 policy와 동일한 action을 선택하기 때문에 모든 state에 대해 $v_\pi = v_{\pi’}$이 되며 수식 (6)에 의해 아래와 같은 수식을 만족한다.</p>\[v_{\pi'}(s) = \max_a \mathbb{E}[R_{t+1} + \gamma v_{\pi'}(S_{t+1}) \ \vert \ S_t = s, A_t = a] \tag{7}\]<p>그런데 위 수식은 Bellman optimality equation과 동일하다. 따라서 $v_{\pi’}$은 $v_\ast$이며 $\pi$와 $\pi’$ 모두 optimal policy이다.</p><h2 id="policy-iteration"><span class="mr-2">Policy Iteration</span><a href="#policy-iteration" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p><em>policy iteration</em>은 policy evaluation과 policy improvement를 번갈아 수행하는 방법이다. 어떤 policy $\pi$를 평가해 $v_\pi$를 구한 뒤 이를 바탕으로 더 나은 policy $\pi’$를 얻는다. 다시 $\pi’$을 평가해 $v_{\pi’}$을 구한 뒤 이를 바탕으로 더 나은 policy $\pi’‘$를 얻는다. 이러한 과정을 반복하는 것이 바로 policy iteration이다. 아래는 policy iteration을 sequence 형태이다.</p>\[\pi_0 \overset{E}{\longrightarrow} v_{\pi_0} \overset{I}{\longrightarrow} \pi_1 \overset{E}{\longrightarrow} v_{\pi_1} \overset{I}{\longrightarrow} \pi_2 \overset{E}{\longrightarrow} \cdots \overset{I}{\longrightarrow} \pi_\ast \overset{E}{\longrightarrow} v_\ast\]<p>위 수식에서 $\overset{E}{\longrightarrow}$는 policy evaluation, $\overset{I}{\longrightarrow}$는 policy improvement를 나타낸다. finite MDP는 유한하기 때문에 이러한 프로세스는 유한한 iteration 안에 optimal policy와 optimal value function으로 반드시 수렴한다.</p><p>아래는 policy iteration 알고리즘이다.</p><blockquote><h5 id="textalgorithm-policy-iteration-using-iterative-policy-evaluation-for-estimating--pi-approx-pi_ast"><span class="mr-2">$\text{Algorithm: Policy Iteration (using iterative policy evaluation) for estimating } \pi \approx \pi_\ast$</span><a href="#textalgorithm-policy-iteration-using-iterative-policy-evaluation-for-estimating--pi-approx-pi_ast" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5><p>\(\begin{align*} &amp; \textstyle \text{1. Initialization} \\ &amp; \textstyle \qquad V(s) \in \mathbb{R} \text{ and } \pi(s) \in \mathcal{A}(s) \text{ arbitrarily for all } s \in \mathcal{S} \\ \\ &amp; \textstyle \text{2. Policy Evaluation} \\ &amp; \textstyle \qquad \text{Loop:} \\ &amp; \textstyle \qquad\qquad \Delta \leftarrow 0 \\ &amp; \textstyle \qquad\qquad \text{Loop for each } s \in \mathcal{S} \text{:} \\ &amp; \textstyle \qquad\qquad\qquad v \leftarrow V(s) \\ &amp; \textstyle \qquad\qquad\qquad V(s) \leftarrow \sum_{s',r}p(s',r \vert s, \pi(s))[r + \gamma V(s')] \\ &amp; \textstyle \qquad\qquad\qquad \Delta \leftarrow \max(\Delta, \vert v - V(s) \vert) \\ &amp; \textstyle \qquad \text{until } \Delta &lt; \theta \text{ (a small positive number determining the accuracy of estimation)} \\ \\ &amp; \textstyle \text{3. Policy Improvement} \\ &amp; \textstyle \qquad \textit{policy-stable} \leftarrow \textit{true} \\ &amp; \textstyle \qquad \text{For each } s \in \mathcal{S} \text{:} \\ &amp; \textstyle \qquad\qquad \textit{old-action} \leftarrow \pi(s) \\ &amp; \textstyle \qquad\qquad \pi(s) \leftarrow \arg\max_a \sum_{s',r}p(s',r \vert s,a)[r + \gamma V(s')] \\ &amp; \textstyle \qquad\qquad \text{If } \textit{old-action} \neq \pi(s),\text{ then } \textit{policy-stable} \leftarrow \textit{false} \\ &amp; \textstyle \qquad \text{If } \textit{policy-stable} \text{, then stop and return } V \approx v_\ast \text{ and } \pi \approx \pi_\ast; \text{ else go to 2} \end{align*}\)</p></blockquote><p>참고로 2. Policy Evaluation에서 state value $V(s)$를 구할 때와 3. Policy Improvement에서 개선된 policy $\pi(s)$를 얻기 위해 action value를 구할 때의 수식이 동일한데 그 이유는 policy $\pi$를 deterministic 하다고 가정했기 때문이다. state value를 구할 때 policy $\pi$를 따를 때의 action value $q_\pi(s, a)$에 대한 expectation을 취하는데 policy $\pi$를 따르는 action $a$의 확률은 1, 나머지 action은 모두 0이기 때문에 $a = \pi(s)$에 대한 action value $q_\pi(s, \pi(s))$가 곧 state value이다.</p><h2 id="value-iteration"><span class="mr-2">Value Iteration</span><a href="#value-iteration" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>policy iteration은 policy evaluation을 통해 현재 policy에 대한 value function을 수렴시키고 나서 policy improvement를 수행할 수 있었다. 그러나 <em>value iteration</em> 기법은 policy evaluation을 수행 시 현재 policy에 대한 value function을 수렴시키지 않는다. 대신 현재 policy에 대한 value function을 딱 한 번만 계산 후 바로 policy improvement를 수행한다. 즉, <strong>policy evaluation의 1 iteration과 policy improvement를 결합</strong>한 것을 반복해서 수행한다. 이 과정을 하나의 단순한 update rule로 나타낼 수 있다.</p>\[\begin{align} v_{k+1}(s) &amp;\doteq \max_a\mathbb{E}[R_{t+1} + \gamma v_k(S_{t+1}) \ \vert \ S_t = s, A_t = a] \\ &amp;= \max_a \sum_{s',r}p(s',r \vert s,a) \Big[r + \gamma v_k(s') \Big] \tag{8} \end{align}\]<p>위 수식처럼 나타낼 수 있는 이유는 policy improvement시 action value가 최대인 action으로 policy가 개선되기 때문이다. 즉, greedy policy이기 때문에 다음 policy evaluation에서 action value가 최대인 action을 제외한 나머지 action이 policy에 의해 선택될 확률은 0다. 따라서 한 번의 update에서 action value의 max값을 선택하는 것과 동일해진다.</p><p>또한 위 수식 (8)은 Bellman optimality equation과 동일하다. 즉, Bellman optimality equation을 iterative한 update rule로 변경한 것이 value iteration이다. value iteration 역시 $k \rightarrow \infty$이면 $v_\ast$로 수렴하며 이때의 greedy policy가 곧 $\pi_\ast$이다.</p><p>아래는 value iteration 알고리즘이다.</p><blockquote><h5 id="textalgorithm-value-iteration-for-estimating--pi-approx-pi_ast"><span class="mr-2">$\text{Algorithm: Value Iteration, for estimating } \pi \approx \pi_\ast$</span><a href="#textalgorithm-value-iteration-for-estimating--pi-approx-pi_ast" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5><p>\(\begin{align*} &amp; \textstyle \text{Algorithm parameter: a small threshold } \theta &gt; 0 \text{ determining accuracy of estimation} \\ &amp; \textstyle \text{Initialize } V(s) \text{, for all } s \in \mathcal{S}^+ \text{, arbitrarily except that } V(\textit{terminal}) = 0 \\ \\ &amp; \textstyle \text{Loop:} \\ &amp; \textstyle \qquad \Delta \leftarrow 0 \\ &amp; \textstyle \qquad \text{Loop for each } s \in \mathcal{S} \text{:} \\ &amp; \textstyle \qquad\qquad v \leftarrow V(s) \\ &amp; \textstyle \qquad\qquad V(s) \leftarrow \max_a \sum_{s',r}p(s',r \vert s,a)[r + \gamma V(s')] \\ &amp; \textstyle \qquad\qquad \Delta \leftarrow \max(\Delta, \vert v - V(s) \vert) \\ &amp; \textstyle \text{until } \Delta &lt; \theta \\ \\ &amp; \textstyle \text{Output a deterministic policy, } \pi \approx \pi_\ast \text{, such that} \\ &amp; \textstyle \qquad \pi(s) = \arg\max_a \sum_{s',r}p(s',r \vert s,a)[r + \gamma V(s')] \end{align*}\)</p></blockquote><p>지금까지 value iteration에 대해 알아보았다.</p><h2 id="summary"><span class="mr-2">Summary</span><a href="#summary" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>이번 포스트에서는 finite MDPs를 풀기 위해 dynamic programming 기법을 활용한 방법을 알아보았다. <em>policy evaluation</em>은 주어진 policy에 대한 value function을 계산한다. <em>policy improvement</em>는 계산된 value function을 바탕으로 policy를 개선한다. DP에서의 <em>policy iteration</em>은 policy evaluation과 policy improvement를 번갈아 수행한다. 반면 <em>value iteration</em>은 policy evaluation의 1 iteration과 policy improvement를 결합한 방식을 수행한다. 이때 policy iteration은 Bellman expected equation, value iteration은 Bellman optimality equation이 사용된다는 차이가 있다. DP에서의 이러한 과정은 <em>generalized policy iteration</em> (GPI)로 나타낼 수 있으며 이는 대부분의 <em>reinforcement learning</em> (RL)에도 적용된다. 따라서 RL에서의 DP를 이해하는 것은 중요하다고 볼 수 있다.</p><h2 id="references"><span class="mr-2">References</span><a href="#references" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>[1] Richard S. Sutton and Andrew G. Barto. <a href="/assets/materials/Reinforcement%20Learning%20An%20Introduction;%202nd%20Edition.%202018.pdf">Reinforcement Learning: An Introduction; 2nd Edition. 2018</a>.<br /> [2] Towards Data Science. Rohan Jagtap. <a href="https://towardsdatascience.com/dynamic-programming-in-rl-52b44b3d4965">Dynamic Programming in RL</a>.</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/reinforcement-learning/'>Reinforcement Learning</a>, <a href='/categories/rl-fundamental/'>RL Fundamental</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/rl/" class="post-tag no-text-decoration" >RL</a> <a href="/tags/ai/" class="post-tag no-text-decoration" >AI</a> <a href="/tags/dp/" class="post-tag no-text-decoration" >DP</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Dynamic+Programming+in+RL+-+DevSlem+Blog&url=https%3A%2F%2Fdevslem.github.io%2Freinforcement-learning%2Frl-fundamental%2Fdynamic-programming-in-rl%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Dynamic+Programming+in+RL+-+DevSlem+Blog&u=https%3A%2F%2Fdevslem.github.io%2Freinforcement-learning%2Frl-fundamental%2Fdynamic-programming-in-rl%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fdevslem.github.io%2Freinforcement-learning%2Frl-fundamental%2Fdynamic-programming-in-rl%2F&text=Dynamic+Programming+in+RL+-+DevSlem+Blog" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div><script src="https://utteranc.es/client.js" repo="DevSlem/DevSlem.github.io" issue-term="pathname" label="comment" theme="github-dark" crossorigin="anonymous" async> </script></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/reinforcement-learning/drl-methods/dqn/">DQN: Deep Q-Networks</a><li><a href="/reinforcement-learning/drl-methods/dtqn/">DTQN: Deep Transformer Q-Networks for Partially Observable Reinforcement Learning</a><li><a href="/wsl-setup/">Windows Subsystem for Linux (WSL) Setup for Reinforcement Learning</a><li><a href="/algorithm/sorting/merge-sort/">[알고리즘] 합병 정렬</a><li><a href="/algorithm/sorting/shell-sort/">[알고리즘] 쉘 정렬</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/rl/">RL</a> <a class="post-tag" href="/tags/algorithm/">Algorithm</a> <a class="post-tag" href="/tags/unity/">Unity</a> <a class="post-tag" href="/tags/function-approximation-rl/">Function Approximation RL</a> <a class="post-tag" href="/tags/sorting/">Sorting</a> <a class="post-tag" href="/tags/enemy/">Enemy</a> <a class="post-tag" href="/tags/game-development/">Game Development</a> <a class="post-tag" href="/tags/project/">Project</a> <a class="post-tag" href="/tags/attribute/">Attribute</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 pl-3 pr-3 pr-xl-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/reinforcement-learning/rl-fundamental/policy-gradient-methods/"><div class="card-body"> <em class="small" data-ts="1665586800" data-df="ll" > Oct 13, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Policy Gradient Methods</h3><div class="text-muted small"><p> 드디어 긴 장정 끝에 대망의 마지막 챕터로 왔다. 이번 포스트에서는 그 유명한 policy gradient method에 대해 소개하려고 한다. 길고 긴 여정이 드디어 끝났다. Introduction 지금까지 우리는 value-based 기반의 방법을 다뤘었다. policy는 추정된 action value에 기반해 action을 선택했다. 이번에는...</p></div></div></a></div><div class="card"> <a href="/reinforcement-learning/rl-fundamental/multi-armed-bandits/"><div class="card-body"> <em class="small" data-ts="1653058800" data-df="ll" > May 21, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Multi-armed Bandits</h3><div class="text-muted small"><p> 이 포스트에서는 Reinforcement learning (RL)의 기본 내용인 Multi-armed Bandits 환경과 기본적인 아이디어들에 대해 알아본다. Reinforcement learning vs others Reinforcement learning (RL)과 다른 learning의 가장 큰 구별점은 사용하는 정보의 차이에 있다. 다른 ...</p></div></div></a></div><div class="card"> <a href="/reinforcement-learning/rl-fundamental/finite-markov-decision-processes/"><div class="card-body"> <em class="small" data-ts="1653836400" data-df="ll" > May 30, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Finite Markov Decision Processes</h3><div class="text-muted small"><p> 이 포스트에서는 Reinforcement Learning에서 기반이 되는 finite Markov Decision Processes (MDPs)와 finite MDPs 문제를 해결하기 위한 Bellman equations에 대해 소개한다. What is MDPs Markov Decision Processes (MDPs)는 연속적인 의사 결정을 형식...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/reinforcement-learning/rl-fundamental/finite-markov-decision-processes/" class="btn btn-outline-primary" prompt="Older"><p>Finite Markov Decision Processes</p></a> <a href="/reinforcement-learning/rl-fundamental/monte-carlo-methods/" class="btn btn-outline-primary" prompt="Newer"><p>Monte Carlo Methods in RL</p></a></div></div></div><footer class="row pl-3 pr-3"><div class="col-12 d-flex justify-content-between align-items-center text-muted pl-0 pr-0"><div class="footer-left"><p class="mb-0"> © 2024 <a href="https://github.com/username">DevSlem</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/rl/">RL</a> <a class="post-tag" href="/tags/algorithm/">Algorithm</a> <a class="post-tag" href="/tags/unity/">Unity</a> <a class="post-tag" href="/tags/function-approximation-rl/">Function Approximation RL</a> <a class="post-tag" href="/tags/sorting/">Sorting</a> <a class="post-tag" href="/tags/enemy/">Enemy</a> <a class="post-tag" href="/tags/game-development/">Game Development</a> <a class="post-tag" href="/tags/project/">Project</a> <a class="post-tag" href="/tags/attribute/">Attribute</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a><div id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-animation="true" data-autohide="false"><div class="toast-header"> <button type="button" class="ml-2 ml-auto close" data-dismiss="toast" aria-label="Close"> <span aria-hidden="true">&times;</span> </button></div><div class="toast-body text-center pt-0"><p class="pl-2 pr-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></div><script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] }, svg: { scale: 1.1 } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script>
