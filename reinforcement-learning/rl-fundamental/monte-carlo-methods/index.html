<!DOCTYPE html><html lang="en" data-mode="dark" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="Monte Carlo Methods in RL" /><meta property="og:locale" content="en" /><meta name="description" content="이 포스트에서는 RL에서 environment에 대한 지식을 완전히 알 수 없을 때 experience를 통해 문제를 해결하는 Monte Carlo methods를 소개한다." /><meta property="og:description" content="이 포스트에서는 RL에서 environment에 대한 지식을 완전히 알 수 없을 때 experience를 통해 문제를 해결하는 Monte Carlo methods를 소개한다." /><link rel="canonical" href="https://devslem.github.io/reinforcement-learning/rl-fundamental/monte-carlo-methods/" /><meta property="og:url" content="https://devslem.github.io/reinforcement-learning/rl-fundamental/monte-carlo-methods/" /><meta property="og:site_name" content="DevSlem Blog" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-07-03T00:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Monte Carlo Methods in RL" /><meta name="google-site-verification" content="SSJwUK1JSPZV3wjQS19f2bXfHYIupbYEFsisH4Nuns4" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-07-19T00:00:00+09:00","datePublished":"2022-07-03T00:00:00+09:00","description":"이 포스트에서는 RL에서 environment에 대한 지식을 완전히 알 수 없을 때 experience를 통해 문제를 해결하는 Monte Carlo methods를 소개한다.","headline":"Monte Carlo Methods in RL","mainEntityOfPage":{"@type":"WebPage","@id":"https://devslem.github.io/reinforcement-learning/rl-fundamental/monte-carlo-methods/"},"url":"https://devslem.github.io/reinforcement-learning/rl-fundamental/monte-carlo-methods/"}</script><title>Monte Carlo Methods in RL | DevSlem Blog</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="DevSlem Blog"><meta name="application-name" content="DevSlem Blog"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> </a></div><div class="site-title mt-3"> <a href="/">DevSlem Blog</a></div><div class="site-subtitle font-italic">It's a blog where i write posts about AI, RL, Unity, etc.</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/DevSlem" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['devslem12','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper"><div id="topbar" class="container d-flex align-items-center justify-content-between h-100 pl-3 pr-3 pl-md-4 pr-md-4"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Monte Carlo Methods in RL</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper" class="d-flex justify-content-center"><div id="main" class="container pl-xl-4 pr-xl-4"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pr-xl-4"><div class="post pl-1 pr-1 pl-md-2 pr-md-2"><h1 data-toc-skip>Monte Carlo Methods in RL</h1><div class="post-meta text-muted"> <span> Posted <em class="" data-ts="1656774000" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Jul 3, 2022 </em> </span> <span> Updated <em class="" data-ts="1658188800" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Jul 19, 2022 </em> </span><div class="d-flex justify-content-between"> <span> By <em> <a href="https://github.com/username">DevSlem</a> </em> </span><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="5506 words"> <em>30 min</em> read</span></div></div></div><div class="post-content"><p>이 포스트에서는 RL에서 environment에 대한 지식을 완전히 알 수 없을 때 experience를 통해 문제를 해결하는 Monte Carlo methods를 소개한다.</p><h2 id="introduction"><span class="mr-2">Introduction</span><a href="#introduction" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p><em>Monte Carlo</em> (MC) <em>methods</em>는 <strong>반복된 random sampling을 통해 numerical한 근사해를 얻는 방법</strong>으로 일종의 simulation 기법이다. <em>Reinforcement Learning</em> (RL)에서 environment에 대한 지식을 완전히 알지 못할 때 MC methods는 굉장히 유용하다. DP에서와 같이 어떤 state에서 next state로의 transition에 대한 정보를 사전에 알 필요 없이 <strong>model은 단순히 transition을 sampling하기만 하면 된다</strong>. agent는 sampling된 state, action, reward와 같은 <em>experience</em>의 sequence를 바탕으로 sample return들에 대해 평균을 냄으로써 RL problem을 해결한다.</p><p>MC methods는 일반적으로 experience를 episode 단위로 나누어 적용된다. 따라서 모든 episode는 <strong>반드시 terminal state가 존재하는 episodic task</strong>여야 한다. MC methods 역시 <em>Generalized Policy Iteration</em> (GPI)를 따르며 한 episode가 끝날 때마다 policy evaluation과 policy improvement가 수행되는 step-by-step (online)이 아닌 <strong>episode-by-episode</strong> 방법이다.</p><h2 id="monte-carlo-prediction"><span class="mr-2">Monte Carlo Prediction</span><a href="#monte-carlo-prediction" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>먼저 value function을 추정하는 policy evaluation 혹은 prediction에 대해 알아보자. state value의 가장 간단한 정의는 <strong>state $s$에서 시작하여 policy $\pi$를 따를 때 얻을 수 있는 expected return</strong>이다.</p>\[v_\pi(s) \doteq \mathbb{E}_\pi[G_t \ \vert \ S_t = s]\]<p>Monte Carlo methods는 episode 단위로 수행되기 때문에 먼저 policy $\pi$를 따라 episode를 생성한다. 그 후 episode 내의 experience로부터 위 정의를 그대로 따라 방문된 state에 대한 value function을 추정한다.</p><p>같은 episode 내에서 동일한 state가 여러번 방문될 수 있다. 이때 크게 2가지 방법으로 처리한다.</p><ul><li>first-visit MC method<li>every-visit MC method</ul><p><em>first-visit MC method</em>는 한 episode 내에서 state $s$를 처음 방문했을때의 return만 그 state의 average return에 반영한다. 반면 <em>every-visit MC method</em>는 한 episode 내에서 state $s$를 방문할때마다 그때의 return을 모두 average return에 반영한다. 이 포스트에서는 first-visit MC method만 고려할 계획이다.</p><p>MC method를 backup diagram으로 나타내면 아래와 같다. 빨간색 영역이 한 episode이다.</p><p><img data-src="/assets/images/rl-mc-backup-diagram.png" alt="" width="43%" data-proofer-ignore> <em>Fig 1. MC backup diagram.<br /> (Image source: Robotic Sea Bass. <a href="https://roboticseabass.com/2020/08/02/an-intuitive-guide-to-reinforcement-learning/">An Intuitive Guide to Reinforcement Learning</a>.)</em></p><p><em>Dynamic Programming</em> (DP)에서는 가능한 모든 transition을 고려했다면 MC methods에서는 sampling된 한 episode만 고려하고 있음을 알 수 있다. 또한 DP에서는 one-step transition만을 고려했지만 MC methods에서는 <strong>episode가 끝날 때까지의 모든 transition을 고려</strong>한다.</p><h2 id="monte-carlo-estimation-of-action-values"><span class="mr-2">Monte Carlo Estimation of Action Values</span><a href="#monte-carlo-estimation-of-action-values" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p><a href="#monte-carlo-prediction">Monte Carlo Prediction</a>에서 소개한 방법은 value-based 방법이다. 그러나 이 방법은 policy를 개선할 때 큰 문제가 발생하는데 <strong>결국 environment에 대한 지식인 state transition과 관련된 probability distribution을 알아야 한다</strong>.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> policy improvement를 통해 new greedy policy $\pi’$을 획득한다고 할 때 수식은 아래와 같다.</p>\[\pi'(s) \doteq \underset{a}{\arg\max} \ q_\pi(s, a)\]<p>new policy를 결정하는 방법은 간단하다. 그 state에서의 action value만 알고 있으면 된다. 문제는 action value를 구할 때 발생한다. action value $q_\pi(s, a)$를 구하는 수식은 아래와 같다.</p>\[\begin{align} q_\pi(s,a) &amp;\doteq \mathbb{E}[R_{t+1} + \gamma v_\pi(S_{t+1}) \ \vert \ S_t = s, A_t = a] \\ &amp;= \sum_{s',r}p(s',r \vert s,a)\Big[r + \gamma v_\pi(s') \Big] \end{align}\]<p>위 수식을 보면 알 수 있듯이 state value function $v_\pi$를 통해 action value $q_\pi$를 구할 때 environment dynamics $p(s’,r \vert s,a)$를 알아야만 한다. policy evaluation에서 random sampling을 통해 environment에 대한 지식 없이 state value $v_\pi(s)$를 추정할 수 있었지만, policy improvement를 수행하기 위해서는 결국 environment에 대해 알고 있어야만 하는 문제가 발생한다.</p><p>위 문제는 environment에 대해 알고 있다면 value-based MC methods를 통해서도 해결할 수 있다. 그러나 environment에 대해 알지 못한다면 다른 접근법이 필요한데, state value를 추정하는 것이 아니라 state-action pair에 대한 <em>action</em> value 그 자체를 추정하면 이 문제를 해결할 수 있다. action value $q_\pi$의 가장 간단한 정의는 <strong>state $s$에서 policy $\pi$에 따라 action $a$를 선택했을 때 얻을 수 있는 expected return</strong>이다.</p>\[q_\pi(s, a) \doteq \mathbb{E}_\pi[G_t \ \vert \ S_t = s, A_t = a]\]<p>위 정의에 따라 action value 자체를 추정해 획득한 값으로 policy improvement를 수행하면 environment에 대한 정보가 필요 없어진다.</p><h2 id="monte-carlo-control"><span class="mr-2">Monte Carlo Control</span><a href="#monte-carlo-control" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>이제 Monte Carlo estimation이 optimal policy를 추정하는 policy improvement 혹은 control에 어떻게 적용될 수 있는지 알아보자. 앞서 언급했지만 MC methods 역시 GPI를 따른다. 다만 이제 policy evaluation에서 state value가 아닌 <strong>action value $q_\pi$를 추정</strong>할 것이다.</p><p><img data-src="/assets/images/rl-sutton-gpi-action-value.png" alt="" width="40%" data-proofer-ignore> <em>Fig 2. GPI for action value.<br /> (Image source: Sec 5.3 Sutton &amp; Barto (2018).)</em></p><p>위 그림을 sequence로 나타내면 아래와 같다.</p>\[\pi_0 \overset{E}{\longrightarrow} q_{\pi_0} \overset{I}{\longrightarrow} \pi_1 \overset{E}{\longrightarrow} q_{\pi_1} \overset{I}{\longrightarrow} \pi_2 \overset{E}{\longrightarrow} \cdots \overset{I}{\longrightarrow} \pi_\ast \overset{E}{\longrightarrow} q_\ast\]<p>위에서 $\overset{E}{\longrightarrow}$는 policy evaluation, $\overset{I}{\longrightarrow}$는 policy improvement를 나타낸다.</p><p>policy evaluation은 앞서 <a href="#monte-carlo-prediction">Monte Carlo Prediction</a>에서 소개한 방식과 동일하다. 현재 policy $\pi$에 따라 state-action pair에 대한 experience를 통해 episode가 생성되고, 이 episode에 대해 관찰된 return들을 바탕으로 action value $q_\pi$를 추정한다. 그 후 계산된 action value $q_\pi$를 바탕으로 episode 내에 방문된 모든 state에 대해 policy가 개선된다. 이를 episode 단위로 반복하다 보면 optimal policy $\pi_\ast$와 optimal action value $q_\ast$를 획득할 수 있다. 즉, 아래와 같이 3가지 과정으로 요약할 수 있다.</p><ol><li>policy $\pi$에 따라 episode 생성<li>episode 내의 experience를 바탕으로 $q_\pi$에 대한 policy evaluation<li>policy improvement</ol><p>다만 optimal policy와 action value로 수렴하기 위해서는 반드시 <strong>모든 state-action pair가 방문된다는 가정이 필요</strong>하다. 즉, agent는 environment에 대해 골고루 탐색해야 한다. 이는 RL에서 굉장히 일반적인 <em>maintaining exploration</em> 문제이다. action value를 통해 policy evaluation을 효과적으로 수행하기 위해서는 반드시 지속적인 exploration을 보장해야한다. 이 포스트에서는 이 문제에 대한 해결책으로 2가지 방법을 소개한다.</p><ul><li>exploring starts<li>$\epsilon$-soft policy</ul><p><em>exploring starts</em> (ES)는 episode 시작 시 state-action pair $(s, a)$를 stochastic하게 선택하며, 이 때 모든 시작 state-action pair는 반드시 0이 아닌 확률을 가진다. 이 경우 episode의 수가 무한할 경우 모든 state-action pair가 방문됨을 보장할 수 있다.</p><p>그러나 위 방법은 특정 state에서 시작해야하는 조건이 있는 경우 유용하지 않다. 이에 대한 대안으로 각 state에서 선택할 모든 action에 대해 0이 아닌 확률을 보장하는 stochastic policy를 고려한다. 대표적으로 <em>$\epsilon$-soft policy</em>가 있다.</p><p>이제 각 방법을 바탕으로 한 MC methods 알고리즘을 알아보자.</p><h2 id="monte-carlo-es"><span class="mr-2">Monte Carlo ES</span><a href="#monte-carlo-es" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>first-visit Monte Carlo ES 알고리즘은 아래와 같다.</p><blockquote><h5 id="textalgorithm-monte-carlo-es-exploring-starts-for-estimating--pi-approx-pi_ast"><span class="mr-2">$\text{Algorithm: Monte Carlo ES (Exploring Starts), for estimating } \pi \approx \pi_\ast$</span><a href="#textalgorithm-monte-carlo-es-exploring-starts-for-estimating--pi-approx-pi_ast" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5><p>\(\begin{align*} &amp; \textstyle \text{Initialize:} \\ &amp; \textstyle \qquad \pi(s) \in \mathcal{A}(s) \text{ (arbitrarily), for all } s \in \mathcal{S} \\ &amp; \textstyle \qquad Q(s,a) \in \mathbb{R} \text{ (arbitrarily), for all } s \in \mathcal{S}, \ a \in \mathcal{A}(s) \\ &amp; \textstyle \qquad \textit{Returns}(s,a) \leftarrow \text{empty list, for all } s \in \mathcal{S}, \ a \in \mathcal{A}(s) \\ \\ &amp; \textstyle \text{Loop forever (for each episode):} \\ &amp; \textstyle \qquad \text{Choose } S_0 \in \mathcal{S}, \ A_0 \in \mathcal{A}(S_0) \text{ randomly such that all pairs have probability} &gt; 0 \\ &amp; \textstyle \qquad \text{Generate an episode from } S_0, A_0, \text{ following } \pi \text{: } S_0, A_0, R_1, \dots, S_{T-1}, A_{T-1}, R_T \\ &amp; \textstyle \qquad G \leftarrow 0 \\ &amp; \textstyle \qquad \text{Loop for each step of episode, } t = T-1, T-2, \dots, 0 \text{:} \\ &amp; \textstyle \qquad\qquad G \leftarrow \gamma G + R_{t+1} \\ &amp; \textstyle \qquad\qquad \text{Unless the pair } S_t, A_t \text{ appears in } S_0, A_0, S_1, A_1, \dots, S_{t-1}, A_{t-1} \text{:} \\ &amp; \textstyle \qquad\qquad\qquad \text{Append } G \text{ to } \textit{Returns}(S_t,A_t) \\ &amp; \textstyle \qquad\qquad\qquad Q(S_t,A_t) \leftarrow \text{average}(\textit{Returns}(S_t,A_t)) \\ &amp; \textstyle \qquad\qquad\qquad \pi(S_t) \leftarrow \arg\max_a Q(S_t,a) \end{align*}\)</p></blockquote><p>exploring starts이기 때문에 각 episode의 시작마다 모든 state-action pair의 확률이 0보다 큰 조건 하에 랜덤하게 state-action pair를 선택한다. 또한 action value $Q(S_t, A_t)$를 정의에 따라 $S_t, A_t$ pair에 대해 지금까지 획득한 return $G_t$들의 expectation으로 update한다.</p><h2 id="monte-carlo-control-without-es"><span class="mr-2">Monte Carlo Control without ES</span><a href="#monte-carlo-control-without-es" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>exploring starts는 쉽게 말해 랜덤 스타트로, 문제가 있는 방식이라 소개했었다. 따라서 ES를 사용하지 않고 모든 state-action pair가 방문됨을 보장하려고 한다. 이를 달성할 수 있는 일반적인 방법은 episode의 시작에서만 하는게 아니라 episode 동안 지속적으로 agent가 state-action pair를 무한히 선택할 수 있도록 보장한다. 이것을 달성하기 위한 2가지 접근법이 있다.</p><ul><li>on-policy methods<li>off-policy methods</ul><p><em>on-policy</em> methods는 experience 생성에 사용되는 policy를 평가하고 개선한다. 반면 <em>off-policy</em> methods는 experience를 생성하는데 사용되는 policy와 평가 및 개선하는데 사용되는 policy를 분리하는 방법이다.</p><h2 id="on-policy-monte-carlo"><span class="mr-2">On-policy Monte Carlo</span><a href="#on-policy-monte-carlo" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>on-policy methods에서는 policy는 일반적으로 <em>soft</em>하다. soft하다는 것은 모든 $s \in \mathcal{S}$와 $a \in \mathcal{A}$에 대해 $\pi(a \vert s) &gt; 0$는 의미이다. 즉, 모든 state-action pair에게 0이 아닌 확률을 보장한다. 이러한 soft한 policy를 GPI 과정을 거치면서 점점 deterministic한 optimal policy로 수렴한다.</p><p>위를 달성할 수 있는 가장 간단하 방법 중 하나가 $\epsilon$<em>-greedy</em> policy이다. $\epsilon$-greedy policy는 대부분은 action value가 최대인 action을 선택하지만 $\epsilon$의 확률로 랜덤하게 action을 선택한다. 따라서 모든 action들은 아래와 같이 선택될 확률을 가진다.</p>\[\pi(a \vert s) = \begin{cases} 1 - \epsilon + \dfrac{\epsilon}{\vert \mathcal{A}(s) \vert} &amp; \text{if } a = \text{greedy} \\ \dfrac{\epsilon}{\vert \mathcal{A}(s) \vert} \qquad &amp; \text{if } a = \text{nongreedy} \end{cases}\]<p>$\epsilon$-greedy policy는 $\epsilon$<em>-soft</em> policy의 가장 대표적인 예시이다. $\epsilon$-soft policy는 모든 state와 action에 대해 $\pi(a \vert s) \geq \dfrac{\epsilon}{\vert \mathcal{A}(s) \vert}$인 policy이다.</p><p>On-policy first-visit MC methods 알고리즘을 살펴보자. policy는 $\epsilon$-greedy이다.</p><blockquote><h5 id="textalgorithm-on-policy-first-visit-mc-control-for--epsilon-text-soft-policies-estimates--pi-approx-pi_ast"><span class="mr-2">$\text{Algorithm: On-policy first-visit MC control (for } \epsilon \text{-soft policies), estimates } \pi \approx \pi_\ast$</span><a href="#textalgorithm-on-policy-first-visit-mc-control-for--epsilon-text-soft-policies-estimates--pi-approx-pi_ast" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5><p>\(\begin{align*} &amp; \textstyle \text{Algorithm parameter: small } \epsilon &gt; 0 \\ &amp; \textstyle \text{Initialize: } \\ &amp; \textstyle \qquad \pi \leftarrow \text{an arbitrary } \epsilon \text{-soft policy} \\ &amp; \textstyle \qquad Q(s, a) \in \mathbb{R} \text{ (arbitrarily), for all } s \in \mathcal{S}, \ a \in \mathcal{A}(s) \\ &amp; \textstyle \qquad \textit{Returns}(s,a) \leftarrow \text{empty list, for all } s \in \mathcal{S}, \ a \in \mathcal{A}(s) \\ \\ &amp; \textstyle \text{Loop forever (for each episode):} \\ &amp; \textstyle \qquad \text{Generate an episode following } \pi \text{: } S_0, A_0, R_1, \dots, S_{T-1}, A_{T-1}, R_T \\ &amp; \textstyle \qquad G \leftarrow 0 \\ &amp; \textstyle \qquad \text{Loop for each step of episode, } t = T-1, T-2, \dots, 0 \text{:} \\ &amp; \textstyle \qquad\qquad G \leftarrow \gamma G + R_{t+1} \\ &amp; \textstyle \qquad\qquad \text{Unless the pair } S_t,A_t \text{ appears in } S_0, A_0, S_1, A_1, \dots, S_{t-1}, A_{t-1} \text{:} \\ &amp; \textstyle \qquad\qquad\qquad \text{Append } G \text{ to } \textit{Returns}(S_t,A_t) \\ &amp; \textstyle \qquad\qquad\qquad Q(S_t, A_t) \leftarrow \text{average}(\textit{Returns}(S_t, A_t)) \\ &amp; \textstyle \qquad\qquad\qquad A^\ast \leftarrow \arg\max_a Q(S_t, a) \\ &amp; \textstyle \qquad\qquad\qquad \text{For all } a \in \mathcal{A}(S_t)\text{:} \\ &amp; \textstyle \qquad\qquad\qquad\qquad \pi(a \vert S_t) \leftarrow \begin{cases} 1 - \epsilon + \epsilon / \vert \mathcal{A}(S_t) \vert &amp; \text{if } a = A_\ast \\ \epsilon / \vert \mathcal{A}(S_t) \vert \qquad &amp; \text{if } a \neq A_\ast \end{cases} \end{align*}\)</p></blockquote><p>on-policy methods이기 때문에 episode 생성에 사용되는 policy $\pi$를 평가하고 개선한다. 이 때 $q_\pi$에 관한 $\epsilon$-soft policy는 policy improvement theorem에 의해 보장된다.<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup></p><h2 id="off-policy-methods"><span class="mr-2">Off-policy methods</span><a href="#off-policy-methods" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>off-policy methods는 아래와 같은 2개의 policy를 사용하는 방법이다.</p><ul><li>target policy<li>behavior policy</ul><p><em>target policy</em>는 학습에 사용되는 policy로 target policy를 optimal policy로 수렴시키는 것이 목적이다. <em>behavior policy</em>는 behavior 혹은 experience를 생성하는 policy로 target policy에 비해 조금 더 exploratory한 policy이다.</p><p>on-policy는 일반적으로 비교적 쉬운 편이기 때문에 자주 고려된다. off-policy는 대게 더 높은 분산과 느린 수렴 속도를 가지지만 더 강력하고 general하다. on-policy는 target policy와 behavior policy가 동일한 경우로 취급할 수 있다.</p><p>조금 더 구체적으로 들어가자면 아래와 같은 과정을 거친다.</p><ol><li>behavior policy $b$로부터 episode 생성<li>episode 내의 experience를 바탕으로 $q_\pi$에 대한 policy evaluation<li>policy improvement</ol><p>그런데 이상한 점이 있다. behavior policy $b$와 target policy $\pi$의 distribution은 전혀 다를 것이다. 우리가 구할 수 있는 것은 $\mathbb{E} _ b[X]$인데 어떻게 $b$로부터 생성된 episode를 가지고 $q_\pi$를 추정할 수 있을까? 이 문제를 해결하기 위해 importance sampling이라는 기법을 사용한다.</p><h2 id="importance-sampling"><span class="mr-2">Importance Sampling</span><a href="#importance-sampling" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p><em>importance sampling</em>은 <strong>다른 distribution을 따르는 sample이 주어졌을 때 목표로 하는 distribution의 expected value를 추정</strong>하는 기법이다. 대부분의 off-policy methods는 서로 다른 policy를 사용하기 때문에 importance sampling을 통해 expected value를 추정한다.</p><p>target policy와 behavior policy에 대한 trajectory의 상대적 확률에 따라 return에 가중치를 부여한다. 이를 <em>importance-sampling ratio</em>라고 한다. 먼저 <strong>시작 state $S_t$가 주어졌을 때</strong> 생성된 state-action trajectory가 아래와 같이 있다고 하자.</p>\[A_t, S_{t+1}, A_{t+1}, \dots, S_T\]<p>이 때 임의의 policy $\pi$를 따를 때 위 trajectory의 발생 확률은 아래와 같다. 이때 $\pi$는 trajectory를 생성한 policy가 아니여도 된다.</p>\[\begin{align} &amp; \text{Pr} \lbrace A_t, S_{t+1}, A_{t+1}, \dots, S_t \ \vert \ S_t, A_{t:T-1} \sim \pi \rbrace \\ &amp; \qquad = \pi(A_t \vert S_t) p(S_{t+1} \vert S_t, A_t) \pi(A_{t+1} \vert S_{t+1}) \cdots p(S_T \vert S_{T-1}, A_{T-1}) \\ &amp; \qquad = \prod_{k=t}^{T-1} \pi(A_k \vert S_k) p(S_{k+1} \vert S_k, A_k) \end{align}\]<p>위 수식에서 $p$는 state-transition probability function이다. 임의의 policy가 각각 trajectory를 생성한 behavior policy $b$, expected value를 추정하려는 target policy $\pi$라고 할 때 위 trajectory의 확률을 바탕으로 importance-sampling ratio $\rho$를 구할 수 있다.</p>\[\rho_{t:T-1} \doteq \dfrac{\prod_{k=t}^{T-1} \pi(A_k \vert S_k) p(S_{k+1} \vert S_k, A_k)}{\prod_{k=t}^{T-1} b(A_k \vert S_k) p(S_{k+1} \vert S_k, A_k)} = \prod_{k=t}^{T-1}\dfrac{\pi(A_k \vert S_k)}{b(A_k \vert S_k)}\]<p>이미 알고 있는 distribution은 분모, expected value를 추정할 distribution은 분자여야 한다. 위 수식의 분모, 분자에 있던 state-transition probability는 소거할 수 있다. 그 이유는 동일한 environment에서의 동일한 trajectory이기 때문에 state-transition probability는 동일하다. 결국 environment에 대한 지식은 필요 없게 된다. 오직 $b$와 $\pi$, 생성된 trajectory만 있으면 된다.</p><p>위에서 구한 importance-sampling ratio를 통해 이제 올바른 expected value를 추정할 수 있다. 이제 importance-sampling ratio를 통해 action value를 추정하는 방법을 알아보자.</p><h2 id="off-policy-monte-carlo-via-importance-sampling"><span class="mr-2">Off-policy Monte Carlo via Importance Sampling</span><a href="#off-policy-monte-carlo-via-importance-sampling" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>importance-sampling ratio $\rho$를 사용하면 behavior policy $b$를 따르는 episode를 통해서도 target policy $\pi$를 따르는 action value $q_\pi$를 추정할 수 있다. 아래는 $q_\pi$를 추정하는 수식이다.</p>\[q_\pi(s,a) = \mathbb{E}[\rho_{t:T-1}G_t \ \vert \ S_t = s, A_t = a]\]<p>각 state-action pair에 대한 return $G_t$에 importance-sampling ratio $\rho$를 곱한 후 이 값들에 대한 expected value를 구하면 된다. 그렇다면 어떻게 expected value를 구할 수 있을까? 가장 간단한 방법은 위 value를 다 더한 뒤 개수로 나눈다. 이 방법을 <em>ordinary importance sampling</em>이라고 한다.<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup> 이 방법은 unbiased하지만 높은 variance를 가진다. 따라서 biased하지만 variance가 매우 낮은 <em>weighted importance sampling</em>을 대안으로 활용한다. 아래는 weighted importance sampling을 활용해 추정한 action value $q_\pi$이다.</p>\[q_\pi(s,a) \doteq \dfrac{\sum_{t \in \mathcal{J}(s,a)} \rho_{t:T(t)-1}G_t}{\sum_{t \in \mathcal{J}(s,a)} \rho_{t:T(t)-1}}\]<p>$\mathcal{J}(s,a)$는 state-action pair s, a가 방문된 time step $t$에 대한 집합이다. $T(t)$는 $t \in \mathcal{J}(s,a)$일 때의 termination time이다.</p><p><img data-src="/assets/images/rl-sutton-ordinary-vs-weighted-importance-sampling.png" alt="" width="70%" data-proofer-ignore> <em>Fig 3. Ordinary importance sampling vs weighted importance sampling.<br /> (Image source: Sec 5.5 Sutton &amp; Barto (2018).)</em></p><p>위 그림은 ordinary importance sampling과 weighted importance sampling을 비교하는 그래프이다. 둘다 error가 0으로 수렴하지만 weighted importance sampling이 더 안정적임을 확인할 수 있다.</p><h3 id="incremental-implementation-of-weighted-average"><span class="mr-2">Incremental Implementation of Weighted Average</span><a href="#incremental-implementation-of-weighted-average" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>이제 off-policy Monte Carlo를 구현해보자. weighted average를 incremental 방식으로 구현해보려 한다. 먼저 $n-1$개의 return sequence $G_1, G_2, \dots, G_{n-1}$과 각각에 대응하는 weight $W_i \ (\text{e.g., } W_i = \rho_{t_i:T(t_i)-1})$가 있다고 하자. weighted average $V_n$은 아래와 같다.</p>\[V_n \doteq \dfrac{\sum_{k=1}^{n-1} W_k G_k}{\sum_{k=1}^{n-1} W_k}, \qquad n \geq 2\]<p>위 $V_n$과 weight들의 cumulative sum $C_n = \sum_{k=1}^{n-1} W_k$을 유지하고 있을 때 추가적인 return $G_n$을 획득할 경우 incremental한 방식으로 $V$를 update할 수 있다. $V$에 대한 update rule은 아래와 같다.</p>\[V_{n+1} \doteq V_n + \dfrac{W_n}{C_n} \Big[G_n - V_n \Big] \qquad n \geq 1,\] \[C_{n+1} \doteq C_n + W_{n+1}\]<p>위에서 $C_0 \doteq 0$이고 $V_1$은 임의의 값이다. 위 $V$를 state value라면 $v$로, action value라면 $q$로 변경하기만 하면 된다.</p><h3 id="off-policy-mc-prediction-algorithm"><span class="mr-2">Off-policy MC Prediction Algorithm</span><a href="#off-policy-mc-prediction-algorithm" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>이제 Off-policy MC methods 알고리즘을 보자. 여기서는 prediction 부분만 보이도록 하겠다. 한가지 중요한 사실은 target policy $\pi$와 behavior policy $b$ 모두 어떤 policy도 가능하지만 <strong><em>coverage</em>를 만족</strong>해야한다. coverage란 $\pi$에 의해 선택될 수 있는 모든 action은 $b$에 의해서도 선택될 수 있어야 함을 의미한다. 즉, $\pi(a \vert s) &gt; 0$면 $b(a \vert s) &gt; 0$이어야 한다.</p><blockquote><h5 id="textalgorithm-off-policy-mc-prediction-policy-evaluation-for-estimating--q-approx-q_pi"><span class="mr-2">$\text{Algorithm: Off-policy MC prediction (policy evaluation) for estimating } Q \approx q_\pi$</span><a href="#textalgorithm-off-policy-mc-prediction-policy-evaluation-for-estimating--q-approx-q_pi" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5><p>\(\begin{align*} &amp; \textstyle \text{Input: an arbitrary target policy } \pi \\ &amp; \textstyle \text{Initialize, for all } s \in \mathcal{S}, \ a \in \mathcal{A}(s) \text{:} \\ &amp; \textstyle \qquad Q(s,a) \in \mathbb{R} \text{ (arbitrarily)} \\ &amp; \textstyle \qquad C(s,a) \leftarrow 0 \\ \\ &amp; \textstyle \text{Loop forever (for each episode):} \\ &amp; \textstyle \qquad b \leftarrow \text{any policy with coverage of } \pi \\ &amp; \textstyle \qquad \text{Generate an episode following } b \text{: } S_0, A_0, R_1, \dots, S_{T-1}, A_{T-1}, R_T \\ &amp; \textstyle \qquad G \leftarrow 0 \\ &amp; \textstyle \qquad W \leftarrow 1 \\ &amp; \textstyle \qquad \text{Loop for each step of episode, } t = T-1, T-2, \dots, 0, \text{ while } W \neq 0 \text{:} \\ &amp; \textstyle \qquad\qquad G \leftarrow \gamma G + R_{t+1} \\ &amp; \textstyle \qquad\qquad C(S_t,A_t) \leftarrow C(S_t,A_t) + W \\ &amp; \textstyle \qquad\qquad Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \frac{W}{C(S_t,A_t)}[G - Q(S_t,A_t)] \\ &amp; \textstyle \qquad\qquad W \leftarrow W \frac{\pi(A_t \vert S_t)}{b(A_t \vert S_t)} \end{align*}\)</p></blockquote><p>그런데 위 알고리즘을 보면 한가지 이상한 점이 있다. 바로 $W = 1$부터 시작하는 것이다. 우리는 앞서 <a href="#importance-sampling">Importance Sampling</a>에서 importance-sampling ratio $\rho_{t:T-1}$는 time step $t$에서의 ratio부터 고려했었다. 즉, $\pi(A_t \vert S_t) / b(A_t \vert S_t)$를 고려했었다. 따라서 $W = \pi(A_{T-1} \vert S_{T-1}) / b(A_t \vert S_{T-1})$부터 시작해야한다. 그런데 왜 $W = 1$부터 시작하는걸까? 필자 역시 처음 공부했을 때 이러한 의문이 있었으며 어느 곳에서도 답을 찾을 수 없었다. 추후 다른 chapter를 공부하다가 그 이유를 알게 되었다. 한번 알아보자.</p><p><a href="#importance-sampling">Importance Sampling</a>에서 정의했던 importance-sampling ratio $\rho_{t:T-1}$는 시작 state $S_t$가 주어졌을 때 임의의 policy를 따라 생성된 state-action trajectory를 전제로 했었다.</p>\[A_t, S_{t+1}, A_{t+1}, \dots, S_T\]<p>그런데 우리는 action value를 추정하고 있다. action value 추정의 전제는 state-action pair가 주어져있다는 것이다. 즉, <strong>$S_t, A_t$는 이미 주어져있기 때문에</strong> 우리가 고려해야할 state-action trajectory는 아래와 같다.</p>\[S_{t+1}, A_{t+1}, \dots, S_T\]<p>따라서 action value를 추정할 떄 필요한 importance-sampling ratio는 $\rho_{t+1:T-1}$이다. <a href="#importance-sampling">Importance Sampling</a> 파트의 trajectory가 임의의 policy 를 따를 때 발생할 확률에 위 trajectory로 대체해보면 간단히 증명할 수 있다. 위와 같은 이유로 $t = T-1$일 때 $\rho_{T:T-1} = 1$이므로, $W=1$부터 시작한다. 아래는 Reinforcement Learning: An Introduction에서 importance sampling ratio를 $t+1$부터 시작하는 이유를 설명하는 문장이다.</p><blockquote><p>We do not have to care how likely we were to select the action; now that we have selected it we want to learn fully from what happens, with importance sampling only for subsequent actions.<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup></p></blockquote><p>근데 위 문장이 Monte Carlo Methods가 아니라 n-step Bootstrapping chapter에 있었어서 다소 아쉬웠다. 이 문장을 보고 나서야 왜 $t+1$부터 시작하는지 위와 같이 이해할 수 있었다.</p><h2 id="summary"><span class="mr-2">Summary</span><a href="#summary" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>지금까지 <em>Monte Carlo</em> (MC) <em>methods</em>에 대해 알아보았다. MC methods는 <em>sample episode</em>안의 experience로부터 value function을 학습하고 optimal policy를 찾는다. MC methods는 <em>Generalized Policy Iteration</em> (GPI)를 따른다. MC methods는 episode-by-episode 단위로 GPI를 수행한다. action-value function을 추정할 경우 environment에 대한 지식(dynamics) 없이도 policy를 개선하는 것이 가능하다.</p><p>MC methods는 sampling을 통해 학습하는 방법이기 때문에 충분한 <em>exploration</em>을 보장해주어야 한다. 이에 대한 방법으로 <em>exploring starts</em>와 <em>on-policy</em> methods, <em>off-policy</em> methods가 있다. exploring start는 state-action pair를 랜덤하게 시작하는 방법이지만 현실과는 동떨어진 방법이다. on-policy methods는 하나의 policy로 학습과 탐색을 모두 수행한다. off-policy methods는 학습에 사용되는 <em>target policy</em>와 탐색에 사용되는 <em>behavior policy</em>로 분리하는 방법이다.</p><p>off-policy methods는 behavior policy를 따라 생성된 data로부터 target policy를 학습하는데 이 두 policy의 distribution이 다르기 때문에 문제가 발생한다. 이를 해결하기 위해 <em>importance sampling</em>이라는 기법을 사용해 behavior policy의 distribution으로부터 target policy의 expected value를 추정한다. 이때 <em>ordinary importance sampling</em>과 <em>weighted importance sampling</em> 2가지 방법이 존재하는데 일반적으로 분산이 낮은 weighted importance sampling이 선호된다.</p><p>MC methods는 DP와 주요한 2가지 차이점이 있다. 먼저, MC methods는 DP와 달리 environment에 대한 지식(dynamics) 없이 sample experience로부터 학습이 가능하다. 두번째는 MC methods는 bootstrap하지 않다. 즉, value function을 update할 때 DP와 달리 다른 value function의 추정치를 통해 update하지 않고 return을 직접 구해 update한다.</p><h2 id="references"><span class="mr-2">References</span><a href="#references" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>[1] Richard S. Sutton and Andrew G. Barto. <a href="/assets/materials/Reinforcement%20Learning%20An%20Introduction;%202nd%20Edition.%202018.pdf">Reinforcement Learning: An Introduction; 2nd Edition. 2018</a>.<br /> [2] Towards Data Science. Sagi Shaier. <a href="https://towardsdatascience.com/introduction-to-reinforcement-learning-rl-part-5-monte-carlo-methods-25067003bb0f">Monte Carlo Methods</a>.<br /> [3] 생각많은 소심남. <a href="https://talkingaboutme.tistory.com/entry/RL-Off-policy-Learning-for-Prediction">[RL] Off-policy Learning for Prediction</a>.</p><h2 id="footnotes"><span class="mr-2">Footnotes</span><a href="#footnotes" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><div class="footnotes" role="doc-endnotes"><ol><li id="fn:1" role="doc-endnote"><p>StackExchange. <a href="https://ai.stackexchange.com/questions/22907/why-are-state-values-alone-not-sufficient-in-determining-a-policy-without-a-mod">Why are state-values alone not sufficient in determining a policy (without a model)?</a>. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:2" role="doc-endnote"><p>Reinforcement Learning: An Introduction; 2nd Edition. 2018. <a href="https://devslem.github.io/assets/materials/Reinforcement%20Learning%20An%20Introduction;%202nd%20Edition.%202018.pdf#page=123">Sec. 5.4, p.123</a>. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:3" role="doc-endnote"><p>Reinforcement Learning: An Introduction; 2nd Edition. 2018. <a href="https://devslem.github.io/assets/materials/Reinforcement%20Learning%20An%20Introduction;%202nd%20Edition.%202018.pdf#page=126">Sec. 5.5, p.126</a>. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:4" role="doc-endnote"><p>Reinforcement Learning: An Introduction; 2nd Edition. 2018. <a href="https://devslem.github.io/assets/materials/Reinforcement%20Learning%20An%20Introduction;%202nd%20Edition.%202018.pdf#page=171">Sec. 7.3, p.171</a>. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p></ol></div></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/reinforcement-learning/'>Reinforcement Learning</a>, <a href='/categories/rl-fundamental/'>RL Fundamental</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/rl/" class="post-tag no-text-decoration" >RL</a> <a href="/tags/ai/" class="post-tag no-text-decoration" >AI</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Monte+Carlo+Methods+in+RL+-+DevSlem+Blog&url=https%3A%2F%2Fdevslem.github.io%2Freinforcement-learning%2Frl-fundamental%2Fmonte-carlo-methods%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Monte+Carlo+Methods+in+RL+-+DevSlem+Blog&u=https%3A%2F%2Fdevslem.github.io%2Freinforcement-learning%2Frl-fundamental%2Fmonte-carlo-methods%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fdevslem.github.io%2Freinforcement-learning%2Frl-fundamental%2Fmonte-carlo-methods%2F&text=Monte+Carlo+Methods+in+RL+-+DevSlem+Blog" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div><script src="https://utteranc.es/client.js" repo="DevSlem/DevSlem.github.io" issue-term="pathname" label="comment" theme="github-dark" crossorigin="anonymous" async> </script></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/reinforcement-learning/drl-methods/dqn/">DQN: Deep Q-Networks</a><li><a href="/reinforcement-learning/drl-methods/dtqn/">DTQN: Deep Transformer Q-Networks for Partially Observable Reinforcement Learning</a><li><a href="/wsl-setup/">Windows Subsystem for Linux (WSL) Setup for Reinforcement Learning</a><li><a href="/algorithm/sorting/merge-sort/">[알고리즘] 합병 정렬</a><li><a href="/algorithm/sorting/shell-sort/">[알고리즘] 쉘 정렬</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/rl/">RL</a> <a class="post-tag" href="/tags/algorithm/">Algorithm</a> <a class="post-tag" href="/tags/unity/">Unity</a> <a class="post-tag" href="/tags/function-approximation-rl/">Function Approximation RL</a> <a class="post-tag" href="/tags/sorting/">Sorting</a> <a class="post-tag" href="/tags/enemy/">Enemy</a> <a class="post-tag" href="/tags/game-development/">Game Development</a> <a class="post-tag" href="/tags/project/">Project</a> <a class="post-tag" href="/tags/attribute/">Attribute</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 pl-3 pr-3 pr-xl-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/reinforcement-learning/rl-fundamental/policy-gradient-methods/"><div class="card-body"> <em class="small" data-ts="1665586800" data-df="ll" > Oct 13, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Policy Gradient Methods</h3><div class="text-muted small"><p> 드디어 긴 장정 끝에 대망의 마지막 챕터로 왔다. 이번 포스트에서는 그 유명한 policy gradient method에 대해 소개하려고 한다. 길고 긴 여정이 드디어 끝났다. Introduction 지금까지 우리는 value-based 기반의 방법을 다뤘었다. policy는 추정된 action value에 기반해 action을 선택했다. 이번에는...</p></div></div></a></div><div class="card"> <a href="/reinforcement-learning/rl-fundamental/multi-armed-bandits/"><div class="card-body"> <em class="small" data-ts="1653058800" data-df="ll" > May 21, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Multi-armed Bandits</h3><div class="text-muted small"><p> 이 포스트에서는 Reinforcement learning (RL)의 기본 내용인 Multi-armed Bandits 환경과 기본적인 아이디어들에 대해 알아본다. Reinforcement learning vs others Reinforcement learning (RL)과 다른 learning의 가장 큰 구별점은 사용하는 정보의 차이에 있다. 다른 ...</p></div></div></a></div><div class="card"> <a href="/reinforcement-learning/rl-fundamental/finite-markov-decision-processes/"><div class="card-body"> <em class="small" data-ts="1653836400" data-df="ll" > May 30, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Finite Markov Decision Processes</h3><div class="text-muted small"><p> 이 포스트에서는 Reinforcement Learning에서 기반이 되는 finite Markov Decision Processes (MDPs)와 finite MDPs 문제를 해결하기 위한 Bellman equations에 대해 소개한다. What is MDPs Markov Decision Processes (MDPs)는 연속적인 의사 결정을 형식...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/reinforcement-learning/rl-fundamental/dynamic-programming-in-rl/" class="btn btn-outline-primary" prompt="Older"><p>Dynamic Programming in RL</p></a> <a href="/reinforcement-learning/rl-fundamental/temporal-difference-learning/" class="btn btn-outline-primary" prompt="Newer"><p>Temporal-Difference Learning</p></a></div></div></div><footer class="row pl-3 pr-3"><div class="col-12 d-flex justify-content-between align-items-center text-muted pl-0 pr-0"><div class="footer-left"><p class="mb-0"> © 2024 <a href="https://github.com/username">DevSlem</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/rl/">RL</a> <a class="post-tag" href="/tags/algorithm/">Algorithm</a> <a class="post-tag" href="/tags/unity/">Unity</a> <a class="post-tag" href="/tags/function-approximation-rl/">Function Approximation RL</a> <a class="post-tag" href="/tags/sorting/">Sorting</a> <a class="post-tag" href="/tags/enemy/">Enemy</a> <a class="post-tag" href="/tags/game-development/">Game Development</a> <a class="post-tag" href="/tags/project/">Project</a> <a class="post-tag" href="/tags/attribute/">Attribute</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a><div id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-animation="true" data-autohide="false"><div class="toast-header"> <button type="button" class="ml-2 ml-auto close" data-dismiss="toast" aria-label="Close"> <span aria-hidden="true">&times;</span> </button></div><div class="toast-body text-center pt-0"><p class="pl-2 pr-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></div><script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] }, svg: { scale: 1.1 } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script>
