<!DOCTYPE html><html lang="en" data-mode="dark" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="Eligibility Traces" /><meta property="og:locale" content="en" /><meta name="description" content="이번 포스트에서는 TD와 Monte Carlo method를 통합 및 일반화하는 eligibility traces에 대해 다룰 것이다." /><meta property="og:description" content="이번 포스트에서는 TD와 Monte Carlo method를 통합 및 일반화하는 eligibility traces에 대해 다룰 것이다." /><link rel="canonical" href="https://devslem.github.io/reinforcement-learning/rl-fundamental/eligibility-traces/" /><meta property="og:url" content="https://devslem.github.io/reinforcement-learning/rl-fundamental/eligibility-traces/" /><meta property="og:site_name" content="DevSlem Blog" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-09-01T00:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Eligibility Traces" /><meta name="google-site-verification" content="SSJwUK1JSPZV3wjQS19f2bXfHYIupbYEFsisH4Nuns4" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-10-11T00:00:00+09:00","datePublished":"2022-09-01T00:00:00+09:00","description":"이번 포스트에서는 TD와 Monte Carlo method를 통합 및 일반화하는 eligibility traces에 대해 다룰 것이다.","headline":"Eligibility Traces","mainEntityOfPage":{"@type":"WebPage","@id":"https://devslem.github.io/reinforcement-learning/rl-fundamental/eligibility-traces/"},"url":"https://devslem.github.io/reinforcement-learning/rl-fundamental/eligibility-traces/"}</script><title>Eligibility Traces | DevSlem Blog</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="DevSlem Blog"><meta name="application-name" content="DevSlem Blog"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> </a></div><div class="site-title mt-3"> <a href="/">DevSlem Blog</a></div><div class="site-subtitle font-italic">It's a blog where i write posts about AI, RL, Unity, etc.</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/DevSlem" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['devslem12','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper"><div id="topbar" class="container d-flex align-items-center justify-content-between h-100 pl-3 pr-3 pl-md-4 pr-md-4"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Eligibility Traces</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper" class="d-flex justify-content-center"><div id="main" class="container pl-xl-4 pr-xl-4"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pr-xl-4"><div class="post pl-1 pr-1 pl-md-2 pr-md-2"><h1 data-toc-skip>Eligibility Traces</h1><div class="post-meta text-muted"> <span> Posted <em class="" data-ts="1661958000" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Sep 1, 2022 </em> </span> <span> Updated <em class="" data-ts="1665446400" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Oct 11, 2022 </em> </span><div class="d-flex justify-content-between"> <span> By <em> <a href="https://github.com/username">DevSlem</a> </em> </span><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2829 words"> <em>15 min</em> read</span></div></div></div><div class="post-content"><p>이번 포스트에서는 TD와 Monte Carlo method를 통합 및 일반화하는 eligibility traces에 대해 다룰 것이다.</p><h2 id="introduction"><span class="mr-2">Introduction</span><a href="#introduction" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p><strong>eligibility traces는 TD와 Monte Carlo (MC) method를 통합 및 일반화하는 방법으로 스펙트럼에 걸쳐 있다.</strong> 스펙트럼의 양 끝에는 MC method ($\lambda=1$)와 1-step TD method ($\lambda=0$)가 있다. 또한 eligibility traces는 MC method를 online 학습과 continuing task에서의 학습을 가능하게 한다.</p><p>eligibility traces와 비슷한 방법으로 $n$-step TD method가 존재한다.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> 그러나 eligibility traces는 $n$-step TD method보다 우아한 알고리즘적 메커니즘을 지니고, 상당한 계산적 이점을 가진다.</p><p>eligibility traces는 <strong>단기 기억 vector인 <em>eligibility trace</em> $\mathbf{z}_t \in \mathbb{R}^d$와 동시에 장기 기억 weight vector $\mathbf{w}_t \in \mathbb{R}^d$를 사용</strong>한다. 이 둘이 무슨 역할을 하는 지 곧 알아볼 것이다.</p><p>eligibility traces가 $n$-step method에 비해 가지는 주요한 계산적 이점은 마지막 $n$개의 feature vector를 저장하는 대신, 단 하나의 trace vector만을 사용한다는 것에서 비롯된다. 또한 $n$-step method는 $n-1$ time step만큼 학습이 지연되고 episode 종료를 포착해야하는 반면, eligibility traces는 학습이 지속적이고 균일하게 수행된다.</p><p>MC method와 $n$-step method는 각각 모든 미래 reward와 $n$개의 reward에 기반해 업데이트가 수행된다. 이렇게 업데이트되는 state로부터 앞 혹은 미래를 바라보는 것에 기반하는 방법을 <em>forward view</em>라고 부른다. 그러나 eligibility trace를 사용할 경우, 업데이트되는 state로부터 최근 방문했던 state를 향해 뒤 혹은 과거를 바라보는데, 이러한 방법을 <em>backward view</em>라고 한다.</p><p>이 내용들을 이제 차근차근 알아볼 것이다. 평소처럼 먼저 state value에 대한 prediction을 알아본 뒤, action value와 control로 확장한다.</p><h2 id="the-lambda-return"><span class="mr-2">The $\lambda$-return</span><a href="#the-lambda-return" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>먼저, $n$-step return에 대해 리뷰를 하자. $n$-step return $G_{t:t+n}$은 아래와 같이 $n$개의 discounted reward와 $n$-step에 도달된 state의 discounted 추정치를 더한 값으로 정의된다.</p>\[G_{t:t+n} \doteq R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1}R_{t+n} + \gamma^n\hat{v}(S_{t+n}, \mathbf{w}_{t+n-1}), \quad 0 \leq t \leq T - n\]<p>$\hat{v}(s,\mathbf{w})$는 weight vector $\mathbf{w}$가 주어졌을 때 state $s$에서의 추정치이다. $T$는 episode의 terminal time step이며 $n \geq 1$이다.</p><p>우리의 이번 목적인 <strong>$\lambda$-return은 모든 $n$에 대한 $n$-step return들의 평균</strong>으로 TD($\lambda$) 알고리즘에 사용된다. 각 가중치는 $\lambda^{n-1} \text{ (where $\lambda \in [0, 1)$)}$에 비례하며, 가중치의 총합을 1로 설정하기 위해 $1 - \lambda$에 의해 정규화된다. 아래는 $\lambda$-return의 정의이다.</p>\[G_t^\lambda \doteq (1 - \lambda) \sum_{n=1}^\infty \lambda^{n-1}G_{t:t+n}\]<p>위 수식을 아래와 같이 episode 종료 전후로 분리할 수 있다.</p>\[G_t^\lambda = (1 - \lambda)\sum_{n=1}^{T-t-1} \lambda^{n-1}G_{t:t+n} + \lambda^{T-t-1}G_t\]<p>이 수식은 $\lambda = 1$일 때의 상황을 더 명확히 해준다. $\lambda = 1$일 때 왼쪽 main sum은 0이 되며 기본적인 return $G_t$만 남게 된다. 따라서 $\lambda = 1$일 때 $\lambda$-return은 MC method이다. 반대로 $\lambda = 0$일 때 오직 one-step return $G_{t:t+1}$만 남게 되기 때문에 one-step TD method이다. $\lambda$-return은 $n$-step return과 비교했을 때 MC method와 one-step TD method 사이를 조금 더 부드럽게 이동할 수 있다. 아래는 $\lambda$-return에서 $n$-step return sequence에 가중치를 부여하는 것을 나타낸 backup diagram이다.</p><p><img data-src="/assets/images/rl-sutton-td-lambda-backup-diagram.png" alt="" width="70%" data-proofer-ignore> <em>Fig 1. The backup diagram for TD($\lambda$).<br /> (Image source: Sec 12.1 Sutton &amp; Barto (2020).)</em></p><p>아래는 각 $n$-step return에 부여되는 가중치의 변화 추이를 나타내는 그림이다. terminal time step 이후의 $n$-step return은 실제 return $G_t$이다.</p><p><img data-src="/assets/images/rl-sutton-fig12.2.png" alt="" data-proofer-ignore> <em>Fig 2. Weighting given in the $\lambda$-return to each of the $n$-step returns.<br /> (Image source: Sec 12.1 Sutton &amp; Barto (2020).)</em></p><p>이제 $\lambda$-return에 기반한 첫번째 알고리즘을 정의하자. 굉장히 naive한 알고리즘으로 <em>off-line $\lambda$-return algorithm</em>이라고 부른다. off-line 알고리즘인 이유는 episode 동안 weight vector가 변하지 않기 때문이다. <strong>episode 종료 후에, 전체 off-line update sequence가 아래 일반적인 semi-gradient rule을 따라 수행</strong>된다. 이 때 target은 $\lambda$-return $G_t^\lambda$이다.</p>\[\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \Big[G_t^\lambda - \hat{v}(S_t, \mathbf{w}_t) \Big] \nabla \hat{v}(S_t, \mathbf{w}_t), \quad t = 0, \dots, T - 1\]<p>그렇다면 왜 episode 종료까지 기다려야 할까? 이유는 비교적 간단하다. $n$-step TD method는 $n$-step return을 계산하기 위해 $n$번의 transition이 발생할 때까지 기다려야 했다. $\lambda$-return은 기본적으로 $G_{t:t+1}$부터 $G_t$까지 모든 return을 포함한다. $G_t$를 계산하기 위해서는 episode 종료를 기다려야만 한다.</p><p>우리가 지금까지 알아본 방법은 forward view이다. time step $t$에서 어떤 state $S_t$를 update할 때 우리는 $t+1, t+2, \dots$와 같이 미래의 보상과 state를 본다. 이 state를 업데이트한 후 다음 state로 넘어가면, 우리는 이전 state를 결코 다시 보지 않는다. 반대로 미래의 것들은 반복적으로 처리된다. 아래는 이러한 forward view의 관계를 나타내는 그림이다.</p><p><img data-src="/assets/images/rl-sutton-fig12.4.png" alt="" data-proofer-ignore> <em>Fig 3. The forward view.<br /> (Image source: Sec 12.1 Sutton &amp; Barto (2020).)</em></p><h2 id="tdlambda"><span class="mr-2">TD($\lambda$)</span><a href="#tdlambda" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>TD($\lambda$)는 off-line $\lambda$-return을 아래와 같이 3가지 방식으로 개선하였다.</p><ol><li>episode의 모든 step에서 weight vector를 업데이트할 수 있음<li>1의 이유로 계산이 동등하게 분배됨<li>1의 이유로 continuing problem에 적용 가능</ol><p>eligibility trace는 vector $\mathbf{z}_t \in \mathbb{R}^d$는 weight vector $\mathbf{w}_t$와 동일한 차원이다. <strong>weight vector는 시스템의 lifetime 동안 누적되는 장기기억인 반면, eligibility trace는 한 episode 길이보다 적게 지속되는 단기기억이다.</strong> 이러한 eligibility trace는 weigh vector에 영향을 미친다.</p><p>TD($\lambda$)에서 eligibility trace vector는 episode 시작 시에 zero vector로 초기화되며 아래와 같이 업데이트 된다.</p>\[\begin{align*} &amp; \mathbf{z}_{-1} \doteq \mathbf{0}, \\ &amp; \mathbf{z}_t \doteq \gamma \lambda \mathbf{z}_{t-1} + \nabla \hat{v}(S_t, \mathbf{w}_t), \quad 0 \leq t \leq T \end{align*}\]<p>$\lambda$는 이전 section에서 소개된 parameter로 이제 trace-decay parameter로 부를 것이다. eligibility trace는 weight vector의 각 원소가 최근 state value에 어떻게 기여하는지를 추적한다. 여기서 “최근”은 $\gamma\lambda$에 의해 정의된다.</p><p>이제 eligibility trace를 사용해 weight vector를 업데이트하는 방법을 살펴보자. 먼저, state value에 대한 TD error는 아래와 같다.</p>\[\delta_t \doteq R_{t+1} + \gamma \hat{v}(S_{t+1}, \mathbf{w}_t) - \hat{v}(S_t, \mathbf{w}_t)\]<p>TD($\lambda$)에서 weight vector는 scalar TD error와 vector eligibility trace에 비례하여 업데이트 된다.</p>\[\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \delta_t \mathbf{z}_t\]<p>아래는 알고리즘이다.</p><blockquote><h5 id="textalgorithm-semi-gradient-tdlambda-for-estimating-hatv-approx-v_pi"><span class="mr-2">$\text{Algorithm: Semi-gradient TD($\lambda$) for estimating $\hat{v} \approx v_\pi$}$</span><a href="#textalgorithm-semi-gradient-tdlambda-for-estimating-hatv-approx-v_pi" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5><p>\(\begin{align*} &amp; \textstyle \text{Input: the policy $\pi$ to be evaluated} \\ &amp; \textstyle \text{Input: a differentiable function $\hat{v} : \mathcal{S}^+ \times \mathbb{R}^d \rightarrow \mathbb{R}$ such that $\hat{v}(\text{terminal}, \cdot) = 0$} \\ &amp; \textstyle \text{Algorithm parameters: step size $\alpha &gt; 0$, trace decay rate $\lambda \in [0,1]$} \\ &amp; \textstyle \text{Initialize value-function weights $\mathbf{w}$ arbitrarily (e.g., $\mathbf{w} = \mathbf{0}$)} \\ \\ &amp; \textstyle \text{Loop for each episode:} \\ &amp; \textstyle \qquad \text{Initialize $S$} \\ &amp; \textstyle \qquad \mathbf{z} \leftarrow \mathbf{0} \qquad \text{(a $d$-dimensional vector)} \\ &amp; \textstyle \qquad \text{Loop for each step of episode:} \\ &amp; \textstyle \qquad\qquad \text{Choose } A \sim \pi(\cdot \vert S) \\ &amp; \textstyle \qquad\qquad \text{Take action $A$, observe $R, S'$} \\ &amp; \textstyle \qquad\qquad \mathbf{z} \leftarrow \gamma \lambda \mathbf{z} + \nabla \hat{v}(S, \mathbf{w}) \\ &amp; \textstyle \qquad\qquad \delta \leftarrow R + \gamma \hat{v}(S', \mathbf{w}) - \hat{v}(S, \mathbf{w}) \\ &amp; \textstyle \qquad\qquad \mathbf{w} \leftarrow \mathbf{w} + \alpha \delta \mathbf{z} \\ &amp; \textstyle \qquad\qquad S \leftarrow S' \\ &amp; \textstyle \qquad \text{until $S'$ is terminal} \\ \end{align*}\)</p></blockquote><p>TD($\lambda$)는 시간을 거꾸로 향한다. $\lambda$에 의해 현재 time step으로부터 시간적으로 더 멀리 떨어진 이전 state일 수록 더 적게 update된다. 더 멀리 떨어질 수록 더 많이 discount되기 때문이다. <strong>즉, 더 이전의 state에게 TD error에 대한 더 낮은 신용을 준다.</strong> 아래는 이에 대한 그림이다.</p><p><img data-src="/assets/images/rl-sutton-fig12.5.png" alt="" data-proofer-ignore> <em>Fig 4. The backward view.<br /> (Image source: Sec 12.2 Sutton &amp; Barto (2020).)</em></p><p>$\lambda = 0$일 때 $\mathbf{z}_t = \nabla\hat{v}(S_t, \mathbf{w}_t)$로 TD($\lambda$)의 update는 one-step semi-gradient TD update가 된다. 이를 TD(0)라 부른다. 반대로 $\lambda$가 1일 때는 신용은 오직 $\gamma$에 의해서만 감소한다. 이는 결국 MC method로 여길 수 있다. 이를 TD(1)이라고 부른다. 그러나 TD(1)은 MC method와 다르게 continuing task에도 적용할 수 있으며, online으로 학습할 수 있다.</p><h2 id="n-step-truncated-lambda-return-methods"><span class="mr-2">$n$-step Truncated $\lambda$-return Methods</span><a href="#n-step-truncated-lambda-return-methods" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>앞서 봤던 off-line $\lambda$-return 알고리즘을 개선시켜보자. <strong>$\lambda$-return의 근본적인 문제는 episode 종료 전까지 실제 return $G_t$를 모른다는 것이다.</strong> 우리는 이 문제에 대해 이미 수없이 다뤄왔다. <strong>실제 return $G_t$를 bootstrapping을 통해 근사화하면 된다.</strong> 데이터가 이후 horizon $h$까지 주어졌을 때, $\lambda$-return을 아래와 같이 변경할 것이며 이를 <em>truncated $\lambda$-return</em>이라고 한다.</p>\[G_{t:h}^\lambda \doteq (1 - \lambda) \sum_{n=1}^{h-t-1} \lambda^{n-1} G_{t:t+n} + \lambda^{h-t-1}G_{t:h}, \quad 0 \leq t &lt; h \leq T\]<p>기존 $\lambda$-return에서 terminal time step $T$가 horizon $h$로 대체되었으며, 실제 return $G_t$가 $n$-step return $G_{t:h}$로 대체되었다. 기존 $n$-step method에서는 $n$-step return만 사용했지만 여기서는 $1 \leq k \leq n$에 대해 모든 $k$-step return이 포함된다. truncated $\lambda$-return을 사용한 TD update를 Truncated TD($\lambda$) (TTD($\lambda$))라고 하며 아래는 이에 대한 backup diagram이다.</p><p><img data-src="/assets/images/rl-sutton-fig12.7.png" alt="" width="70%" data-proofer-ignore> <em>Fig 5. The backup diagram for Truncated TD($\lambda$).<br /> (Image source: Sec 12.3 Sutton &amp; Barto (2020).)</em></p><p>기존 TD($\lambda$)와 비교했을 때 TTD($\lambda$)는 가장 긴 요소가 episode 종료가 아닌, 최대 $n$-step까지 diagram이 이어진다. TTD($\lambda$)는 아래와 같이 정의된다.</p>\[\mathbf{w}_{t+n} \doteq \mathbf{w}_{t+n-1} + \alpha \big[G_{t:t+n}^\lambda - \hat{v}(S_t, \mathbf{w}_{t+n-1}) \big] \nabla\hat{v}(S_t, \mathbf{w}_{t+n-1}), \quad 0 \leq t &lt; T\]<h2 id="sarsalambda"><span class="mr-2">Sarsa($\lambda$)</span><a href="#sarsalambda" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>이제 action-value method로 확장하자. off-line $\lambda$-return algorithm의 action-value 형식은 단순히 $\hat{v}$을 $\hat{q}$으로 대체하기만 하면 된다.</p>\[\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \Big[G_t^\lambda - \hat{q}(S_t,A_t,\mathbf{w}_t) \Big] \nabla \hat{q}(S_t,A_t,\mathbf{w}_t), \quad t = 0,\dots,T-1\]<p>$G_t^\lambda \doteq G_{t:\infty}^\lambda$이다.</p><p>action value에 대한 TD method는 이러한 forward view를 근사화한다. 이를 <em>Sarsa($\lambda$)</em>라고 하며 TD($\lambda$)와 동일한 update rule을 가진다.</p>\[\begin{align*} &amp; \mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \delta_t \mathbf{z}_t, \\ &amp; \delta_t \doteq R_{t+1} + \gamma \hat{q}(S_{t+1},A_{t+1},\mathbf{w}_t) - \hat{q}(S_t,A_t,\mathbf{w}_t), \\ &amp; \mathbf{z}_{-1} \doteq \mathbf{0}, \\ &amp; \mathbf{z_t} \doteq \gamma \lambda \mathbf{z}_{t-1} + \nabla \hat{q}(S_t,A_t,\mathbf{w}_t), \quad 0 \leq t \leq T \end{align*}\]<p>아래는 Sarsa($\lambda$)의 backup diagram이다.</p><p><img data-src="/assets/images/rl-sutton-fig12.9.png" alt="" width="70%" data-proofer-ignore> <em>Fig 6. Sarsa($\lambda$)’s backup diagram.<br /> (Image source: Sec 12.7 Sutton &amp; Barto (2020).)</em></p><p>아래는 그동안 알아보았던 Sarsa를 비교하는 좋은 그림이다. 왜 eligibility trace가 one-step과 $n$-step method보다도 상당히 효율적인지 알 수 있다.</p><p><img data-src="/assets/images/rl-sutton-traces-in-gridworld.png" alt="" data-proofer-ignore> <em>Fig 7. Comparison of control algorithms in Gridworld.<br /> (Image source: Sec 12.7 Sutton &amp; Barto (2020).)</em></p><p><strong>eligibility trace method는 episode의 시작까지 모든 action value를 업데이트 하지만 최근으로부터 멀리 떨어질 수록 더 적은 정도로 업데이트한다.</strong> 가장 매력적이고 종종 가장 강력한 방법이다.</p><h2 id="summary"><span class="mr-2">Summary</span><a href="#summary" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><ul><li>$\lambda$-return은 $n$-step return의 모든 $n$에 대한 평균으로 더 일반화된 형식<li>weight vector는 장기 기억, eligibility trace vector는 단기 기억<li>TD($\lambda$)는 backward view로 이전 state들을 추적<li>eligibility trace는 episode의 시작까지 모든 value를 최근성에 따라 다른 정도로 업데이트</ul><h2 id="references"><span class="mr-2">References</span><a href="#references" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>[1] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction; 2nd Edition. 2020.</p><h2 id="footnotes"><span class="mr-2">Footnotes</span><a href="#footnotes" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><div class="footnotes" role="doc-endnotes"><ol><li id="fn:1" role="doc-endnote"><p>DevSlem. <a href="../n-step-bootstrapping/">n-step Bootstrapping.</a>. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p></ol></div></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/reinforcement-learning/'>Reinforcement Learning</a>, <a href='/categories/rl-fundamental/'>RL Fundamental</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/rl/" class="post-tag no-text-decoration" >RL</a> <a href="/tags/ai/" class="post-tag no-text-decoration" >AI</a> <a href="/tags/function-approximation-rl/" class="post-tag no-text-decoration" >Function Approximation RL</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Eligibility+Traces+-+DevSlem+Blog&url=https%3A%2F%2Fdevslem.github.io%2Freinforcement-learning%2Frl-fundamental%2Feligibility-traces%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Eligibility+Traces+-+DevSlem+Blog&u=https%3A%2F%2Fdevslem.github.io%2Freinforcement-learning%2Frl-fundamental%2Feligibility-traces%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fdevslem.github.io%2Freinforcement-learning%2Frl-fundamental%2Feligibility-traces%2F&text=Eligibility+Traces+-+DevSlem+Blog" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div><script src="https://utteranc.es/client.js" repo="DevSlem/DevSlem.github.io" issue-term="pathname" label="comment" theme="github-dark" crossorigin="anonymous" async> </script></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/reinforcement-learning/drl-methods/dqn/">DQN: Deep Q-Networks</a><li><a href="/reinforcement-learning/drl-methods/dtqn/">DTQN: Deep Transformer Q-Networks for Partially Observable Reinforcement Learning</a><li><a href="/wsl-setup/">Windows Subsystem for Linux (WSL) Setup for Reinforcement Learning</a><li><a href="/algorithm/sorting/merge-sort/">[알고리즘] 합병 정렬</a><li><a href="/algorithm/sorting/shell-sort/">[알고리즘] 쉘 정렬</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/rl/">RL</a> <a class="post-tag" href="/tags/algorithm/">Algorithm</a> <a class="post-tag" href="/tags/unity/">Unity</a> <a class="post-tag" href="/tags/function-approximation-rl/">Function Approximation RL</a> <a class="post-tag" href="/tags/sorting/">Sorting</a> <a class="post-tag" href="/tags/enemy/">Enemy</a> <a class="post-tag" href="/tags/game-development/">Game Development</a> <a class="post-tag" href="/tags/project/">Project</a> <a class="post-tag" href="/tags/attribute/">Attribute</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 pl-3 pr-3 pr-xl-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/reinforcement-learning/rl-fundamental/policy-gradient-methods/"><div class="card-body"> <em class="small" data-ts="1665586800" data-df="ll" > Oct 13, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Policy Gradient Methods</h3><div class="text-muted small"><p> 드디어 긴 장정 끝에 대망의 마지막 챕터로 왔다. 이번 포스트에서는 그 유명한 policy gradient method에 대해 소개하려고 한다. 길고 긴 여정이 드디어 끝났다. Introduction 지금까지 우리는 value-based 기반의 방법을 다뤘었다. policy는 추정된 action value에 기반해 action을 선택했다. 이번에는...</p></div></div></a></div><div class="card"> <a href="/reinforcement-learning/rl-fundamental/on-policy-prediction-with-approximation/"><div class="card-body"> <em class="small" data-ts="1660748400" data-df="ll" > Aug 18, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>On-policy Prediction with Approximation</h3><div class="text-muted small"><p> 이 포스트에서는 reinforcement learning을 수행하는 새로운 방법인 function approximation에 대한 소개와 이를 바탕으로 on-policy method에서 prediction을 수행하는 방법을 소개한다. What is Function Approximation and Why needed? 지금까지 알아본 기존 Reinf...</p></div></div></a></div><div class="card"> <a href="/reinforcement-learning/rl-fundamental/on-policy-control-with-approximation/"><div class="card-body"> <em class="small" data-ts="1661180400" data-df="ll" > Aug 23, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>On-policy Control with Approximation</h3><div class="text-muted small"><p> 이 포스트에서는 function approximation을 사용한 prediction을 control로 확장할 것이다. 이를 위해 state-value function이 아닌 action-value function을 추정한다. 그 후 on-policy GPI의 일반적인 패턴을 따라 학습을 진행하는 방법을 알아본다. 이 포스트에서는 특히 semi-gra...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/reinforcement-learning/rl-fundamental/off-policy-methods-with-approximation/" class="btn btn-outline-primary" prompt="Older"><p>Off-policy Methods with Approximation</p></a> <a href="/reinforcement-learning/exploration-methods/exploration-by-random-network-distillation/" class="btn btn-outline-primary" prompt="Newer"><p>Exploration by Random Network Distillation</p></a></div></div></div><footer class="row pl-3 pr-3"><div class="col-12 d-flex justify-content-between align-items-center text-muted pl-0 pr-0"><div class="footer-left"><p class="mb-0"> © 2024 <a href="https://github.com/username">DevSlem</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/rl/">RL</a> <a class="post-tag" href="/tags/algorithm/">Algorithm</a> <a class="post-tag" href="/tags/unity/">Unity</a> <a class="post-tag" href="/tags/function-approximation-rl/">Function Approximation RL</a> <a class="post-tag" href="/tags/sorting/">Sorting</a> <a class="post-tag" href="/tags/enemy/">Enemy</a> <a class="post-tag" href="/tags/game-development/">Game Development</a> <a class="post-tag" href="/tags/project/">Project</a> <a class="post-tag" href="/tags/attribute/">Attribute</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a><div id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-animation="true" data-autohide="false"><div class="toast-header"> <button type="button" class="ml-2 ml-auto close" data-dismiss="toast" aria-label="Close"> <span aria-hidden="true">&times;</span> </button></div><div class="toast-body text-center pt-0"><p class="pl-2 pr-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></div><script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] }, svg: { scale: 1.1 } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script>
