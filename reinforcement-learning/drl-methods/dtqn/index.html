<!DOCTYPE html><html lang="en" data-mode="dark" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="DTQN: Deep Transformer Q-Networks for Partially Observable Reinforcement Learning" /><meta property="og:locale" content="en" /><meta name="description" content="DQN with Transformer" /><meta property="og:description" content="DQN with Transformer" /><link rel="canonical" href="https://devslem.github.io/reinforcement-learning/drl-methods/dtqn/" /><meta property="og:url" content="https://devslem.github.io/reinforcement-learning/drl-methods/dtqn/" /><meta property="og:site_name" content="DevSlem Blog" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2024-05-07T00:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="DTQN: Deep Transformer Q-Networks for Partially Observable Reinforcement Learning" /><meta name="google-site-verification" content="SSJwUK1JSPZV3wjQS19f2bXfHYIupbYEFsisH4Nuns4" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-05-07T00:00:00+09:00","datePublished":"2024-05-07T00:00:00+09:00","description":"DQN with Transformer","headline":"DTQN: Deep Transformer Q-Networks for Partially Observable Reinforcement Learning","mainEntityOfPage":{"@type":"WebPage","@id":"https://devslem.github.io/reinforcement-learning/drl-methods/dtqn/"},"url":"https://devslem.github.io/reinforcement-learning/drl-methods/dtqn/"}</script><title>DTQN: Deep Transformer Q-Networks for Partially Observable Reinforcement Learning | DevSlem Blog</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="DevSlem Blog"><meta name="application-name" content="DevSlem Blog"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> </a></div><div class="site-title mt-3"> <a href="/">DevSlem Blog</a></div><div class="site-subtitle font-italic">It's a blog where i write posts about AI, RL, Unity, etc.</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/DevSlem" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['devslem12','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper"><div id="topbar" class="container d-flex align-items-center justify-content-between h-100 pl-3 pr-3 pl-md-4 pr-md-4"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>DTQN: Deep Transformer Q-Networks for Partially Observable Reinforcement Learning</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper" class="d-flex justify-content-center"><div id="main" class="container pl-xl-4 pr-xl-4"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pr-xl-4"><div class="post pl-1 pr-1 pl-md-2 pr-md-2"><h1 data-toc-skip>DTQN: Deep Transformer Q-Networks for Partially Observable Reinforcement Learning</h1><div class="post-meta text-muted"> <span> Posted <em class="" data-ts="1715007600" data-df="ll" data-toggle="tooltip" data-placement="bottom"> May 7, 2024 </em> </span> <span> Updated <em class="" data-ts="1715040000" data-df="ll" data-toggle="tooltip" data-placement="bottom"> May 7, 2024 </em> </span><div class="d-flex justify-content-between"> <span> By <em> <a href="https://github.com/username">DevSlem</a> </em> </span><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2977 words"> <em>16 min</em> read</span></div></div></div><div class="post-content"><p>이 포스트에서는 요즘 가장 핫한 딥러닝 모델인 Transformer를 DQN에 적용한 <a href="https://arxiv.org/abs/2106.04569">Deep Transformer Q-Networks for Partially Observable Reinforcement Learning</a> 논문에 대해 소개한다. 이 논문의 주 목적은 POMDP 상황에서 RNN 계열의 한계를 극복하고자 Transformer를 DQN에 적용한 것이다.</p><h2 id="partial-observability"><span class="mr-2">Partial Observability</span><a href="#partial-observability" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>강화학습에서 agent가 현재 state에 대한 모든 정보를 알고 있는 경우, 이를 fully observable MDP라고 한다. 그러나, 대부분의 실제 환경에서는 agent가 현재 state에 대한 모든 정보를 알 수 없는 경우가 많다. agent는 현재 state로부터 <strong>관찰된 일부 정보만을 가지고 의사결정</strong>을 해야하며, 이러한 상황을 <em>partially observable markov decision process</em> (POMDP)라고 한다. 아래는 POMDP environment인 Gym-Gridverse이다:</p><p><img data-src="/assets/images/drl/dtqn/2024-05-07-20-06-16.png" alt="" width="60%" data-proofer-ignore> <em>Fig 1. POMDP: Gym-Gridverse.<br /> (Image source: <a href="https://arxiv.org/abs/2106.04569">Deep Transformer Q-Networks for Partially Observable Reinforcement Learning</a>.)</em></p><p>위 그림에서, 위쪽 행은 전체 state를 나타내는 그림이고, 아래쪽 행은 agent가 관찰할 수 있는 부분만을 나타낸 그림이다. agent는 X표시되어있는 beacon의 색깔과 동일한 깃발에 도달해야한다. 일반적인 MDP는 agent가 현재 state만을 가지고 action을 선택하는 markov property를 가정한다. 그러나, 위와 같은 환경에서는 agent가 현재 state에 대한 정보를 완전히 알지 못하기 때문에 학습에 어려움을 겪으며, 종종 실패한다. 따라서, 이러한 문제를 다루기 위한 방법이 필요하다.</p><h2 id="limitation-of-rnn"><span class="mr-2">Limitation of RNN</span><a href="#limitation-of-rnn" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>partial observability를 다루기 위해서는 RL agent는 <strong>이전의 observation들을 기억</strong>할 필요가 있다. RL에서는 memory component를 추가해 agent가 이전의 observation을 참조할 수 있도록 한다. 이러한 memory component로 recurrent neural network (RNN)이 많이 사용되었다. LSTM이나 GRU와 같은 RNN 계열의 방법들은 observation 혹은 action-observation history에 대한 sequence를 sequential하게 처리함으로써 POMDP 문제를 해결할 수 있었다. 그러나, RNN은 long-term dependency에 취약하다. 이는 RNN이 긴 sequence에 대한 정보를 잘 학습하지 못한다는 것을 의미한다. 이로 인해 RNN을 사용했다고 할 지라도, POMDP 문제에서 종종 학습에 실패하는 경우가 발생한다.</p><p>이는 RL만의 문제가 아니다. NLP 분야에서도 RNN의 한계가 논의되었고, 이를 극복하기 위해 Transformer가 제안되었다. Transformer는 self-attention mechanism을 사용하여 long-term dependency를 잘 학습할 수 있다. 따라서, Transformer를 RL에 적용하여 RNN의 한계를 극복하고자 한 것이 이 논문의 주 목적이다.</p><h2 id="related-work"><span class="mr-2">Related Work</span><a href="#related-work" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>몰론, RL에 Transformer를 적용한 연구는 이 논문이 처음이 아니다. 대표적으로 <a href="https://arxiv.org/abs/2106.01345">Decision Transformer</a> 등이 있다. 그러나, 이전 연구들은 주로 offline RL setting이거나 supervised learning setting이었다. 반면, 이 논문에서 제안한 방법은 완전히 online RL setting이다. 나는 이 논문에서는 DQN을 사용했지만 다른 보다 강력한 RL 알고리즘에도 쉽게 적용할 수 있을 걸로 본다.</p><h2 id="deep-transformer-q-networks"><span class="mr-2">Deep Transformer Q-Networks</span><a href="#deep-transformer-q-networks" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>이제 본격적으로 이 논문에서 제안한 Deep Transformer Q-Networks (DTQN)에 대해 알아보자. DTQN은 DQN에 Transformer를 활용한 모델로 아래는 DTQN의 핵심 요소이다:</p><ol><li><strong>Observation Embedding</strong>: 현재 observation을 포함한 observation history를 embedding한다. 이는 Transformer의 input으로 사용된다.<li><strong>Transformer Decoder</strong>: Transformer의 decoder를 사용하여 observation history sequence를 처리한다. 이는 각 action의 Q-value를 추정하는 데 사용된다.<li><strong>Q-value Prediction</strong>: 예측된 Q-value를 사용하여 action을 선택하고, TD error를 계산하여 네트워크를 학습한다.</ol><p>아래는 DTQN의 architecture이다:</p><p><img data-src="/assets/images/drl/dtqn/2024-05-07-21-55-22.png" alt="" width="100%" data-proofer-ignore> <em>Fig 2. DTQN Overall Architecture.<br /> (Image source: <a href="https://arxiv.org/abs/2106.04569">Deep Transformer Q-Networks for Partially Observable Reinforcement Learning</a>.)</em></p><p>이제 구체적으로 각 요소에 대해 알아보자.</p><h3 id="observation-embedding-and-positional-encodings"><span class="mr-2">Observation Embedding and Positional Encodings</span><a href="#observation-embedding-and-positional-encodings" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>agent의 최근 $k$개의 observation history $h_{t:t+k}$에서 각각의 observation은 observation embedding layer를 통해 transformer의 dimensionality로 linearly projected된다. 이후, learned positional encoding을 각 observation에 더해준다.</p><p>NLP task에서 positional encoding은 Transformer에 흔히 사용되는 방법으로, 주로 sinusoidal positional encoding이 사용된다. 이는 문장의 각 token의 위치에 대한 정보를 제공함으로써 Transformer가 sequence의 순서를 학습할 수 있도록 도와준다. 그러나 <strong>RL에서는 observation history에서 각 observation의 순서가 중요할 수도 있고 아닐수도 있다</strong>. 이는 task와 environment에 따라 다르다. 따라서, 이 논문에서는 positional encoding을 학습 가능한 parameter로 설정하여 observation history의 순서에 대한 정보를 학습하도록 한다. 아래는 각 domain에 따라 학습된 positional encoding과 sinusoidal positional encoding을 비교한 결과이다:</p><p><img data-src="/assets/images/drl/dtqn/2024-05-07-22-10-53.png" alt="" width="80%" data-proofer-ignore> <em>Fig 3. Positional Encoding Comparisons.<br /> (Image source: <a href="https://arxiv.org/abs/2106.04569">Deep Transformer Q-Networks for Partially Observable Reinforcement Learning</a>.)</em></p><h3 id="transformer-decoder"><span class="mr-2">Transformer Decoder</span><a href="#transformer-decoder" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Transformer는 sequence data를 처리하는데 효과적임이 이미 널리 알려져있다. 특히, attention mechanism은 가장 중요한 token들에 더 많은 가중치 혹은 attention을 줌으로써 효과적으로 학습할 수 있다. Transformer는 encoder-decoder 구조로 되어있지만, 최근에는 주로 encoder (BERT)나 decoder (GPT)를 단독으로 사용한다. 두 방법의 주요한 차이점은 decoder는 attention layer에 causal masking을 적용하는 것이다. 즉, $i$번째 token은 $i$번째 이전의 token에만 attention을 줄 수 있다. 이는 decoder가 다음 token을 예측할 때, 이전 token들만을 참조할 수 있도록 한다. DTQN은 decoder만을 사용한다.</p><p>구체적으로 DTQN에서는 GPT와 같이 두개의 submodule로 구성된다: masked multi-headed self-attention과 position-wise feedforward network. 구체적으로는 다음 스텝을 따른다:</p><ol><li>앞서 embedding된 observation history를 weight matrix $W^Q$에 의해 query $Q$, $W^K$에 의해 key $K$, $W^V$에 의해 value $V$로 projection한다.<li>$Q$, $K$, $V$는 masked multi-headed self-attention을 거친 후, observation embedding과 combine된 후, layer normalization을 거친다.<li>이후, position-wise feedforward network를 거친 후, combine, layer normalization을 거친다.<li>이러한 과정을 $N$개의 Transformer block을 통해 $N$번 반복한다.<li>마지막 embedding은 action space로 projection되어 각 action에 대한 Q-value를 추정한다.</ol><p>사실 이는 전형적인 Transformer의 구조이다.</p><h3 id="q-value-prediction"><span class="mr-2">Q-value Prediction</span><a href="#q-value-prediction" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>이후의 과정은 DQN과 거의 유사하다. 먼저, DQN은 Mean Squared Bellman Error를 minimize하도록 학습된다:</p>\[L(\theta) = \mathbb{E}_{(s,a,r,s') \sim \mathcal{D}} [ ( r + \gamma \max_{a' \in \mathcal{A}} Q(s',a';\theta') - Q(s,a;\theta) )^2 ]\]<p>experience tuple $(s,a,r,s’)$는 replay buffer $\mathcal{D}$로부터 uniformly하게 샘플링된다. TD target $r + \max_{a’ \in \mathcal{A}} Q(s’,a’;\theta’)$는 $\theta’$으로 parameterized된 target network를 사용하여 계산된다. 이는 $\theta$에 의해 parameterized된 Q-network보다 지연되어 업데이트되기 때문에 학습을 안정화시킨다.</p><p>그러나 앞서 언급했듯이, partially observable 도메인에서는 네트워크의 입력을 state에서 observation으로 단순히 바꾸는 것만으로는 학습이 어렵다. 따라서, DTQN은 observation history를 Transformer로 처리하여 Q-value를 추정한다.</p><p>이때, DTQN은 observation history의 각 time step에 대한 모든 Q-value를 추정한다. agent가 action을 결정할 때에는 현재 time step $t$에 대한 Q-value만을 사용한다. 즉, 다시 말해 history의 마지막 time step에 대한 Q-value만을 사용한다. 그러나 학습할 때는 history 내의 모든 time step에 대한 Q-value를 사용한다. 몰론 마지막 time step에 대한 Q-value만을 사용하는 것이 직관적일 수 있지만 이는 매우 큰 낭비이다. 이는 실제로 모든 time step에 대한 Q-value를 사용했을 때 학습 성능이 크게 향상되었다.</p><h3 id="algorithm"><span class="mr-2">Algorithm</span><a href="#algorithm" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>아래는 DTQN의 알고리즘이다:</p><p><img data-src="/assets/images/drl/dtqn/2024-05-07-22-56-35.png" alt="" width="100%" data-proofer-ignore> <em>Fig 4. DTQN Algorithm.<br /> (Image source: <a href="https://arxiv.org/abs/2106.04569">Deep Transformer Q-Networks for Partially Observable Reinforcement Learning</a>.)</em></p><p>위 알고리즘에서 Q-value를 추정할 때 $s$가 아닌 $h_{t:t+i}$가 입력됨을 확인할 수 있다. 또한, loss를 계산할 때 casually-masked self-attention mechanism을 사용하기 때문에, 알고리즘에 묘사된 for loop는 실제로 one forward pass로 처리된다.</p><h2 id="experiments"><span class="mr-2">Experiments</span><a href="#experiments" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>이 논문에서는 DTQN을 다양한 POMDP 환경에서 평가하였으며, 다양한 ablation study를 진행하였다. 보다 자세한 결과와 해석은 논문을 직접 참고하기 바란다. 아래는 DTQN을 다른 방법과 비교한 결과이다 (파란색: DTQN, 주황색: DRQN, 갈색: DQN):</p><p><img data-src="/assets/images/drl/dtqn/2024-05-07-23-14-04.png" alt="" width="80%" data-proofer-ignore> <em>Fig 5. DTQN against Baselines.<br /> (Image source: <a href="https://arxiv.org/abs/2106.04569">Deep Transformer Q-Networks for Partially Observable Reinforcement Learning</a>.)</em></p><p>위 실험결과를 보면 알 수 있듯이 naive한 DQN은 POMDP 문제에서 학습에 실패하거나 어려움을 겪음을 알 수 있다. 이는 POMDP 문제를 해결하기 위해서는, memory component가 필요함을 보여준다. DRQN은 LSTM을 통해 DQN에 memory component를 추가한 것이다. DRQN은 분명 좋은 성능을 보이지만, 학습이 느리고 불안정함을 알 수 있다. 반면, DTQN은 빠른 학습 속도와 좋은 성능을 보인다.</p><p>아래는 ablation study 결과에 대한 표이다:</p><p><img data-src="/assets/images/drl/dtqn/2024-05-07-23-06-29.png" alt="" width="80%" data-proofer-ignore> <em>Fig 6. Ablations.<br /> (Image source: <a href="https://arxiv.org/abs/2106.04569">Deep Transformer Q-Networks for Partially Observable Reinforcement Learning</a>.)</em></p><p>먼저, DTQN에 learned position encoding을 사용했을 때 positional encoding을 사용하지 않았을 때보다 성능이 향상되었음을 알 수 있으며, sinusoidal positional encoding을 사용했을 때보다 약간 개선됨을 알 수 있다. 또한, 마지막 time step에 대한 Q-value만을 사용해 학습했을 때 성능이 매우 떨어짐을 확인할 수 있다.</p><p>Transformer의 이점 중 하나는 self-attention weight를 시각화 할 수 있는 것이다. 직관적으로, self-attention mechanism은 agent가 task를 해결하는데 가장 유용한 정보를 제공하는 observation에 더 많이 우선순위를 둘 것이다. 아래 그림은 Gridverse 환경에 대해 self-attention weight를 시각화한 결과이다:</p><p><img data-src="/assets/images/drl/dtqn/2024-05-09-16-59-47.png" alt="" width="60%" data-proofer-ignore> <em>Fig 7. Atttention Visualization.<br /> (Image source: <a href="https://arxiv.org/abs/2106.04569">Deep Transformer Q-Networks for Partially Observable Reinforcement Learning</a>.)</em></p><p>초록색 beacon을 포함하는 observation이 모든 future observations에 의해 attention을 받는 것을 확인할 수 있다. 이는 DTQN이 task를 해결하는데 어느 observation이 중요한지를 적절히 학습하고 있음을 나타낸다. agent가 초록색 flag를 볼 때 (위 그림에서 왼쪽), agent는 초록색 beacon에 attention을 주고, 올바른 flag임을 확실시 할 수 있다.</p><h2 id="summary"><span class="mr-2">Summary</span><a href="#summary" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>아래는 DTQN의 요약이다:</p><ul><li>DQN에 Transformer를 활용한 모델로, POMDP 문제를 해결하기 위해 제안되었다.<li>fully online RL setting에서 사용할 수 있다.<li>observation history를 Transformer decoder로 처리하여 Q-value를 추정한다.<li>learned positional encoding을 사용한다.<li>observation history의 추정된 모든 Q-value를 사용하여 학습한다.<li>DQN과 DRQN에 비해 빠른 학습 속도와 좋은 성능을 보인다.</ul><p>다만, 1가지 우려점이 존재한다. 원래, POMDP setting에서 observation history는 initial time step부터의 full history이다. 그러나, DTQN에서는 observation history가 context length $k$에 의해 truncated된다. 실제 실험에서 $k=50$으로 설정되었다. RNN은 이론상 full history를 처리한다. 그러나, DTQN은 truncated history이다. 만약, environment가 복잡하고 episode length가 매우 길때는 어떨지 궁금하다.</p><h2 id="references"><span class="mr-2">References</span><a href="#references" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>[1] Esslinger, Kevin, Robert Platt, and Christopher Amato. “<a href="https://arxiv.org/abs/2206.01078">Deep transformer q-networks for partially observable reinforcement learning</a>.” arXiv preprint arXiv:2206.01078 (2022).</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/reinforcement-learning/'>Reinforcement Learning</a>, <a href='/categories/drl-methods/'>DRL Methods</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/rl/" class="post-tag no-text-decoration" >RL</a> <a href="/tags/ai/" class="post-tag no-text-decoration" >AI</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=DTQN%3A+Deep+Transformer+Q-Networks+for+Partially+Observable+Reinforcement+Learning+-+DevSlem+Blog&url=https%3A%2F%2Fdevslem.github.io%2Freinforcement-learning%2Fdrl-methods%2Fdtqn%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=DTQN%3A+Deep+Transformer+Q-Networks+for+Partially+Observable+Reinforcement+Learning+-+DevSlem+Blog&u=https%3A%2F%2Fdevslem.github.io%2Freinforcement-learning%2Fdrl-methods%2Fdtqn%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fdevslem.github.io%2Freinforcement-learning%2Fdrl-methods%2Fdtqn%2F&text=DTQN%3A+Deep+Transformer+Q-Networks+for+Partially+Observable+Reinforcement+Learning+-+DevSlem+Blog" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div><script src="https://utteranc.es/client.js" repo="DevSlem/DevSlem.github.io" issue-term="pathname" label="comment" theme="github-dark" crossorigin="anonymous" async> </script></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/reinforcement-learning/drl-methods/dqn/">DQN: Deep Q-Networks</a><li><a href="/reinforcement-learning/drl-methods/dtqn/">DTQN: Deep Transformer Q-Networks for Partially Observable Reinforcement Learning</a><li><a href="/wsl-setup/">Windows Subsystem for Linux (WSL) Setup for Reinforcement Learning</a><li><a href="/algorithm/sorting/merge-sort/">[알고리즘] 합병 정렬</a><li><a href="/algorithm/sorting/shell-sort/">[알고리즘] 쉘 정렬</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/rl/">RL</a> <a class="post-tag" href="/tags/algorithm/">Algorithm</a> <a class="post-tag" href="/tags/unity/">Unity</a> <a class="post-tag" href="/tags/function-approximation-rl/">Function Approximation RL</a> <a class="post-tag" href="/tags/sorting/">Sorting</a> <a class="post-tag" href="/tags/enemy/">Enemy</a> <a class="post-tag" href="/tags/game-development/">Game Development</a> <a class="post-tag" href="/tags/project/">Project</a> <a class="post-tag" href="/tags/attribute/">Attribute</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 pl-3 pr-3 pr-xl-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/reinforcement-learning/drl-methods/dqn/"><div class="card-body"> <em class="small" data-ts="1715007600" data-df="ll" > May 7, 2024 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>DQN: Deep Q-Networks</h3><div class="text-muted small"><p> 이 포스트에서는 deep RL의 기본이자 시대를 열어준 DQN(Deep Q-Networks)을 도입한 Playing Atari with Deep Reinforcement Learning 논문에 대해 소개한다. Introduction DQN 이전에는, 강화학습에서 사용되는 Q-learning과 같은 tabular method는 state와 actio...</p></div></div></a></div><div class="card"> <a href="/reinforcement-learning/exploration-methods/exploration-by-random-network-distillation/"><div class="card-body"> <em class="small" data-ts="1663686000" data-df="ll" > Sep 21, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Exploration by Random Network Distillation</h3><div class="text-muted small"><p> 이 포스트에서는 exploration을 쉽고 효과적으로 수행할 수 있는 방법인 Exploration by Random Network Distillation 논문을 소개한다. Abstract exploration bonus는 observation feature의 예측 error임 고정 랜덤 초기화 신경망이 사용됨 extrinsic rew...</p></div></div></a></div><div class="card"> <a href="/reinforcement-learning/rl-fundamental/policy-gradient-methods/"><div class="card-body"> <em class="small" data-ts="1665586800" data-df="ll" > Oct 13, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Policy Gradient Methods</h3><div class="text-muted small"><p> 드디어 긴 장정 끝에 대망의 마지막 챕터로 왔다. 이번 포스트에서는 그 유명한 policy gradient method에 대해 소개하려고 한다. 길고 긴 여정이 드디어 끝났다. Introduction 지금까지 우리는 value-based 기반의 방법을 다뤘었다. policy는 추정된 action value에 기반해 action을 선택했다. 이번에는...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/reinforcement-learning/drl-methods/dqn/" class="btn btn-outline-primary" prompt="Older"><p>DQN: Deep Q-Networks</p></a><div class="btn btn-outline-primary disabled" prompt="Newer"><p>-</p></div></div></div></div><footer class="row pl-3 pr-3"><div class="col-12 d-flex justify-content-between align-items-center text-muted pl-0 pr-0"><div class="footer-left"><p class="mb-0"> © 2024 <a href="https://github.com/username">DevSlem</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/rl/">RL</a> <a class="post-tag" href="/tags/algorithm/">Algorithm</a> <a class="post-tag" href="/tags/unity/">Unity</a> <a class="post-tag" href="/tags/function-approximation-rl/">Function Approximation RL</a> <a class="post-tag" href="/tags/sorting/">Sorting</a> <a class="post-tag" href="/tags/enemy/">Enemy</a> <a class="post-tag" href="/tags/game-development/">Game Development</a> <a class="post-tag" href="/tags/project/">Project</a> <a class="post-tag" href="/tags/attribute/">Attribute</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a><div id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-animation="true" data-autohide="false"><div class="toast-header"> <button type="button" class="ml-2 ml-auto close" data-dismiss="toast" aria-label="Close"> <span aria-hidden="true">&times;</span> </button></div><div class="toast-body text-center pt-0"><p class="pl-2 pr-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></div><script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] }, svg: { scale: 1.1 } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script>
