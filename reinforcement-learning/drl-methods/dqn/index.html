<!DOCTYPE html><html lang="en" data-mode="dark" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="DQN: Deep Q-Networks" /><meta property="og:locale" content="en" /><meta name="description" content="deep RL의 기본이자 시작점인 DQN(Deep Q-Networks)에 대해 소개한다." /><meta property="og:description" content="deep RL의 기본이자 시작점인 DQN(Deep Q-Networks)에 대해 소개한다." /><link rel="canonical" href="https://devslem.github.io/reinforcement-learning/drl-methods/dqn/" /><meta property="og:url" content="https://devslem.github.io/reinforcement-learning/drl-methods/dqn/" /><meta property="og:site_name" content="DevSlem Blog" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2024-05-07T00:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="DQN: Deep Q-Networks" /><meta name="google-site-verification" content="SSJwUK1JSPZV3wjQS19f2bXfHYIupbYEFsisH4Nuns4" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-05-07T00:00:00+09:00","datePublished":"2024-05-07T00:00:00+09:00","description":"deep RL의 기본이자 시작점인 DQN(Deep Q-Networks)에 대해 소개한다.","headline":"DQN: Deep Q-Networks","mainEntityOfPage":{"@type":"WebPage","@id":"https://devslem.github.io/reinforcement-learning/drl-methods/dqn/"},"url":"https://devslem.github.io/reinforcement-learning/drl-methods/dqn/"}</script><title>DQN: Deep Q-Networks | DevSlem Blog</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="DevSlem Blog"><meta name="application-name" content="DevSlem Blog"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> </a></div><div class="site-title mt-3"> <a href="/">DevSlem Blog</a></div><div class="site-subtitle font-italic">It's a blog where i write posts about AI, RL, Unity, etc.</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/DevSlem" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['devslem12','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper"><div id="topbar" class="container d-flex align-items-center justify-content-between h-100 pl-3 pr-3 pl-md-4 pr-md-4"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>DQN: Deep Q-Networks</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper" class="d-flex justify-content-center"><div id="main" class="container pl-xl-4 pr-xl-4"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pr-xl-4"><div class="post pl-1 pr-1 pl-md-2 pr-md-2"><h1 data-toc-skip>DQN: Deep Q-Networks</h1><div class="post-meta text-muted"> <span> Posted <em class="" data-ts="1715007600" data-df="ll" data-toggle="tooltip" data-placement="bottom"> May 7, 2024 </em> </span> <span> Updated <em class="" data-ts="1715040000" data-df="ll" data-toggle="tooltip" data-placement="bottom"> May 7, 2024 </em> </span><div class="d-flex justify-content-between"> <span> By <em> <a href="https://github.com/username">DevSlem</a> </em> </span><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="1745 words"> <em>9 min</em> read</span></div></div></div><div class="post-content"><p>이 포스트에서는 deep RL의 기본이자 시대를 열어준 DQN(Deep Q-Networks)을 도입한 <a href="https://arxiv.org/abs/1312.5602">Playing Atari with Deep Reinforcement Learning</a> 논문에 대해 소개한다.</p><h2 id="introduction"><span class="mr-2">Introduction</span><a href="#introduction" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>DQN 이전에는, 강화학습에서 사용되는 Q-learning과 같은 tabular method는 state와 action space가 작은 경우에만 사용할 수 있었다. 몰론, DQN 이전에도 function approximation method가 있긴 했지만 한계가 뚜렸했다. 특히, 이미지와 같은 high-dimensional 입력으로부터 직접적으로 Q-value를 추정하는 것은 어려운 문제였다. 그러나 딥러닝의 발전으로 인해 high-dimensional raw data로부터 high-level feature를 추출하는 것이 가능해졌다. DQN은 이러한 딥러닝의 강력한 능력을 강화학습에 적용하고자 한 최초의 사례이다. DQN이 기존 tabular Q-learning과의 차이점은 action-value를 추정하기 위해 deep neural network를 사용한다는 것이다:</p><p><img data-src="/assets/images/drl/dqn/2024-05-07-17-50-26.png" alt="" width="80%" data-proofer-ignore> <em>Fig 1. Tabular Q-learning vs DQN.<br /> (Image source: <a href="https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python/">Ankit Choudhary</a>.)</em></p><p>그러나, 딥러닝을 강화학습에 적용할 때 몇가지 문제가 있다:</p><ol><li><strong>Delayed Rewards</strong>: 대부분의 딥러닝 문제들은 많은 양의 hand-labeled data를 사용하여 학습한다. 그러나 강화학습에서는 agent가 선택한 action에 대해 즉각적인 reward를 받지 않을 수 있으며, 이러한 reward는 sparse하거나 noisy하다.<li><strong>High Correlation</strong>: 연속적인 상태들은 서로 상관관계가 높을 수 있다. 이는 학습 데이터간의 <a href="https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables">i.i.d</a> 가정을 깨뜨릴 수 있다.<li><strong>Non-stationary Distribution</strong>: 강화학습에서는 agent가 학습을 진행함에 따라 얻게 되는 data의 distribution이 변할 수 있다. 이는 fixed underlying distribution을 가정하는 딥러닝에서 문제가 될 수 있다.</ol><p>DQN은 이러한 문제들을 해결하기 위해 다음과 같은 기법들을 사용한다:</p><ol><li><strong>CNN(Convolutional Neural Networks)</strong>: 이미지와 같은 high-dimensional raw data로부터 feature를 추출하기 위해 CNN을 사용한다.<li><strong>Experience Replay</strong>: agent가 경험한 데이터를 저장하고 이를 무작위로 뽑아서 학습한다. 이는 데이터간의 상관관계를 줄이고, 학습 데이터의 분포를 고정시킨다.<li><strong>Fixed Q-targets</strong>: target Q-value를 계산할 때 target network를 사용하여 target Q-value를 계산한다. 이는 학습 중 target Q-value를 고정시킴으로써 학습을 안정화시킨다.</ol><p>DQN이 도입한 이러한 기법들은 후에 나오는 발전된 알고리즘들에도 많은 영향을 끼쳤다.</p><h2 id="objective"><span class="mr-2">Objective</span><a href="#objective" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>DQN은 action-value를 추정하는 Q-function을 학습한다. 구체적으로, action-value를 추정하는 Q-network $Q(s,a;\theta)$를 정의하고, 이를 학습하여 optimal action-value function $Q^*(s,a)$에 근접시킨다. 이때, $\theta$는 neural network의 parameter이다. 만약, 각 iteration $i$마다 Q-network에 대한 target value인 $y_i$가 존재한다면, Q-network를 학습하기 위한 loss function $L_i(\theta_i)$를 아래와 같이 정의할 수 있다:</p>\[L_i(\theta_i) = \mathbb{E}_{s,a \sim \rho(\cdot)} \left[ \left( y_i - Q(s,a;\theta_i) \right)^2 \right]\]<p>그러나, 일반적으로 강화학습에서는 label이 존재하지 않기 때문에 target value인 $y_i$를 구성하기 어렵다. 이를 해결하기 위해 DQN은 target value를 temporal difference(TD) target으로 정의한다. TD target은 아래와 같이 정의된다:</p>\[y_i = \mathbb{E}_{s' \sim \mathcal{E}} \left[ r + \gamma \max_{a'} Q(s',a';\theta_{i-1}) \right \vert s,a]\]<p>TD target의 주요한 의미는, 현재 state $s$에서 action $a$를 선택했을 때 얻게 되는 실제 데이터 reward $r$과 다음 state $s’$에서 최적의 action을 선택했을 때 얻게 되는 action-value의 추정치를 고려하여 현재 state에서의 action-value를 추정한다는 것이다. 이러한 loss function의 gradient는 아래와 같다:</p>\[\nabla_{\theta_i} L_i(\theta_i) = \mathbb{E}_{s,a \sim \rho(\cdot); s' \sim \mathcal{E}} \left[ \left( r + \gamma \max_{a'} Q(s',a';\theta_{i-1}) - Q(s,a;\theta_i) \right) \nabla_{\theta_i} Q(s,a;\theta_i) \right]\]<p>몰론, 실제로는 위처럼 완전한 expectation을 계산하는 것이 아닌, stochastic gradient descent(SGD)를 사용하여 mini-batch를 통해 gradient를 추정한다.</p><p>이러한 알고리즘에는 주요한 특징이 있다:</p><ol><li><strong>Model-free</strong>: DQN은 model-free 알고리즘이다. 즉, environment의 dynamics에 대한 정보를 알 필요가 없으며, sample로부터 직접적으로 학습한다.<li><strong>Off-policy</strong>: DQN은 off-policy 알고리즘이다. experience samples는 behavior policy (e.g., epsilon-greedy policy)로부터 수집되지만, agent는 target policy인 greedy policy $a = \arg \max_a Q(s,a;\theta)$를 학습한다.</ol><h2 id="method"><span class="mr-2">Method</span><a href="#method" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>DQN의 핵심 component들은 아래와 같다:</p><ul><li><strong>Q-Network</strong>: Q-network는 state $s$로부터 action들에 대해 Q-value를 추정하는 neural network이다.<li><strong>Target Network</strong>: Target network는 target Q-value를 계산하기 위한 neural network이다. Target network는 fixed interval로 Q-network의 parameter를 복사하여 업데이트한다. 이는 학습 중 target Q-value를 고정시킴으로써 학습을 안정화시킨다.<li><strong>Experience Replay</strong>: Experience replay는 experience tuple $(s,a,r,s’)$를 저장하고 이를 무작위로 뽑아서 학습하는 기법이다. 이는 데이터간의 상관관계를 줄인다.</ul><p>아래는 DQN의 architecture이다:</p><p><img data-src="/assets/images/drl/dqn/2024-05-07-17-58-20.png" alt="" width="80%" data-proofer-ignore> <em>Fig 2. DQN Architecture.<br /> (Image source: <a href="https://ai-com.tistory.com/entry/RL-%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-1-DQN-Deep-Q-Network">이것저것 테크블로그</a>.)</em></p><p>agent는 매 time step $t$마다 environment 상호작용하면서 experience sample을 획득한 후 이를 replay buffer에 저장한다. 이후, replay buffer로부터 mini-batch를 뽑아서 Q-value를 학습한다. 이러한 과정을 통해 DQN은 optimal Q-value function에 근접하도록 학습된다. 아래는 DQN의 알고리즘이다:</p><p><img data-src="/assets/images/drl/dqn/2024-05-07-18-24-08.png" alt="" width="100%" data-proofer-ignore> <em>Fig 3. DQN Algorithm.<br /> (Image source: <a href="https://arxiv.org/abs/1312.5602">Playing Atari with Deep Reinforcement Learning</a>.)</em></p><p>몰론, 위 알고리즘에서는 따로 target network에 대해 기술되어있지 않지만, TD target $y_j$를 계산할 때 target network를 사용한다. 또한, target network는 fixed interval로 Q-network의 parameter를 복사하여 업데이트한다.</p><h2 id="limitations"><span class="mr-2">Limitations</span><a href="#limitations" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>DQN은 분명 Atari 게임과 같은 high-dimensional state space이며 다루는데 효과적이었다. 그러나 DQN에는 몇가지 한계점이 있다:</p><ol><li><strong>High-dimensional Action Space</strong>: DQN은 high-dimensional discrete action space나 continuous action space에는 적용하기 어렵다. 이는 Q-value를 추정하기 위해 discrete action space를 가정하기 때문이며, 모든 action에 대해 Q-value를 추정하기 때문이다.<li><strong>Off-policy</strong>: DQN은 off-policy method이기 때문에 sample-efficient하지만, 학습이 느리고 불안정할 수 있다.</ol><p>후에 나오는 알고리즘들은 이러한 한계점을 극복하기 위해 다양한 기법들을 사용한다.</p><h2 id="summary"><span class="mr-2">Summary</span><a href="#summary" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><ul><li>deep neural network를 사용해 high-dimensional state space를 효과적으로 다룰 수 있다.<li>experience replay와 target network를 사용하여 학습을 안정화시킨다.<li>model-free, off-policy method이다.<li>high-dimensional action space나 continuous action space에는 적용하기 어렵다.</ul><h2 id="references"><span class="mr-2">References</span><a href="#references" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>[1] Mnih, Volodymyr, et al. “<a href="https://arxiv.org/abs/1312.5602">Playing atari with deep reinforcement learning</a>.” arXiv preprint arXiv:1312.5602 (2013).</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/reinforcement-learning/'>Reinforcement Learning</a>, <a href='/categories/drl-methods/'>DRL Methods</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/rl/" class="post-tag no-text-decoration" >RL</a> <a href="/tags/ai/" class="post-tag no-text-decoration" >AI</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=DQN%3A+Deep+Q-Networks+-+DevSlem+Blog&url=https%3A%2F%2Fdevslem.github.io%2Freinforcement-learning%2Fdrl-methods%2Fdqn%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=DQN%3A+Deep+Q-Networks+-+DevSlem+Blog&u=https%3A%2F%2Fdevslem.github.io%2Freinforcement-learning%2Fdrl-methods%2Fdqn%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fdevslem.github.io%2Freinforcement-learning%2Fdrl-methods%2Fdqn%2F&text=DQN%3A+Deep+Q-Networks+-+DevSlem+Blog" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div><script src="https://utteranc.es/client.js" repo="DevSlem/DevSlem.github.io" issue-term="pathname" label="comment" theme="github-dark" crossorigin="anonymous" async> </script></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/reinforcement-learning/drl-methods/dqn/">DQN: Deep Q-Networks</a><li><a href="/reinforcement-learning/drl-methods/dtqn/">DTQN: Deep Transformer Q-Networks for Partially Observable Reinforcement Learning</a><li><a href="/wsl-setup/">Windows Subsystem for Linux (WSL) Setup for Reinforcement Learning</a><li><a href="/algorithm/sorting/merge-sort/">[알고리즘] 합병 정렬</a><li><a href="/algorithm/sorting/shell-sort/">[알고리즘] 쉘 정렬</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/rl/">RL</a> <a class="post-tag" href="/tags/algorithm/">Algorithm</a> <a class="post-tag" href="/tags/unity/">Unity</a> <a class="post-tag" href="/tags/function-approximation-rl/">Function Approximation RL</a> <a class="post-tag" href="/tags/sorting/">Sorting</a> <a class="post-tag" href="/tags/enemy/">Enemy</a> <a class="post-tag" href="/tags/game-development/">Game Development</a> <a class="post-tag" href="/tags/project/">Project</a> <a class="post-tag" href="/tags/attribute/">Attribute</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 pl-3 pr-3 pr-xl-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/reinforcement-learning/drl-methods/dtqn/"><div class="card-body"> <em class="small" data-ts="1715007600" data-df="ll" > May 7, 2024 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>DTQN: Deep Transformer Q-Networks for Partially Observable Reinforcement Learning</h3><div class="text-muted small"><p> 이 포스트에서는 요즘 가장 핫한 딥러닝 모델인 Transformer를 DQN에 적용한 Deep Transformer Q-Networks for Partially Observable Reinforcement Learning 논문에 대해 소개한다. 이 논문의 주 목적은 POMDP 상황에서 RNN 계열의 한계를 극복하고자 Transformer를 DQN에 적...</p></div></div></a></div><div class="card"> <a href="/reinforcement-learning/exploration-methods/exploration-by-random-network-distillation/"><div class="card-body"> <em class="small" data-ts="1663686000" data-df="ll" > Sep 21, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Exploration by Random Network Distillation</h3><div class="text-muted small"><p> 이 포스트에서는 exploration을 쉽고 효과적으로 수행할 수 있는 방법인 Exploration by Random Network Distillation 논문을 소개한다. Abstract exploration bonus는 observation feature의 예측 error임 고정 랜덤 초기화 신경망이 사용됨 extrinsic rew...</p></div></div></a></div><div class="card"> <a href="/reinforcement-learning/rl-fundamental/policy-gradient-methods/"><div class="card-body"> <em class="small" data-ts="1665586800" data-df="ll" > Oct 13, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Policy Gradient Methods</h3><div class="text-muted small"><p> 드디어 긴 장정 끝에 대망의 마지막 챕터로 왔다. 이번 포스트에서는 그 유명한 policy gradient method에 대해 소개하려고 한다. 길고 긴 여정이 드디어 끝났다. Introduction 지금까지 우리는 value-based 기반의 방법을 다뤘었다. policy는 추정된 action value에 기반해 action을 선택했다. 이번에는...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/wsl-setup/" class="btn btn-outline-primary" prompt="Older"><p>Windows Subsystem for Linux (WSL) Setup for Reinforcement Learning</p></a> <a href="/reinforcement-learning/drl-methods/dtqn/" class="btn btn-outline-primary" prompt="Newer"><p>DTQN: Deep Transformer Q-Networks for Partially Observable Reinforcement Learning</p></a></div></div></div><footer class="row pl-3 pr-3"><div class="col-12 d-flex justify-content-between align-items-center text-muted pl-0 pr-0"><div class="footer-left"><p class="mb-0"> © 2024 <a href="https://github.com/username">DevSlem</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/rl/">RL</a> <a class="post-tag" href="/tags/algorithm/">Algorithm</a> <a class="post-tag" href="/tags/unity/">Unity</a> <a class="post-tag" href="/tags/function-approximation-rl/">Function Approximation RL</a> <a class="post-tag" href="/tags/sorting/">Sorting</a> <a class="post-tag" href="/tags/enemy/">Enemy</a> <a class="post-tag" href="/tags/game-development/">Game Development</a> <a class="post-tag" href="/tags/project/">Project</a> <a class="post-tag" href="/tags/attribute/">Attribute</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a><div id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-animation="true" data-autohide="false"><div class="toast-header"> <button type="button" class="ml-2 ml-auto close" data-dismiss="toast" aria-label="Close"> <span aria-hidden="true">&times;</span> </button></div><div class="toast-body text-center pt-0"><p class="pl-2 pr-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></div><script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] }, svg: { scale: 1.1 } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script>
