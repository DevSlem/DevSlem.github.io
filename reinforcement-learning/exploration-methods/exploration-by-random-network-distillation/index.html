<!DOCTYPE html><html lang="en" data-mode="dark" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="Exploration by Random Network Distillation" /><meta property="og:locale" content="en" /><meta name="description" content="이 포스트에서는 exploration을 쉽고 효과적으로 수행할 수 있는 방법인 Exploration by Random Network Distillation 논문을 소개한다." /><meta property="og:description" content="이 포스트에서는 exploration을 쉽고 효과적으로 수행할 수 있는 방법인 Exploration by Random Network Distillation 논문을 소개한다." /><link rel="canonical" href="https://devslem.github.io/reinforcement-learning/exploration-methods/exploration-by-random-network-distillation/" /><meta property="og:url" content="https://devslem.github.io/reinforcement-learning/exploration-methods/exploration-by-random-network-distillation/" /><meta property="og:site_name" content="DevSlem Blog" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-09-21T00:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Exploration by Random Network Distillation" /><meta name="google-site-verification" content="SSJwUK1JSPZV3wjQS19f2bXfHYIupbYEFsisH4Nuns4" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-09-21T00:00:00+09:00","datePublished":"2022-09-21T00:00:00+09:00","description":"이 포스트에서는 exploration을 쉽고 효과적으로 수행할 수 있는 방법인 Exploration by Random Network Distillation 논문을 소개한다.","headline":"Exploration by Random Network Distillation","mainEntityOfPage":{"@type":"WebPage","@id":"https://devslem.github.io/reinforcement-learning/exploration-methods/exploration-by-random-network-distillation/"},"url":"https://devslem.github.io/reinforcement-learning/exploration-methods/exploration-by-random-network-distillation/"}</script><title>Exploration by Random Network Distillation | DevSlem Blog</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="DevSlem Blog"><meta name="application-name" content="DevSlem Blog"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> </a></div><div class="site-title mt-3"> <a href="/">DevSlem Blog</a></div><div class="site-subtitle font-italic">It's a blog where i write posts about AI, RL, Unity, etc.</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/DevSlem" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['devslem12','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper"><div id="topbar" class="container d-flex align-items-center justify-content-between h-100 pl-3 pr-3 pl-md-4 pr-md-4"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Exploration by Random Network Distillation</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper" class="d-flex justify-content-center"><div id="main" class="container pl-xl-4 pr-xl-4"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pr-xl-4"><div class="post pl-1 pr-1 pl-md-2 pr-md-2"><h1 data-toc-skip>Exploration by Random Network Distillation</h1><div class="post-meta text-muted"> <span> Posted <em class="" data-ts="1663686000" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Sep 21, 2022 </em> </span> <span> Updated <em class="" data-ts="1663718400" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Sep 21, 2022 </em> </span><div class="d-flex justify-content-between"> <span> By <em> <a href="https://github.com/username">DevSlem</a> </em> </span><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="1753 words"> <em>9 min</em> read</span></div></div></div><div class="post-content"><p>이 포스트에서는 exploration을 쉽고 효과적으로 수행할 수 있는 방법인 Exploration by Random Network Distillation 논문을 소개한다.</p><h2 id="abstract"><span class="mr-2">Abstract</span><a href="#abstract" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><ul><li>exploration bonus는 observation feature의 예측 error임<li>고정 랜덤 초기화 신경망이 사용됨<li>extrinsic reward와 intrinsic reward를 유연하게 결합<li>Montezuma’s Revenge에서 SOTA를 달성</ul><h2 id="introduction"><span class="mr-2">Introduction</span><a href="#introduction" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Reinforcement Learning (RL) method는 dense reward 환경에서는 잘 작동한다. dense reward 환경은 랜덤한 action sequence로부터 쉽게 reward를 찾을 수 있는 환경을 의미한다. 반대로 reward를 찾기 어려운 spare reward 환경에서는 학습에 종종 실패한다.</p><p>dense reward 환경을 모델링하는 것은 어려우며 부적절하다. dense reward 환경은 일종의 cheating 행위가 될 수도 있다. 따라서 spare reward 환경에서 적절히 환경을 탐색할 수 있는 방법이 필요하다.</p><p>이 논문에서 제시한 방법의 특징은 아래와 같다.</p><ul><li>간단한 구현<li>high-dimensional observation에서도 잘 작동<li>어떤 policy optimization 알고리즘에도 적용 가능<li>experience batch에 대한 신경망의 단일 forward pass만 요구하기 때문에 효율적</ul><p>신경망은 학습된 예제와 비슷한 것들에 대해 상당히 낮은 prediction error를 가지는 경향이 있다. 이러한 특징을 기반으로 새로운 experience의 novelty를 정량화함으로써 exploration bonus를 정의할 수 있다.</p><p>prediction error를 최대화하려는 agent는 대체로 확률적 transition을 추구하는 경향이 있다. 가장 대표적인 예시가 TV noise이다. TV noise는 계속 확률적으로 변하기 때문에 agent는 항상 새롭다고 느끼게 된다. 그 결과 쓸모없는 noise로 가득찬 TV 화면만 계속 쳐다보게 된다.</p><p>이 논문은 위와 같은 문제를 입력에 대한 결정론적 함수를 사용한 exploration bonus를 정의함으로써 해결한다. 이 결정론적 함수는 observation에 대한 고정 랜덤 초기화 신경망이다.</p><p>exploration bonus를 extrinsic reward와 결합하기 위해 PPO algorithm을 두 reward stream에 대한 두 개의 state value function을 사용하도록 변형한다. 이를 통해 각 reward에 대해 서로 다른 discount rate 적용이 가능해지며, episodic과 non-episodic return을 결합할 수 있게 해준다.</p><h2 id="exploration-bonus"><span class="mr-2">Exploration Bonus</span><a href="#exploration-bonus" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>환경으로부터 획득하는 reward를 $e_t$라고 하자. 이 때 $e_t$는 sparse하다. exploration bonus $i_t$는 agent가 spare reward 환경을 적절히 탐색하도록 돕는 역할을 한다. 최종적으로 reward function은 $r_t = e_t + i_t$로 정의된다. agent가 새로운 state를 탐색하도록 돕기 위해서는 당연히 자주 방문했던 state보다 새로운 state에서 $i_t$가 높아야 할 것이다.</p><h2 id="random-network-distillation"><span class="mr-2">Random Network Distillation</span><a href="#random-network-distillation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>이 논문에서 exploration bonus $i_t$를 어떻게 정의하는지 알아보자. 먼저, 이 논문에서는 2개의 network를 사용한다.</p><ul><li>target - 고정 랜덤 초기화 신경망 (fixed randomly initialized network)<li>predictor - observation을 예측</ul><p>target network $f : \mathcal{O} \rightarrow \mathbb{R}^k$는 observation을 임베딩한다. predictor network $\hat{f} : \mathcal{O} \rightarrow \mathbb{R}^k$는 expected MSE $\lVert \hat{f}(x;\theta) - f(x) \rVert^2$를 최소화하도록 학습된다. prediction error는 predictor가 학습했던 것과 비슷하지 않은 새로운 state에 대해서 높을 것이다. 이를 통해 exploration을 도울 수 있다.</p><h3 id="prediction-error"><span class="mr-2">Prediction Error</span><a href="#prediction-error" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>prediction error의 요인은 아래와 같다.</p><ol><li>Amount of training data - 비슷한 example을 적게 관찰 했을 때<li>Stochasticity - target function이 stochastic할 때<li>Model misspecification - 반드시 필요한 정보를 놓쳤거나 target function의 복잡성에 맞추기 어려울 때<li>Learning dynamics - target function을 가장 잘 근사하는 predictor를 찾는데 실패할 때</ol><p>위 첫번째 요소는 prediction error를 exploration bonus로 사용하게 하는 근본적 요인이다. 만약 예측 문제가 forward dynamics ($s_t$와 $a_t$를 통해 $s_{t+1}$을 예측하는 모델) 일 경우 두번째 요소는 ‘noisy-TV’ 문제를 일으킨다. deterministic한 transition보다 stochastic한 transition 예측이 어려운 건 너무나 당연하다. 또한 세번째 요소 역시 부적절하다.</p><p>RND는 target network가 deterministic하게 선택되고, predictor network의 model-class 내에 있기 때문에 두번째와 세번째 요소를 피할 수 있다.</p><h3 id="combining-intrinsic-and-extrinsic-returns"><span class="mr-2">Combining Intrinsic and Extrinsic Returns</span><a href="#combining-intrinsic-and-extrinsic-returns" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>이 논문에서는 intrinsic reward를 non-episodic return으로 좋다고 주장한다. 그 이유는 아래와 같다.</p><ul><li>agent의 intrinsic return은 미래에 발견할 수도 있는 모든 새로운 상태와 관련됨<li>episodic intrinsic reward는 정보 누락을 발생시킴<li>이 접근법은 인간이 게임을 탐색할 때와 유사함<li>episodic하다면 탐색 도중 game over 시 return이 0이 되기 때문에 risk 감수를 꺼리게 됨</ul><p>그러나 extrinsic reward에 대해서는 episodic return으로 다루는 것이 좋다고 주장한다. 그 이유는 만약 게임 시작 근처에서 reward를 발견할 경우, 그 reward를 계속 획득하기 위해 의도적으로 game over를 반복적으로 당하도록 악용할 것이다.</p><p>그렇다면 어떻게 intrinsic reward $i_t$의 non-episodic stream과 extrinsic reward $e_t$의 episodic stream을 적절히 결합할 수 있을까? 이 논문에서는 extrinsic return $R_E$와 intrinsic return $R_I$ 각각을 더한 $R = R_E + R_I$를 관찰한다. 즉, 각 return에 대한 value $V_E$와 $V_I$를 구한 뒤 value function $V = V_E + V_I$로 결합한다. 이러한 아이디어로 서로 다른 discount factor를 사용한 reward stream을 결합할 수도 있다.</p><p>episodic과 non-episodic reward stream의 결합 혹은 서로 다른 discount factor를 가진 reward stream의 결합을 하지 않더라도, value function에 대한 추가적인 supervisory signal의 존재 때문에 여전히 value function을 분리하는 것에 이점이 있다. 이는 특히 exploration bonus에 중요한데 extrinsic reward function은 stationary한 반면 intrinsic reward function은 non-stationary하기 때문이다.</p><h3 id="reward-and-observation-normalization"><span class="mr-2">Reward and Observation Normalization</span><a href="#reward-and-observation-normalization" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>prediction error를 exploration bonus로 사용할 때, 환경이 달라지거나 다른 순간에 있을 때 reward 크기가 너무 달라진다는 문제가 있다. reward를 일관된 크기로 유지하기 위해 intrinsic return의 표준편차 추정치로 나눔으로써 정규화를 수행한다.</p><p>observation도 정규화를 수행한다. observation을 정규화하지 않을 경우 임베딩의 분산이 극도로 낮아 입력에 대한 정보가 전혀 전달되지 않을 수 있다. 이를 위해 observation에 평균을 빼고 표준편차로 나눈 뒤 -5와 5 사이의 범위로 clipping한다. 정규화 파라미터 (평균, 표준편차)는 최적화 시작 전에 random agent로 약간의 step을 통해 초기화된다.</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/reinforcement-learning/'>Reinforcement Learning</a>, <a href='/categories/exploration-methods/'>Exploration Methods</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/rl/" class="post-tag no-text-decoration" >RL</a> <a href="/tags/ai/" class="post-tag no-text-decoration" >AI</a> <a href="/tags/drl/" class="post-tag no-text-decoration" >DRL</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Exploration+by+Random+Network+Distillation+-+DevSlem+Blog&url=https%3A%2F%2Fdevslem.github.io%2Freinforcement-learning%2Fexploration-methods%2Fexploration-by-random-network-distillation%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Exploration+by+Random+Network+Distillation+-+DevSlem+Blog&u=https%3A%2F%2Fdevslem.github.io%2Freinforcement-learning%2Fexploration-methods%2Fexploration-by-random-network-distillation%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fdevslem.github.io%2Freinforcement-learning%2Fexploration-methods%2Fexploration-by-random-network-distillation%2F&text=Exploration+by+Random+Network+Distillation+-+DevSlem+Blog" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div><script src="https://utteranc.es/client.js" repo="DevSlem/DevSlem.github.io" issue-term="pathname" label="comment" theme="github-dark" crossorigin="anonymous" async> </script></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/reinforcement-learning/drl-methods/dqn/">DQN: Deep Q-Networks</a><li><a href="/reinforcement-learning/drl-methods/dtqn/">DTQN: Deep Transformer Q-Networks for Partially Observable Reinforcement Learning</a><li><a href="/wsl-setup/">Windows Subsystem for Linux (WSL) Setup for Reinforcement Learning</a><li><a href="/algorithm/sorting/merge-sort/">[알고리즘] 합병 정렬</a><li><a href="/algorithm/sorting/shell-sort/">[알고리즘] 쉘 정렬</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/rl/">RL</a> <a class="post-tag" href="/tags/algorithm/">Algorithm</a> <a class="post-tag" href="/tags/unity/">Unity</a> <a class="post-tag" href="/tags/function-approximation-rl/">Function Approximation RL</a> <a class="post-tag" href="/tags/sorting/">Sorting</a> <a class="post-tag" href="/tags/enemy/">Enemy</a> <a class="post-tag" href="/tags/game-development/">Game Development</a> <a class="post-tag" href="/tags/project/">Project</a> <a class="post-tag" href="/tags/attribute/">Attribute</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 pl-3 pr-3 pr-xl-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/reinforcement-learning/exploration-methods/curiosity-driven-exploration/"><div class="card-body"> <em class="small" data-ts="1651849200" data-df="ll" > May 7, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Curiosity-driven Exploration by Self-supervised Prediction</h3><div class="text-muted small"><p> 이 포스트에서는 Reinforcement Learning을 효과적으로 수행할 수 있도록 intrinsic curiosity reward를 사용하는 아이디어를 제시한 Curiosity-driven Exploration by Self-supervised Prediction에 대해 소개한다. Introduction Curiosity-driven Expl...</p></div></div></a></div><div class="card"> <a href="/reinforcement-learning/rl-fundamental/policy-gradient-methods/"><div class="card-body"> <em class="small" data-ts="1665586800" data-df="ll" > Oct 13, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Policy Gradient Methods</h3><div class="text-muted small"><p> 드디어 긴 장정 끝에 대망의 마지막 챕터로 왔다. 이번 포스트에서는 그 유명한 policy gradient method에 대해 소개하려고 한다. 길고 긴 여정이 드디어 끝났다. Introduction 지금까지 우리는 value-based 기반의 방법을 다뤘었다. policy는 추정된 action value에 기반해 action을 선택했다. 이번에는...</p></div></div></a></div><div class="card"> <a href="/reinforcement-learning/rl-fundamental/multi-armed-bandits/"><div class="card-body"> <em class="small" data-ts="1653058800" data-df="ll" > May 21, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Multi-armed Bandits</h3><div class="text-muted small"><p> 이 포스트에서는 Reinforcement learning (RL)의 기본 내용인 Multi-armed Bandits 환경과 기본적인 아이디어들에 대해 알아본다. Reinforcement learning vs others Reinforcement learning (RL)과 다른 learning의 가장 큰 구별점은 사용하는 정보의 차이에 있다. 다른 ...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/reinforcement-learning/rl-fundamental/eligibility-traces/" class="btn btn-outline-primary" prompt="Older"><p>Eligibility Traces</p></a> <a href="/reinforcement-learning/rl-fundamental/policy-gradient-methods/" class="btn btn-outline-primary" prompt="Newer"><p>Policy Gradient Methods</p></a></div></div></div><footer class="row pl-3 pr-3"><div class="col-12 d-flex justify-content-between align-items-center text-muted pl-0 pr-0"><div class="footer-left"><p class="mb-0"> © 2024 <a href="https://github.com/username">DevSlem</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/rl/">RL</a> <a class="post-tag" href="/tags/algorithm/">Algorithm</a> <a class="post-tag" href="/tags/unity/">Unity</a> <a class="post-tag" href="/tags/function-approximation-rl/">Function Approximation RL</a> <a class="post-tag" href="/tags/sorting/">Sorting</a> <a class="post-tag" href="/tags/enemy/">Enemy</a> <a class="post-tag" href="/tags/game-development/">Game Development</a> <a class="post-tag" href="/tags/project/">Project</a> <a class="post-tag" href="/tags/attribute/">Attribute</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a><div id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-animation="true" data-autohide="false"><div class="toast-header"> <button type="button" class="ml-2 ml-auto close" data-dismiss="toast" aria-label="Close"> <span aria-hidden="true">&times;</span> </button></div><div class="toast-body text-center pt-0"><p class="pl-2 pr-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></div><script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] }, svg: { scale: 1.1 } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script>
