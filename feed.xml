<feed xmlns="http://www.w3.org/2005/Atom"> <id>https://devslem.github.io/</id><title>DevSlem Blog</title><subtitle>It's a blog where i write posts about AI, RL, Unity, etc.</subtitle> <updated>2024-05-09T17:27:47+09:00</updated> <author> <name>DevSlem</name> <uri>https://devslem.github.io/</uri> </author><link rel="self" type="application/atom+xml" href="https://devslem.github.io/feed.xml"/><link rel="alternate" type="text/html" hreflang="en" href="https://devslem.github.io/"/> <generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator> <rights> © 2024 DevSlem </rights> <icon>/assets/img/favicons/favicon.ico</icon> <logo>/assets/img/favicons/favicon-96x96.png</logo> <entry><title>DTQN: Deep Transformer Q-Networks for Partially Observable Reinforcement Learning</title><link href="https://devslem.github.io/reinforcement-learning/drl-methods/dtqn/" rel="alternate" type="text/html" title="DTQN: Deep Transformer Q-Networks for Partially Observable Reinforcement Learning" /><published>2024-05-07T00:00:00+09:00</published> <updated>2024-05-07T00:00:00+09:00</updated> <id>https://devslem.github.io/reinforcement-learning/drl-methods/dtqn/</id> <content src="https://devslem.github.io/reinforcement-learning/drl-methods/dtqn/" /> <author> <name>DevSlem</name> </author> <category term="Reinforcement Learning" /> <category term="DRL Methods" /> <summary> 이 포스트에서는 요즘 가장 핫한 딥러닝 모델인 Transformer를 DQN에 적용한 Deep Transformer Q-Networks for Partially Observable Reinforcement Learning 논문에 대해 소개한다. 이 논문의 주 목적은 POMDP 상황에서 RNN 계열의 한계를 극복하고자 Transformer를 DQN에 적용한 것이다. Partial Observability 강화학습에서 agent가 현재 state에 대한 모든 정보를 알고 있는 경우, 이를 fully observable MDP라고 한다. 그러나, 대부분의 실제 환경에서는 agent가 현재 state에 대한 모든 정보를 알 수 없는 경우가 많다. agent는 현재 state로부터 관찰된 일부 정보만을 가지... </summary> </entry> <entry><title>DQN: Deep Q-Networks</title><link href="https://devslem.github.io/reinforcement-learning/drl-methods/dqn/" rel="alternate" type="text/html" title="DQN: Deep Q-Networks" /><published>2024-05-07T00:00:00+09:00</published> <updated>2024-05-07T00:00:00+09:00</updated> <id>https://devslem.github.io/reinforcement-learning/drl-methods/dqn/</id> <content src="https://devslem.github.io/reinforcement-learning/drl-methods/dqn/" /> <author> <name>DevSlem</name> </author> <category term="Reinforcement Learning" /> <category term="DRL Methods" /> <summary> 이 포스트에서는 deep RL의 기본이자 시대를 열어준 DQN(Deep Q-Networks)을 도입한 Playing Atari with Deep Reinforcement Learning 논문에 대해 소개한다. Introduction DQN 이전에는, 강화학습에서 사용되는 Q-learning과 같은 tabular method는 state와 action space가 작은 경우에만 사용할 수 있었다. 몰론, DQN 이전에도 function approximation method가 있긴 했지만 한계가 뚜렸했다. 특히, 이미지와 같은 high-dimensional 입력으로부터 직접적으로 Q-value를 추정하는 것은 어려운 문제였다. 그러나 딥러닝의 발전으로 인해 high-dimensional raw data... </summary> </entry> <entry><title>Windows Subsystem for Linux (WSL) Setup for Reinforcement Learning</title><link href="https://devslem.github.io/wsl-setup/" rel="alternate" type="text/html" title="Windows Subsystem for Linux (WSL) Setup for Reinforcement Learning" /><published>2023-02-12T00:00:00+09:00</published> <updated>2023-02-12T00:00:00+09:00</updated> <id>https://devslem.github.io/wsl-setup/</id> <content src="https://devslem.github.io/wsl-setup/" /> <author> <name>DevSlem</name> </author> <summary> 이 포스트에서는 Reinforcement Learning (RL) 작업을 위한 Windows Subsystem for Linux (WSL) 설치 및 Setup에 대해 소개한다. WSL에 대한 보다 자세한 내용은 MS 공식 문서를 참조하기 바란다. 이 포스트는 추후 더 자세한 내용과 함께 업데이트 될 예정입니다. WSL PowerShell을 관리자 권한으로 실행 후 아래 커맨드를 입력한다: wsl --install 설치가 완료되면 컴퓨터를 재시작한다. 위 커맨드는 WSL이 전혀 설치되지 않은 경우에만 작동한다. wsl --install 실행 시 도움말 텍스트를 보는 경우 wsl --list --online을 실행해 사용 가능한 배포판 목록을 확인하고 wsl --install -d... </summary> </entry> <entry><title>[알고리즘] 합병 정렬</title><link href="https://devslem.github.io/algorithm/sorting/merge-sort/" rel="alternate" type="text/html" title="[알고리즘] 합병 정렬" /><published>2022-11-02T00:00:00+09:00</published> <updated>2022-11-02T00:00:00+09:00</updated> <id>https://devslem.github.io/algorithm/sorting/merge-sort/</id> <content src="https://devslem.github.io/algorithm/sorting/merge-sort/" /> <author> <name>DevSlem</name> </author> <category term="Algorithm" /> <category term="Sorting" /> <summary> 합병 정렬은 효율적이고 일반적인 목적으로 사용되는 divide-and-conquer 기반의 정렬 알고리즘이다. 합병 정렬의 특징은 아래와 같다. 비교 기반 non-in-place 시간 복잡도: $O(n \log n)$ stable Key Idea 합병 정렬의 핵심 아이디어는 아래와 같다. 정렬되지 않은 $n$개의 서브 리스트로 분할한다. 각 서브 리스트는 1개의 원소를 가진다. 서브 리스트를 합쳐 새로운 정렬된 서브 리스트를 만든다. 이 과정은 하나의 서브 리스트가 될 때까지 반복한다. 위와 같이 비교적 간단한 아이디어임을 알 수 있다. 합병 정렬은 top-down과 bottom-up 방식 모두 존재하는데 여기서는 top-down 방식에 대해 알아볼 것이다. 먼저... </summary> </entry> <entry><title>[알고리즘] 쉘 정렬</title><link href="https://devslem.github.io/algorithm/sorting/shell-sort/" rel="alternate" type="text/html" title="[알고리즘] 쉘 정렬" /><published>2022-10-31T00:00:00+09:00</published> <updated>2022-10-31T00:00:00+09:00</updated> <id>https://devslem.github.io/algorithm/sorting/shell-sort/</id> <content src="https://devslem.github.io/algorithm/sorting/shell-sort/" /> <author> <name>DevSlem</name> </author> <category term="Algorithm" /> <category term="Sorting" /> <summary> 쉘 정렬은 삽입 정렬을 최적화한 알고리즘으로 $h$ 간격으로 부분적으로 정렬한다. 특징은 아래와 같다. 비교 기반 in-place 시간 복잡도: 간격 $h$에 따라 다름 unstable Key Idea 기존 삽입 정렬에서 왼쪽은 정렬된 리스트, 오른쪽은 정렬이 안된 리스트로 구분했었다. 따라서 삽입 정렬은 어느 정도 정렬이 되어 있는 리스트에 대해 강력한 성능을 낼 수 있다. 이 특징을 확장한 것이 쉘 정렬이다. 쉘 정렬은 $h$ 간격으로 원소들을 삽입 정렬을 사용해 정렬한다. $h$는 매우 큰 수에서 점점 작아지다가 최종적으로는 1로 끝난다. 중요한 점은 반드시 $h$는 1로 끝나야 한다. $h$가 작아질 수록 부분적으로 정렬된 원소들이 많아지기 때문에 삽입 정렬의 성능은 향... </summary> </entry> </feed>
