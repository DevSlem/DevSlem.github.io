[ { "title": "DTQN: Deep Transformer Q-Networks for Partially Observable Reinforcement Learning", "url": "/reinforcement-learning/drl-methods/dtqn/", "categories": "Reinforcement Learning, DRL Methods", "tags": "RL, AI", "date": "2024-05-07 00:00:00 +0900", "snippet": "이 포스트에서는 요즘 가장 핫한 딥러닝 모델인 Transformer를 DQN에 적용한 Deep Transformer Q-Networks for Partially Observable Reinforcement Learning 논문에 대해 소개한다. 이 논문의 주 목적은 POMDP 상황에서 RNN 계열의 한계를 극복하고자 Transformer를 DQN에 적용한 것이다.Partial Observability강화학습에서 agent가 현재 state에 대한 모든 정보를 알고 있는 경우, 이를 fully observable MDP라고 한다. 그러나, 대부분의 실제 환경에서는 agent가 현재 state에 대한 모든 정보를 알 수 없는 경우가 많다. agent는 현재 state로부터 관찰된 일부 정보만을 가지고 의사결정을 해야하며, 이러한 상황을 partially observable markov decision process (POMDP)라고 한다. 아래는 POMDP environment인 Gym-Gridverse이다:Fig 1. POMDP: Gym-Gridverse.(Image source: Deep Transformer Q-Networks for Partially Observable Reinforcement Learning.)위 그림에서, 위쪽 행은 전체 state를 나타내는 그림이고, 아래쪽 행은 agent가 관찰할 수 있는 부분만을 나타낸 그림이다. agent는 X표시되어있는 beacon의 색깔과 동일한 깃발에 도달해야한다. 일반적인 MDP는 agent가 현재 state만을 가지고 action을 선택하는 markov property를 가정한다. 그러나, 위와 같은 환경에서는 agent가 현재 state에 대한 정보를 완전히 알지 못하기 때문에 학습에 어려움을 겪으며, 종종 실패한다. 따라서, 이러한 문제를 다루기 위한 방법이 필요하다.Limitation of RNNpartial observability를 다루기 위해서는 RL agent는 이전의 observation들을 기억할 필요가 있다. RL에서는 memory component를 추가해 agent가 이전의 observation을 참조할 수 있도록 한다. 이러한 memory component로 recurrent neural network (RNN)이 많이 사용되었다. LSTM이나 GRU와 같은 RNN 계열의 방법들은 observation 혹은 action-observation history에 대한 sequence를 sequential하게 처리함으로써 POMDP 문제를 해결할 수 있었다. 그러나, RNN은 long-term dependency에 취약하다. 이는 RNN이 긴 sequence에 대한 정보를 잘 학습하지 못한다는 것을 의미한다. 이로 인해 RNN을 사용했다고 할 지라도, POMDP 문제에서 종종 학습에 실패하는 경우가 발생한다.이는 RL만의 문제가 아니다. NLP 분야에서도 RNN의 한계가 논의되었고, 이를 극복하기 위해 Transformer가 제안되었다. Transformer는 self-attention mechanism을 사용하여 long-term dependency를 잘 학습할 수 있다. 따라서, Transformer를 RL에 적용하여 RNN의 한계를 극복하고자 한 것이 이 논문의 주 목적이다.Related Work몰론, RL에 Transformer를 적용한 연구는 이 논문이 처음이 아니다. 대표적으로 Decision Transformer 등이 있다. 그러나, 이전 연구들은 주로 offline RL setting이거나 supervised learning setting이었다. 반면, 이 논문에서 제안한 방법은 완전히 online RL setting이다. 나는 이 논문에서는 DQN을 사용했지만 다른 보다 강력한 RL 알고리즘에도 쉽게 적용할 수 있을 걸로 본다.Deep Transformer Q-Networks이제 본격적으로 이 논문에서 제안한 Deep Transformer Q-Networks (DTQN)에 대해 알아보자. DTQN은 DQN에 Transformer를 활용한 모델로 아래는 DTQN의 핵심 요소이다: Observation Embedding: 현재 observation을 포함한 observation history를 embedding한다. 이는 Transformer의 input으로 사용된다. Transformer Decoder: Transformer의 decoder를 사용하여 observation history sequence를 처리한다. 이는 각 action의 Q-value를 추정하는 데 사용된다. Q-value Prediction: 예측된 Q-value를 사용하여 action을 선택하고, TD error를 계산하여 네트워크를 학습한다.아래는 DTQN의 architecture이다:Fig 2. DTQN Overall Architecture.(Image source: Deep Transformer Q-Networks for Partially Observable Reinforcement Learning.)이제 구체적으로 각 요소에 대해 알아보자.Observation Embedding and Positional Encodingsagent의 최근 $k$개의 observation history $h_{t:t+k}$에서 각각의 observation은 observation embedding layer를 통해 transformer의 dimensionality로 linearly projected된다. 이후, learned positional encoding을 각 observation에 더해준다.NLP task에서 positional encoding은 Transformer에 흔히 사용되는 방법으로, 주로 sinusoidal positional encoding이 사용된다. 이는 문장의 각 token의 위치에 대한 정보를 제공함으로써 Transformer가 sequence의 순서를 학습할 수 있도록 도와준다. 그러나 RL에서는 observation history에서 각 observation의 순서가 중요할 수도 있고 아닐수도 있다. 이는 task와 environment에 따라 다르다. 따라서, 이 논문에서는 positional encoding을 학습 가능한 parameter로 설정하여 observation history의 순서에 대한 정보를 학습하도록 한다. 아래는 각 domain에 따라 학습된 positional encoding과 sinusoidal positional encoding을 비교한 결과이다:Fig 3. Positional Encoding Comparisons.(Image source: Deep Transformer Q-Networks for Partially Observable Reinforcement Learning.)Transformer DecoderTransformer는 sequence data를 처리하는데 효과적임이 이미 널리 알려져있다. 특히, attention mechanism은 가장 중요한 token들에 더 많은 가중치 혹은 attention을 줌으로써 효과적으로 학습할 수 있다. Transformer는 encoder-decoder 구조로 되어있지만, 최근에는 주로 encoder (BERT)나 decoder (GPT)를 단독으로 사용한다. 두 방법의 주요한 차이점은 decoder는 attention layer에 causal masking을 적용하는 것이다. 즉, $i$번째 token은 $i$번째 이전의 token에만 attention을 줄 수 있다. 이는 decoder가 다음 token을 예측할 때, 이전 token들만을 참조할 수 있도록 한다. DTQN은 decoder만을 사용한다.구체적으로 DTQN에서는 GPT와 같이 두개의 submodule로 구성된다: masked multi-headed self-attention과 position-wise feedforward network. 구체적으로는 다음 스텝을 따른다: 앞서 embedding된 observation history를 weight matrix $W^Q$에 의해 query $Q$, $W^K$에 의해 key $K$, $W^V$에 의해 value $V$로 projection한다. $Q$, $K$, $V$는 masked multi-headed self-attention을 거친 후, observation embedding과 combine된 후, layer normalization을 거친다. 이후, position-wise feedforward network를 거친 후, combine, layer normalization을 거친다. 이러한 과정을 $N$개의 Transformer block을 통해 $N$번 반복한다. 마지막 embedding은 action space로 projection되어 각 action에 대한 Q-value를 추정한다.사실 이는 전형적인 Transformer의 구조이다.Q-value Prediction이후의 과정은 DQN과 거의 유사하다. 먼저, DQN은 Mean Squared Bellman Error를 minimize하도록 학습된다:\\[L(\\theta) = \\mathbb{E}_{(s,a,r,s') \\sim \\mathcal{D}} [ ( r + \\gamma \\max_{a' \\in \\mathcal{A}} Q(s',a';\\theta') - Q(s,a;\\theta) )^2 ]\\]experience tuple $(s,a,r,s’)$는 replay buffer $\\mathcal{D}$로부터 uniformly하게 샘플링된다. TD target $r + \\max_{a’ \\in \\mathcal{A}} Q(s’,a’;\\theta’)$는 $\\theta’$으로 parameterized된 target network를 사용하여 계산된다. 이는 $\\theta$에 의해 parameterized된 Q-network보다 지연되어 업데이트되기 때문에 학습을 안정화시킨다.그러나 앞서 언급했듯이, partially observable 도메인에서는 네트워크의 입력을 state에서 observation으로 단순히 바꾸는 것만으로는 학습이 어렵다. 따라서, DTQN은 observation history를 Transformer로 처리하여 Q-value를 추정한다.이때, DTQN은 observation history의 각 time step에 대한 모든 Q-value를 추정한다. agent가 action을 결정할 때에는 현재 time step $t$에 대한 Q-value만을 사용한다. 즉, 다시 말해 history의 마지막 time step에 대한 Q-value만을 사용한다. 그러나 학습할 때는 history 내의 모든 time step에 대한 Q-value를 사용한다. 몰론 마지막 time step에 대한 Q-value만을 사용하는 것이 직관적일 수 있지만 이는 매우 큰 낭비이다. 이는 실제로 모든 time step에 대한 Q-value를 사용했을 때 학습 성능이 크게 향상되었다.Algorithm아래는 DTQN의 알고리즘이다:Fig 4. DTQN Algorithm.(Image source: Deep Transformer Q-Networks for Partially Observable Reinforcement Learning.)위 알고리즘에서 Q-value를 추정할 때 $s$가 아닌 $h_{t:t+i}$가 입력됨을 확인할 수 있다. 또한, loss를 계산할 때 casually-masked self-attention mechanism을 사용하기 때문에, 알고리즘에 묘사된 for loop는 실제로 one forward pass로 처리된다.Experiments이 논문에서는 DTQN을 다양한 POMDP 환경에서 평가하였으며, 다양한 ablation study를 진행하였다. 보다 자세한 결과와 해석은 논문을 직접 참고하기 바란다. 아래는 DTQN을 다른 방법과 비교한 결과이다 (파란색: DTQN, 주황색: DRQN, 갈색: DQN):Fig 5. DTQN against Baselines.(Image source: Deep Transformer Q-Networks for Partially Observable Reinforcement Learning.)위 실험결과를 보면 알 수 있듯이 naive한 DQN은 POMDP 문제에서 학습에 실패하거나 어려움을 겪음을 알 수 있다. 이는 POMDP 문제를 해결하기 위해서는, memory component가 필요함을 보여준다. DRQN은 LSTM을 통해 DQN에 memory component를 추가한 것이다. DRQN은 분명 좋은 성능을 보이지만, 학습이 느리고 불안정함을 알 수 있다. 반면, DTQN은 빠른 학습 속도와 좋은 성능을 보인다.아래는 ablation study 결과에 대한 표이다:Fig 6. Ablations.(Image source: Deep Transformer Q-Networks for Partially Observable Reinforcement Learning.)먼저, DTQN에 learned position encoding을 사용했을 때 positional encoding을 사용하지 않았을 때보다 성능이 향상되었음을 알 수 있으며, sinusoidal positional encoding을 사용했을 때보다 약간 개선됨을 알 수 있다. 또한, 마지막 time step에 대한 Q-value만을 사용해 학습했을 때 성능이 매우 떨어짐을 확인할 수 있다.Transformer의 이점 중 하나는 self-attention weight를 시각화 할 수 있는 것이다. 직관적으로, self-attention mechanism은 agent가 task를 해결하는데 가장 유용한 정보를 제공하는 observation에 더 많이 우선순위를 둘 것이다. 아래 그림은 Gridverse 환경에 대해 self-attention weight를 시각화한 결과이다:Fig 7. Atttention Visualization.(Image source: Deep Transformer Q-Networks for Partially Observable Reinforcement Learning.)초록색 beacon을 포함하는 observation이 모든 future observations에 의해 attention을 받는 것을 확인할 수 있다. 이는 DTQN이 task를 해결하는데 어느 observation이 중요한지를 적절히 학습하고 있음을 나타낸다. agent가 초록색 flag를 볼 때 (위 그림에서 왼쪽), agent는 초록색 beacon에 attention을 주고, 올바른 flag임을 확실시 할 수 있다.Summary아래는 DTQN의 요약이다: DQN에 Transformer를 활용한 모델로, POMDP 문제를 해결하기 위해 제안되었다. fully online RL setting에서 사용할 수 있다. observation history를 Transformer decoder로 처리하여 Q-value를 추정한다. learned positional encoding을 사용한다. observation history의 추정된 모든 Q-value를 사용하여 학습한다. DQN과 DRQN에 비해 빠른 학습 속도와 좋은 성능을 보인다.다만, 1가지 우려점이 존재한다. 원래, POMDP setting에서 observation history는 initial time step부터의 full history이다. 그러나, DTQN에서는 observation history가 context length $k$에 의해 truncated된다. 실제 실험에서 $k=50$으로 설정되었다. RNN은 이론상 full history를 처리한다. 그러나, DTQN은 truncated history이다. 만약, environment가 복잡하고 episode length가 매우 길때는 어떨지 궁금하다.References[1] Esslinger, Kevin, Robert Platt, and Christopher Amato. “Deep transformer q-networks for partially observable reinforcement learning.” arXiv preprint arXiv:2206.01078 (2022)." }, { "title": "DQN: Deep Q-Networks", "url": "/reinforcement-learning/drl-methods/dqn/", "categories": "Reinforcement Learning, DRL Methods", "tags": "RL, AI", "date": "2024-05-07 00:00:00 +0900", "snippet": "이 포스트에서는 deep RL의 기본이자 시대를 열어준 DQN(Deep Q-Networks)을 도입한 Playing Atari with Deep Reinforcement Learning 논문에 대해 소개한다.IntroductionDQN 이전에는, 강화학습에서 사용되는 Q-learning과 같은 tabular method는 state와 action space가 작은 경우에만 사용할 수 있었다. 몰론, DQN 이전에도 function approximation method가 있긴 했지만 한계가 뚜렸했다. 특히, 이미지와 같은 high-dimensional 입력으로부터 직접적으로 Q-value를 추정하는 것은 어려운 문제였다. 그러나 딥러닝의 발전으로 인해 high-dimensional raw data로부터 high-level feature를 추출하는 것이 가능해졌다. DQN은 이러한 딥러닝의 강력한 능력을 강화학습에 적용하고자 한 최초의 사례이다. DQN이 기존 tabular Q-learning과의 차이점은 action-value를 추정하기 위해 deep neural network를 사용한다는 것이다:Fig 1. Tabular Q-learning vs DQN.(Image source: Ankit Choudhary.)그러나, 딥러닝을 강화학습에 적용할 때 몇가지 문제가 있다: Delayed Rewards: 대부분의 딥러닝 문제들은 많은 양의 hand-labeled data를 사용하여 학습한다. 그러나 강화학습에서는 agent가 선택한 action에 대해 즉각적인 reward를 받지 않을 수 있으며, 이러한 reward는 sparse하거나 noisy하다. High Correlation: 연속적인 상태들은 서로 상관관계가 높을 수 있다. 이는 학습 데이터간의 i.i.d 가정을 깨뜨릴 수 있다. Non-stationary Distribution: 강화학습에서는 agent가 학습을 진행함에 따라 얻게 되는 data의 distribution이 변할 수 있다. 이는 fixed underlying distribution을 가정하는 딥러닝에서 문제가 될 수 있다.DQN은 이러한 문제들을 해결하기 위해 다음과 같은 기법들을 사용한다: CNN(Convolutional Neural Networks): 이미지와 같은 high-dimensional raw data로부터 feature를 추출하기 위해 CNN을 사용한다. Experience Replay: agent가 경험한 데이터를 저장하고 이를 무작위로 뽑아서 학습한다. 이는 데이터간의 상관관계를 줄이고, 학습 데이터의 분포를 고정시킨다. Fixed Q-targets: target Q-value를 계산할 때 target network를 사용하여 target Q-value를 계산한다. 이는 학습 중 target Q-value를 고정시킴으로써 학습을 안정화시킨다.DQN이 도입한 이러한 기법들은 후에 나오는 발전된 알고리즘들에도 많은 영향을 끼쳤다.ObjectiveDQN은 action-value를 추정하는 Q-function을 학습한다. 구체적으로, action-value를 추정하는 Q-network $Q(s,a;\\theta)$를 정의하고, 이를 학습하여 optimal action-value function $Q^*(s,a)$에 근접시킨다. 이때, $\\theta$는 neural network의 parameter이다. 만약, 각 iteration $i$마다 Q-network에 대한 target value인 $y_i$가 존재한다면, Q-network를 학습하기 위한 loss function $L_i(\\theta_i)$를 아래와 같이 정의할 수 있다:\\[L_i(\\theta_i) = \\mathbb{E}_{s,a \\sim \\rho(\\cdot)} \\left[ \\left( y_i - Q(s,a;\\theta_i) \\right)^2 \\right]\\]그러나, 일반적으로 강화학습에서는 label이 존재하지 않기 때문에 target value인 $y_i$를 구성하기 어렵다. 이를 해결하기 위해 DQN은 target value를 temporal difference(TD) target으로 정의한다. TD target은 아래와 같이 정의된다:\\[y_i = \\mathbb{E}_{s' \\sim \\mathcal{E}} \\left[ r + \\gamma \\max_{a'} Q(s',a';\\theta_{i-1}) \\right \\vert s,a]\\]TD target의 주요한 의미는, 현재 state $s$에서 action $a$를 선택했을 때 얻게 되는 실제 데이터 reward $r$과 다음 state $s’$에서 최적의 action을 선택했을 때 얻게 되는 action-value의 추정치를 고려하여 현재 state에서의 action-value를 추정한다는 것이다. 이러한 loss function의 gradient는 아래와 같다:\\[\\nabla_{\\theta_i} L_i(\\theta_i) = \\mathbb{E}_{s,a \\sim \\rho(\\cdot); s' \\sim \\mathcal{E}} \\left[ \\left( r + \\gamma \\max_{a'} Q(s',a';\\theta_{i-1}) - Q(s,a;\\theta_i) \\right) \\nabla_{\\theta_i} Q(s,a;\\theta_i) \\right]\\]몰론, 실제로는 위처럼 완전한 expectation을 계산하는 것이 아닌, stochastic gradient descent(SGD)를 사용하여 mini-batch를 통해 gradient를 추정한다.이러한 알고리즘에는 주요한 특징이 있다: Model-free: DQN은 model-free 알고리즘이다. 즉, environment의 dynamics에 대한 정보를 알 필요가 없으며, sample로부터 직접적으로 학습한다. Off-policy: DQN은 off-policy 알고리즘이다. experience samples는 behavior policy (e.g., epsilon-greedy policy)로부터 수집되지만, agent는 target policy인 greedy policy $a = \\arg \\max_a Q(s,a;\\theta)$를 학습한다.MethodDQN의 핵심 component들은 아래와 같다: Q-Network: Q-network는 state $s$로부터 action들에 대해 Q-value를 추정하는 neural network이다. Target Network: Target network는 target Q-value를 계산하기 위한 neural network이다. Target network는 fixed interval로 Q-network의 parameter를 복사하여 업데이트한다. 이는 학습 중 target Q-value를 고정시킴으로써 학습을 안정화시킨다. Experience Replay: Experience replay는 experience tuple $(s,a,r,s’)$를 저장하고 이를 무작위로 뽑아서 학습하는 기법이다. 이는 데이터간의 상관관계를 줄인다.아래는 DQN의 architecture이다:Fig 2. DQN Architecture.(Image source: 이것저것 테크블로그.)agent는 매 time step $t$마다 environment 상호작용하면서 experience sample을 획득한 후 이를 replay buffer에 저장한다. 이후, replay buffer로부터 mini-batch를 뽑아서 Q-value를 학습한다. 이러한 과정을 통해 DQN은 optimal Q-value function에 근접하도록 학습된다. 아래는 DQN의 알고리즘이다:Fig 3. DQN Algorithm.(Image source: Playing Atari with Deep Reinforcement Learning.)몰론, 위 알고리즘에서는 따로 target network에 대해 기술되어있지 않지만, TD target $y_j$를 계산할 때 target network를 사용한다. 또한, target network는 fixed interval로 Q-network의 parameter를 복사하여 업데이트한다.LimitationsDQN은 분명 Atari 게임과 같은 high-dimensional state space이며 다루는데 효과적이었다. 그러나 DQN에는 몇가지 한계점이 있다: High-dimensional Action Space: DQN은 high-dimensional discrete action space나 continuous action space에는 적용하기 어렵다. 이는 Q-value를 추정하기 위해 discrete action space를 가정하기 때문이며, 모든 action에 대해 Q-value를 추정하기 때문이다. Off-policy: DQN은 off-policy method이기 때문에 sample-efficient하지만, 학습이 느리고 불안정할 수 있다.후에 나오는 알고리즘들은 이러한 한계점을 극복하기 위해 다양한 기법들을 사용한다.Summary deep neural network를 사용해 high-dimensional state space를 효과적으로 다룰 수 있다. experience replay와 target network를 사용하여 학습을 안정화시킨다. model-free, off-policy method이다. high-dimensional action space나 continuous action space에는 적용하기 어렵다.References[1] Mnih, Volodymyr, et al. “Playing atari with deep reinforcement learning.” arXiv preprint arXiv:1312.5602 (2013)." }, { "title": "Windows Subsystem for Linux (WSL) Setup for Reinforcement Learning", "url": "/wsl-setup/", "categories": "", "tags": "Linux, WSL", "date": "2023-02-12 00:00:00 +0900", "snippet": "이 포스트에서는 Reinforcement Learning (RL) 작업을 위한 Windows Subsystem for Linux (WSL) 설치 및 Setup에 대해 소개한다. WSL에 대한 보다 자세한 내용은 MS 공식 문서를 참조하기 바란다. 이 포스트는 추후 더 자세한 내용과 함께 업데이트 될 예정입니다.WSLPowerShell을 관리자 권한으로 실행 후 아래 커맨드를 입력한다:wsl --install설치가 완료되면 컴퓨터를 재시작한다. 위 커맨드는 WSL이 전혀 설치되지 않은 경우에만 작동한다. wsl --install 실행 시 도움말 텍스트를 보는 경우 wsl --list --online을 실행해 사용 가능한 배포판 목록을 확인하고 wsl --install -d &lt;DistroName&gt;을 실행하여 배포판을 설치한다. (e.g., wsl --install -d Ubuntu)그 외 참고할 문서: WSL 개발 환경 설정 Linux용 Windows 하위 시스템에서 Visual Studio Code 사용 시작Zsh &amp; Oh My Zsh윈도우 10/11 환경에서 리눅스(wsl2)의 zsh 꾸미기 포스트를 따라 커스텀 터미널 환경을 구축한다.NVIDIA CUDACUDA on Windows Subsystem for Linux (WSL)에서 CUDA를 설치할 수 있다. NVIDIA 최신 드라이버가 설치되어 있지 않다면 Get GUDA Driver를 클릭해 설치한다. NVIDIA 드라이버는 반드시 Windows 용으로 설치한다! WSL 환경에 따로 드라이버를 절대 설치하지 않는다.Docs에 따라 wsl에 CUDA를 설치한다.WSL에 아래 커맨드를 입력해 CUDA가 정상적으로 설치 되었는지 확인하다:nvidia-smi Windows Terminal을 관리자 권한으로 실행 시 NVIDIA 드라이버를 load하는데 실패할 수 있다.~/.zshrc 파일에 아래와 같이 환경 변수를 설정한다 (&lt;X.X&gt;에 설치한 CUDA 버전을 입력, e.g., 11.6):export PATH=/usr/local/cuda-&lt;X.X&gt;/bin:$PATHexport LD_LIBRARY_PATH=/usr/local/cuda-&lt;X.X&gt;/lib64:$LD_LIBRARY_PATH아래 커맨드를 입력해 CUDA가 설치됐는지 확인한다:nvcc --versionAnaconda &amp; PyTorch여기서는 Anaconda의 경량화 버전인 Miniconda를 사용한다. 아래 커맨드를 입력해 설치한다 (설치 전 다운로드 파일을 모와 놓는 디렉토로 이동하는 것을 권장한다):wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.shbash Miniconda3-latest-Linux-x86_64.sh아래 커맨드를 입력해 테스트 Conda 환경을 만든다:conda create -n torch-test python=3.7 -yconda activate torch-testPyTorch를 설치한다 (PyTorch 1.11.0, CUDA 11.3):conda install pytorch==1.11.0 torchvision==0.12.0 torchaudio==0.11.0 cudatoolkit=11.3 -c pytorch아래 커맨드를 입력 해 Python Interactive 모드로 진입 한다:python아래 코드를 한 줄씩 입력 해 PyTorch에서 CUDA 사용 가능 여부를 확인한다:&gt;&gt;&gt; import torch&gt;&gt;&gt; torch.cuda.is_available()OpenAI GymWSL에서 OpenAI Gym 환경 실행 자체는 문제가 되지 않으나 렌더링 시 libGL과 X Error 등의 에러가 발생할 수 있다. 이 경우 먼저 Windows에 Xming X Server for Windows을 설치한다.WSL에 아래 커맨드를 입력 해 다음 패키지들을 추가로 설치한다:sudo apt-get install x11-appssudo apt-get install gnome-calculator~/.zshrc 파일에 아래와 같이 환경 변수를 설정한다:export LIBGL_ALWAYS_INDIRECT=1export DISPLAY=$(cat /etc/resolv.conf | grep nameserver | awk '{print $2}'):0Windows에서 현재 Xming이 실행 중이라면 종료 후 XLaunch 앱을 실행 후 아래 과정에 따라 Xming을 재실행한다: 다음 클릭 (Default: Multiple windows) 다음 클릭 (Default: Start no client) No Access Control 체크 후 다음 클릭 마침 클릭아래 커맨드를 입력 해 아래 그림과 같이 앱이 실행되는지 확인한다:xeyes이제 OpenAI Gym 환경이 정상적으로 렌더링 되는지 확인한다 (Gym 0.25.2):import gymenv = gym.make(\"CartPole-v1\", new_step_api=True, render_mode=\"human\")obs = env.reset()for _ in range(100): action = env.action_space.sample() next_obs, _, terminated, truncated, _ = env.step(action) if terminated | truncated: obs = env.reset() env.close()" }, { "title": "[알고리즘] 합병 정렬", "url": "/algorithm/sorting/merge-sort/", "categories": "Algorithm, Sorting", "tags": "Algorithm, Sorting", "date": "2022-11-02 00:00:00 +0900", "snippet": "합병 정렬은 효율적이고 일반적인 목적으로 사용되는 divide-and-conquer 기반의 정렬 알고리즘이다. 합병 정렬의 특징은 아래와 같다. 비교 기반 non-in-place 시간 복잡도: $O(n \\log n)$ stableKey Idea합병 정렬의 핵심 아이디어는 아래와 같다. 정렬되지 않은 $n$개의 서브 리스트로 분할한다. 각 서브 리스트는 1개의 원소를 가진다. 서브 리스트를 합쳐 새로운 정렬된 서브 리스트를 만든다. 이 과정은 하나의 서브 리스트가 될 때까지 반복한다.위와 같이 비교적 간단한 아이디어임을 알 수 있다. 합병 정렬은 top-down과 bottom-up 방식 모두 존재하는데 여기서는 top-down 방식에 대해 알아볼 것이다.먼저, 어떤 입력 리스트가 있다고 할 때 이를 재귀적으로 절반씩 서브 리스트로 분할한다. 서브 리스트의 원소가 1개가 될 떄까지 반복한다. 이제 이 서브 리스트를 정렬된 상태로 합치기만 하면 끝이다.Example아래 그림은 top-down 방식의 합병 정렬 예시이다. 빨간색은 분할, 초록색은 합병을 나타낸다.Fig 1. A recursive merge sort (top-down).(Image source: Wikipedia. Merge sort.)Algorithm알고리즘을 작성하기 전에 아래와 같은 요소를 고려해야 한다.먼저, 서브 리스트를 분할하기 위해 필요한 것은 서브 리스트의 시작과 끝 인덱스를 나타내는 $l$와 $r$이다. 어떤 서브 리스트를 2개의 하위 서브 리스트로 분할할 때 $l$과 $r$의 가운데 값 $m = \\dfrac{l + r}{2}$울 사용한다. 왼쪽 서브 리스트는 $l$부터 $m$, 오른쪽 서브 리스트는 $m + 1$부터 $r$로 분할된다.두 번째로, 합병 시 필요한 것은 임시 리스트이다. 두 서브 리스트를 합병할 때 원래 리스트에 합병하는 것이 아니라, 임시 배열에 합병한다. 구체적으로는 두 서브 리스트에서 현재 각각 가리키고 있는 원소 중 더 작은 원소를 현재 임시 리스트를 가리키고 있는 위치에 복사한다. 두 서브 리스트의 합병이 완료되면 다시 원래 리스트에 합병된 부분을 업데이트한다.자, 이제 알고리즘을 작성해보자. $\\mathbf{a}_i$는 a[i]와 동일한 의미이다. $\\text{Algorithm: Merge sort (top-down)}$ \\(\\begin{align*}&amp; \\textstyle \\text{Input: an array $\\mathbf{a} \\in \\mathbb{R}^n$ to sort, a temporary array $\\mathbf{b} \\in \\mathbb{R}^n$, the number of elements $n$} \\\\&amp; \\textstyle \\text{$l$ is the left begin index, $r$ is the right end index} \\\\\\\\&amp; \\textstyle l \\leftarrow 0 \\\\&amp; \\textstyle r \\leftarrow n - 1 \\\\&amp; \\textstyle \\text{split_merge($\\mathbf{a}$, $\\mathbf{b}$, $l$, $r$)} \\\\\\\\&amp; \\textstyle \\text{function split_merge($\\mathbf{a}$, $\\mathbf{b}$, $l$, $r$):} \\\\&amp; \\textstyle \\qquad \\text{If $l &lt; r$, then:} \\\\&amp; \\textstyle \\qquad\\qquad m \\leftarrow \\lfloor (l + r) \\div 2 \\rfloor \\\\&amp; \\textstyle \\qquad\\qquad \\text{split_merge($\\mathbf{a}$, $\\mathbf{b}$, $l$, $m$)} \\\\&amp; \\textstyle \\qquad\\qquad \\text{split_merge($\\mathbf{a}$, $\\mathbf{b}$, $m+1$, $r$)} \\\\&amp; \\textstyle \\qquad\\qquad \\text{merge($\\mathbf{a}$, $\\mathbf{b}$, $l$, $m$, $r$)} \\\\&amp; \\textstyle \\text{end} \\\\\\\\&amp; \\textstyle \\text{function merge($\\mathbf{a}$, $\\mathbf{b}$, $l$, $m$, $r$):} \\\\&amp; \\textstyle \\qquad i \\leftarrow l \\\\&amp; \\textstyle \\qquad j \\leftarrow m + 1 \\\\&amp; \\textstyle \\qquad \\text{Loop for $k = l, l + 1, \\dots$:} \\\\&amp; \\textstyle \\qquad|\\qquad \\text{If $i \\leq m$ and ($j &gt; r$ or $\\mathbf{a}_i \\leq \\mathbf{a_j}$), then:} \\\\&amp; \\textstyle \\qquad|\\qquad\\qquad \\mathbf{b}_k \\leftarrow \\mathbf{a}_i \\\\&amp; \\textstyle \\qquad|\\qquad\\qquad i \\leftarrow i + 1 \\\\&amp; \\textstyle \\qquad|\\qquad \\text{else:} \\\\&amp; \\textstyle \\qquad|\\qquad\\qquad \\mathbf{b}_k \\leftarrow \\mathbf{a}_j \\\\&amp; \\textstyle \\qquad|\\qquad\\qquad j \\leftarrow j + 1 \\\\&amp; \\textstyle \\qquad \\text{until $k = r$} \\\\&amp; \\textstyle \\qquad \\text{copy $\\mathbf{b}_{l:r+1}$ to $\\mathbf{a}_{l:r+1}$} \\qquad \\text{($l:r+1$ is $l, l+1, \\dots, r$)} \\\\&amp; \\textstyle \\text{end} \\\\\\end{align*}\\)C++ Code이제 위 알고리즘을 바탕으로 C++ 코드를 작성해보자. 위 알고리즘과 거의 동일하다. dtype은 임의의 비교 가능한 데이터 타입이다.void merge(dtype a[], dtype b[], int l, int m, int r){ // merge to b int i = l; int j = m + 1; for (int k = l; k &lt;= r; k++) { if (i &lt;= m &amp;&amp; (j &gt; r || a[i] &lt;= a[j])) { b[k] = a[i]; i++; } else { b[k] = a[j]; j++; } } // copy merged part of b to a for (int k = l; k &lt;= r; k++) { a[k] = b[k]; }}void split_merge(dtype a[], dtype b[], int l, int r){ if (l &gt;= r) return; int m = (l + r) / 2; split_merge(a, b, l, m); split_merge(a, b, m + 1, r); merge(a, b, l, m, r);}void merge_sort(dtype a[], dtype b[], int n){ split_merge(a, b, 0, n - 1);}References[1] Wikipedia. Merge sort." }, { "title": "[알고리즘] 쉘 정렬", "url": "/algorithm/sorting/shell-sort/", "categories": "Algorithm, Sorting", "tags": "Algorithm, Sorting", "date": "2022-10-31 00:00:00 +0900", "snippet": "쉘 정렬은 삽입 정렬을 최적화한 알고리즘으로 $h$ 간격으로 부분적으로 정렬한다. 특징은 아래와 같다. 비교 기반 in-place 시간 복잡도: 간격 $h$에 따라 다름 unstableKey Idea기존 삽입 정렬에서 왼쪽은 정렬된 리스트, 오른쪽은 정렬이 안된 리스트로 구분했었다. 따라서 삽입 정렬은 어느 정도 정렬이 되어 있는 리스트에 대해 강력한 성능을 낼 수 있다. 이 특징을 확장한 것이 쉘 정렬이다.쉘 정렬은 $h$ 간격으로 원소들을 삽입 정렬을 사용해 정렬한다. $h$는 매우 큰 수에서 점점 작아지다가 최종적으로는 1로 끝난다. 중요한 점은 반드시 $h$는 1로 끝나야 한다. $h$가 작아질 수록 부분적으로 정렬된 원소들이 많아지기 때문에 삽입 정렬의 성능은 향상되는 것이 핵심이다.$h$ 값은 알고리즘마다 다르며, $h$ 값에 따라 시간 복잡도가 달라진다. 여기서는 아래와 같이 비교적 간단한 간격을 사용할 것이다.\\[h = \\dfrac{3^k - 1}{2} &lt; n = 1, 4, 13, 40, 121, \\dots\\]$h$는 간격, $n$은 원소 개수이다. 위 간격의 시간 복잡도는 $O(n^\\frac{3}{2})$이다.Example Index 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Element 3 14 12 4 10 13 15 5 2 7 9 6 8 11 1 먼저, 위 배열 $\\mathbf{a}$에서 간격 $h=13$부터 시작한다. $(\\mathbf{a}_ 0, \\mathbf{a}_ {13})$에 대해 $(3, 11) \\rightarrow (3, 11)$, $(\\mathbf{a}_ 1, \\mathbf{a}_{14})$에 대해 $(14, 1) \\rightarrow (1, 14)$로 삽입 정렬을 수행한다. Index 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 After 13-sorting 3 1 12 4 10 13 15 5 2 7 9 6 8 11 14 그 다음 $h=4$이다. 마찬가지로 $(\\mathbf{a}_ 0, \\mathbf{a}_ 4, \\mathbf{a}_ 8, \\mathbf{a}_ {12})$, $(\\mathbf{a}_ 1, \\mathbf{a}_ 5, \\mathbf{a}_ 9, \\mathbf{a}_ {13})$, $(\\mathbf{a}_ 2, \\mathbf{a}_ 6, \\mathbf{a}_ {10}, \\mathbf{a}_ {14})$, $(\\mathbf{a}_ 3, \\mathbf{a}_ 7, \\mathbf{a}_ {11})$에 대해 삽입 정렬을 수행한다. 그 결과는 아래와 같다. Index 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 After 4-sorting 2 1 9 4 3 7 12 5 8 11 14 6 10 13 15 마지막으로 $h=1$에 대해 삽입 정렬을 수행한다. 간격이 1이라는 것은 곧 모든 원소에 대해 한번에 삽입 정렬을 수행한다는 의미로 일반적인 삽입 정렬과 동일하다. Index 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 After 1-sorting 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Algorithm위 예제를 바탕으로 알고리즘을 작성해보자. 크게 2가지를 고려해야한다. 시작 간격 $h$를 어떻게 설정할 것인가? 각 간격에 대한 부분 리스트를 어떻게 처리할 것인가?시작 간격 $h$는 $h$를 계속 키우다가 $h \\geq n$이 되는 순간을 포착하면 된다. 두 번째가 조금 난해할 수 있는데 부분 리스트 끼리 나누어 생각하다 보면 난관에 봉착할 수 있다. 특히 위 Example의 $h=4$일 때 어느 부분 리스트의 원소는 4개이고, 어느 부분 리스트의 원소는 3개이다. 이를 처리하기에는 상당히 까다롭다. 따라서 생각의 전환이 필요하다.쉘 정렬은 기본적으로 삽입 정렬 기반이다. 삽입 정렬은 왼쪽은 정렬된 리스트, 오른쪽은 정렬이 안된 리스트로 구분한다. 결국 왼쪽에서 오른쪽으로 진행한다. 따라서 부분 리스트끼리 처리하는 것이 아니라, 한 칸씩 오른쪽으로 이동하면서 삽입 정렬을 수행하되 그 삽입 정렬의 간격이 $h$라고 생각한다. 구체적으로 삽입 할 원소의 인덱스를 $i$라고 할때, $i-h, i-2h, \\dots$는 이미 정렬된 리스트이다. 따라서 $A_i$를 왼쪽 $h$간격의 정렬된 리스트에서 적당한 위치를 찾아 삽입한다. 이를 $i+1, i+2, \\dots$에 대해 반복한다.자 이제 알고리즘을 작성해보자. $\\mathbf{a}_i$는 a[i]와 동일한 의미이다. $\\text{Algorithm: Shell sort}$ \\(\\begin{align*}&amp; \\textstyle \\text{Input: an array $\\mathbf{a} \\in \\mathbb{R}^n$, the number of elements $n$} \\\\\\\\&amp; \\textstyle h \\leftarrow 1 \\\\&amp; \\textstyle \\text{Loop while $h &lt; n$:} \\\\&amp; \\textstyle \\qquad h \\leftarrow 3h + 1 \\\\\\\\&amp; \\textstyle h \\leftarrow (h - 1) \\div 3 \\\\&amp; \\textstyle \\text{Loop while $h \\geq 1$:} \\\\&amp; \\textstyle \\qquad \\text{Loop for $i = h, h+1, \\dots$:} \\\\&amp; \\textstyle \\qquad|\\qquad x \\leftarrow \\mathbf{a}_i \\\\&amp; \\textstyle \\qquad|\\qquad j \\leftarrow i - h \\\\&amp; \\textstyle \\qquad|\\qquad \\text{Loop while $j \\geq 0$ and $\\mathbf{a}_j &gt; x$:} \\\\&amp; \\textstyle \\qquad|\\qquad\\qquad \\mathbf{a}_{j+h} \\leftarrow \\mathbf{a}_j \\\\&amp; \\textstyle \\qquad|\\qquad\\qquad j \\leftarrow j - h \\\\&amp; \\textstyle \\qquad|\\qquad \\mathbf{a}_{j+h} \\leftarrow x \\\\&amp; \\textstyle \\qquad \\text{until $i=n-1$} \\\\&amp; \\textstyle \\qquad h \\leftarrow (h - 1) \\div 3 \\\\\\end{align*}\\)위 알고리즘에서 내부 루프는 간격이 $h$인 부분 리스트들에 대한 삽입 정렬이다. $h=1$로 바꿔보면 삽입 정렬 알고리즘과 완벽하게 동일함을 알 수 있다.C++ Code위 알고리즘을 바탕으로 C++ 코드를 작성해보자. dtype은 임의의 비교 가능한 데이터 타입이다.void shell_sort(dtype a[], int n){ int h = 1; while (h &lt; n) { h = 3 * h + 1; } for (h = (h - 1) / 3; h &gt;= 1; h = (h - 1) / 3) { for (int i = h; i &lt; n; i++) { dtype x = a[i]; int j = i - h; while (j &gt;= 0 &amp;&amp; a[j] &gt; x) { a[j + h] = a[j]; j -= h; } a[j + h] = x; } }}References[1] Wikipedia. Shellsort." }, { "title": "[알고리즘] 선택 정렬", "url": "/algorithm/sorting/selection-sort/", "categories": "Algorithm, Sorting", "tags": "Algorithm, Sorting", "date": "2022-10-28 00:00:00 +0900", "snippet": "선택 정렬은 가장 간단한 컨셉을 가지는 정렬 방법 중 하나이다. 선택 정령의 특징은 아래와 같다. 비교 기반 in-place 시간 복잡도: $O(n^2)$ unstableKey Idea오름차순으로 정렬한다고 할 때 선택 정렬의 핵심 아이디어 다음과 같다. 가장 작은 원소를 선택 해 0번 원소와 교환 그 다음 작은 원소를 선택 해 1번 원소와 교환 위 과정을 총 $n-2$번 원소까지 총 $n - 1$번 반복Example아래는 선택 정령의 과정을 보여주는 예시이다. bold체로 표시된 왼쪽 영역은 정렬이 완료되었음을 의미한다. List Smallest Element 11 25 22 64 12 11 11 12 22 64 25 12 11 12 22 64 25 22 11 12 22 25 64 25 11 12 22 25 64 64 위 예시에서 마지막 원소 64에 대해서는 굳이 과정을 거치지 않아도 된다.Algorithm아래는 선택 정렬 알고리즘이다. $\\mathbf{a}_i$는 a[i]와 동일한 의미이며, $\\arg\\min$은 가장 작은 원소의 인덱스를 의미한다. $\\text{Algorithm: Selection sort}$ \\(\\begin{align*}&amp; \\textstyle \\text{Input: an array $\\mathbf{a} \\in \\mathbb{R}^n$, the number of elements $n$} \\\\\\\\&amp; \\textstyle \\text{Loop for $i = 0, 1, \\dots$:} \\\\&amp; \\textstyle \\qquad m \\leftarrow \\arg\\min \\mathbf{a}_{i:n} \\qquad \\text{($\\mathbf{a}_{l:r}$ is from $\\mathbf{a}_l$ to $\\mathbf{a}_{r-1}$)} \\\\&amp; \\textstyle \\qquad \\text{Swap($\\mathbf{a}_i$, $\\mathbf{a}_m$)} \\\\&amp; \\textstyle \\text{until $i = n-2$} \\\\\\end{align*}\\)C++ Code위 알고리즘을 구현한 C++는 아래와 같다. $\\arg\\min$은 내부 루프를 통해 구할 수 있다. dtype은 임의의 비교 가능한 데이터 타입이며 swap() 함수는 두 변수의 값을 교환하는 함수이다.void selection_sort(dtype a[], int n){ for (int i = 0; i &lt; n - 1; i++) { int m = i; // the index of smallest element for (int j = i + 1; j &lt; n; j++) { if (a[j] &lt; a[m]) m = j; } swap(a[i], a[m]); }}References[1] Wikipedia. Selection sort." }, { "title": "[알고리즘] 삽입 정렬", "url": "/algorithm/sorting/insertion-sort/", "categories": "Algorithm, Sorting", "tags": "Algorithm, Sorting", "date": "2022-10-28 00:00:00 +0900", "snippet": "삽입 정렬은 선택된 원소를 이미 정렬된 영역에 삽입하는 방식의 간단한 정렬 알고리즘으로, 실제 사람이 카드 게임 시 카드를 정렬할 때와 유사한 방식이다. 특징은 아래와 같다. 비교 기반 in-place 시간 복잡도: $O(n^2)$ stableKey Idea삽입 정렬의 과정은 아래와 같다. $i$번째 원소를 선택 $0, 1, \\dots, i-1$번째 원소는 이미 오름차순으로 정렬되어 있음 $i-1$부터 $0$번째까지의 $j$번째 원소를 선택된 원소와 비교 후 선택 된 원소가 작을 경우 $j$번째 원소를 오른쪽으로 이동 3번 과정을 선택된 원소가 $j$번째 원소보다 작을 동안 반복 $j + 1$번째에 선택된 원소를 삽입 위 과정을 모든 원소에 대해 반복Example아래 움짤을 보면 삽입 정렬이 동작하는 방식을 쉽게 이해할 수 있다.Fig 1. A graphical example of insertion sort.(Image source: Wikipedia. Insertion sort.)AlgorithmKey Idea와 위 예시를 바탕으로 아래와 같이 알고리즘을 작성할 수 있다. $\\mathbf{a}_i$는 a[i]와 동일한 의미이다. $\\text{Algorithm: Insertion sort}$ \\(\\begin{align*}&amp; \\textstyle \\text{Input: an array $\\mathbf{a} \\in \\mathbb{R}^n$, the number of elements $n$} \\\\\\\\&amp; \\textstyle \\text{Loop for $i=1,2,\\dots$:} \\\\&amp; \\textstyle \\qquad x \\leftarrow \\mathbf{a}_i \\\\&amp; \\textstyle \\qquad j \\leftarrow i - 1 \\\\&amp; \\textstyle \\qquad \\text{Loop while $j \\geq 0$ and $\\mathbf{a}_j &gt; x$:} \\\\&amp; \\textstyle \\qquad\\qquad \\mathbf{a}_{j+1} \\leftarrow \\mathbf{a}_j \\\\&amp; \\textstyle \\qquad\\qquad j \\leftarrow j - 1 \\\\&amp; \\textstyle \\qquad \\mathbf{a}_{j+1} \\leftarrow x \\\\&amp; \\textstyle \\text{until $i=n-1$} \\\\\\end{align*}\\)C++ Code아래는 위 알고리즘을 C++로 작성한 코드이다. dtype은 임의의 비교 가능한 데이터 타입이다.void insertion_sort(dtype a[], int n){ for (int i = 1; i &lt; n; i++) { dtype x = a[i]; int j = i - 1; while (j &gt;= 0 &amp;&amp; a[j] &gt; x) { a[j + 1] = a[j]; j--; } a[j + 1] = x; }}References[1] Wikipedia. Insertion sort." }, { "title": "[알고리즘] 버블 정렬", "url": "/algorithm/sorting/bubble-sort/", "categories": "Algorithm, Sorting", "tags": "Algorithm, Sorting", "date": "2022-10-28 00:00:00 +0900", "snippet": "버블 정렬은 인접한 두 원소를 비교하여 정렬하는 간단한 방식의 알고리즘이다. 버블 정렬의 특징은 아래와 같다. 비교 기반 in-place 시간 복잡도: $O(n^2)$ stableKey Idea컨셉은 간단하다. 오름차순으로 정렬한다고 할 떄, $i$번째 원소와 $i+1$번째 원소를 비교해 $i$번째 원소가 더 크면 교환한다. $i$를 계속 키워나가다 보면 가장 큰 원소가 맨 마지막 위치에 있게 된다.Example배열 [ 5 1 4 2 8 ]이 있을 때 정렬은 아래와 같이 수행된다.First Pass$i=4$일 때:[ 5 1 4 2 8 ] $\\rightarrow$ [ 1 5 4 2 8 ][ 1 5 4 2 8 ] $\\rightarrow$ [ 1 4 5 2 8 ][ 1 4 5 2 8 ] $\\rightarrow$ [ 1 4 2 5 8 ][ 1 4 2 5 8 ] $\\rightarrow$ [ 1 4 2 5 8 ]Second Pass$i=3$일 때:[ 1 4 2 5 8 ] $\\rightarrow$ [ 1 4 2 5 8 ][ 1 4 2 5 8 ] $\\rightarrow$ [ 1 2 4 5 8 ][ 1 2 4 5 8 ] $\\rightarrow$ [ 1 2 4 5 8 ]위 과정을 $i=1$일 때까지 반복하면 된다.Algorithm위 예시를 바탕으로 알고리즘을 작성해보자. $\\mathbf{a}_i$는 a[i]와 동일한 의미이다. $\\text{Algorithm: Bubble sort}$ \\(\\begin{align*}&amp; \\textstyle \\text{Input: an array $\\mathbf{a} \\in \\mathbb{R}^n$, the number of elements $n$} \\\\\\\\&amp; \\textstyle \\text{Loop for $i = n - 1, n - 2, \\dots$:} \\\\&amp; \\textstyle \\qquad \\text{Loop for $j = 0, 1, \\dots$:} \\\\&amp; \\textstyle \\qquad\\qquad \\text{If $\\mathbf{a}_j &gt; \\mathbf{a}_{j+1}$, then:} \\\\&amp; \\textstyle \\qquad\\qquad\\qquad \\text{Swap($\\mathbf{a}_j$, $\\mathbf{a}_{j+1}$)} \\\\&amp; \\textstyle \\qquad \\text{until $j=i-1$} \\\\&amp; \\textstyle \\text{until $i = 1$} \\\\\\end{align*}\\)C++ Code위 알고리즘을 바탕으로 C++ 코드를 작성해보자. 위 알고리즘과 거의 동일하다. dtype은 임의의 비교 가능한 데이터 타입이며 swap() 함수는 두 변수의 값을 교환하는 함수이다.void bubble_sort(dtype a[], int n){ for (int i = n - 1; i &gt; 0; i--) { for (int j = 0; j &lt; i; j++) { if (a[j] &gt; a[j + 1]) swap(a[j], a[j + 1]); } }}References[1] Wikipedia. Bubble sort." }, { "title": "Policy Gradient Methods", "url": "/reinforcement-learning/rl-fundamental/policy-gradient-methods/", "categories": "Reinforcement Learning, RL Fundamental", "tags": "RL, AI, Function Approximation RL, Policy Gradient", "date": "2022-10-13 00:00:00 +0900", "snippet": "드디어 긴 장정 끝에 대망의 마지막 챕터로 왔다. 이번 포스트에서는 그 유명한 policy gradient method에 대해 소개하려고 한다. 길고 긴 여정이 드디어 끝났다.Introduction지금까지 우리는 value-based 기반의 방법을 다뤘었다. policy는 추정된 action value에 기반해 action을 선택했다. 이번에는 policy 자체를 매개변수화된 함수로써 학습하는 방법을 볼 것이다. policy parameter vector를 $\\mathbf{\\theta} \\in \\mathbb{R}^{d’}$라고 할 때 policy를 parameter $\\mathbf{\\theta}$가 주어졌을 때 action $a$를 선택하는 확률로써 정의할 수 있다.\\[\\pi(a \\vert s, \\mathbf{\\theta}) = \\Pr\\{A_t = a \\ \\vert \\ S_t = s, \\mathbf{\\theta}_t = \\mathbf{\\theta} \\}\\]policy parameter에 관한 scalar 성능 수치 $J(\\mathbf{\\theta})$가 있다고 하자. policy를 이 수치에 대한 gradient에 기반해 policy parameter를 학습할 것이다. 이 성능 수치를 maximize하는 것이 목적이며 이에 따라 $J$에 대해 gradient ascent를 수행한다.\\[\\mathbf{\\theta}_{t+1} + \\mathbf{\\theta}_t + \\alpha \\widehat{\\nabla J(\\mathbf{\\theta}_t)}\\]$\\widehat{\\nabla J(\\mathbf{\\theta}_t)} \\in \\mathbb{R}^{d’}$는 stochastic 추정치로, 이것의 기대값은 성능 수치의 gradient를 근사화한다. 이러한 일반적인 schema를 따르는 모든 방법들을 policy gradient methods라고 부른다.Policy Approximation and its Advantagespolicy gradient method에서는 $\\pi(a \\vert s, \\mathbf{\\theta})$가 미분 가능하다. 또한 exploration을 보장하기 위해 일반적으로 policy는 절대 deterministic하지 않으며 즉, stochastic (i.e., $\\pi(a \\vert s, \\mathbf{\\theta}) \\in (0, 1), \\text{ for all } s, a, \\mathbf{\\theta}$) 하다. policy-based method는 discrete action space 뿐만 아니라 continuous action space에서도 쉽게 적용 가능하다는 엄청난 장점이 있다.action space가 discrete이고 너무 크지 않다고 하자. 이때 state-action pair에 대한 매개변수화된 수치적인 선호도를 $h(s, a, \\mathbf{\\theta})$라고 한다면, 아래와 같이 policy $\\pi$를 soft-max distribution으로 정의할 수 있다.\\[\\pi(a \\vert s, \\mathbf{\\theta}) \\doteq \\dfrac{e^{h(s,a,\\mathbf{\\theta})}}{\\sum_b e^{h(s,b,\\mathbf{\\theta})}}\\]이러한 방식이 가지는 이점은 크게 아래와 같다. policy가 점점 deterministic policy로 다가가도록 근사화됨 임의의 확률을 가진 action 선택을 가능하게 함 policy가 종종 근사화하기에 더 단순한 함수이며 학습 속도가 빠름 사전 지식을 주입하기에 더 좋음1번의 경우 어떤 state에서의 optimal policy가 deterministic할 때 policy-based method는 deterministic 하도록 근사화하지만, action-value method는 $\\epsilon$-greedy policy를 사용할 때 항상 $\\epsilon$의 확률로 랜덤하게 action을 선택해야 한다. $\\epsilon$을 0으로 설정하면 다른 state에서도 deterministic하게 되므로 결코 바람직 하지 않음2번의 경우 카드게임 같은 확률 게임을 생각해볼 수 있다. 이러한 게임들은 optimal policy가 stochastic하다. 아래 그림을 보면 왜 $\\epsilon$-greedy에 비해 훨씬 우월한지 알 수 있다.Fig 1. When optimal policy is stochastic.(Image source: Sec 13.1 Sutton &amp; Barto (2020).)위 그림은 $\\epsilon = 0.1$일 때의 상황으로 optimal (약 0.59의 확률)에 비해 획득한 value가 훨씬 낮다.The Policy Gradient Theorempolicy 매개변수화에 대한 중요한 이론적인 이점이 하나 더 있다. 연속적인 policy 매개변수화는 action 선택 확률을 학습된 parameter의 함수로써 부드럽게 변화시킨다. 반면 $\\epsilon$-greedy는 매우 작은 변화로 maximum action value를 가지는 action이 변경되면 확률이 급격히 변경된다. 따라서 policy-gradient method는 수렴성이 더 강력히 보장된다.성능 수치 $J(\\mathbf{\\theta})$는 episodic과 continuing task에서 다르게 정의되긴 한다. 이 포스트에서는 episodic case에 대해서만 다룬다. 성능 수치를 episode의 시작 state에서의 value로 정의하자. 모든 episode는 특정한 state $s_0$에서 시작한다고 할 때 아래와 같이 정의된다.\\[J(\\mathbf{\\theta}) \\doteq v_{\\pi_\\mathbf{\\theta}}(s_0)\\]$v_{\\pi_\\mathbf{\\theta}}$는 $\\pi_\\mathbf{\\theta}$에 대한 true value function이다. 여기서는 no discounting ($\\gamma = 1$)을 가정한다.성능 개선을 보장하는 방향으로 policy parameter를 변경해야 한다. 문제는 성능이 action 선택과 state distribution에 의존한다는 점이다. action 선택은 별 문제되지 않지만 state distribution은 다르다. 우리는 일반적으로 state distribution을 모른다. 이것은 environment의 함수이기 때문이다.policy gradient theorem을 통해 이 문제를 쉽게 해결할 수 있다. state distribution에 대한 미분을 포함하지 않고, policy parameter에 관한 성능의 gradient에 대한 analytic 표현을 제공한다. 아래는 episodic case에 대한 policy gradient theorem이다.\\[\\nabla J(\\mathbf{\\theta}) \\propto \\sum_s \\mu(s) \\sum_a q_\\pi(s, a) \\nabla \\pi(a \\vert s, \\mathbf{\\theta})\\]$\\propto$는 “비례한다”의 의미이다. distribution $\\mu$는 $\\pi$하에서의 on-policy distribution이다.1이제 policy gradient theorem 증명 과정을 살펴보자. notation의 단순화를 위해 $\\pi$는 $\\mathbf{\\theta}$의 함수이며, 모든 gradient는 $\\mathbf{\\theta}$에 관한 것임을 암시적으로 나타낸다. 먼저 시작은 state-value function의 gradient를 action-value function에 관해 나타내는 것으로 시작한다.\\[\\begin{align} \\nabla v_\\pi(s) &amp;= \\nabla \\bigg[ \\sum_a \\pi(a \\vert s) q_\\pi(s,a) \\bigg], \\quad \\text{for all $s \\in \\mathcal{S}$} \\\\ &amp;= \\sum_a \\Big[ \\nabla \\pi(a \\vert s) q_\\pi(s,a) + \\pi(a \\vert s) \\nabla q_\\pi(s,a) \\Big] \\quad \\text{(product rule of calculus)} \\\\ &amp;= \\sum_a \\Big[ \\nabla \\pi(a \\vert s) q_\\pi(s,a) + \\pi(a \\vert s) \\nabla \\sum_{s',r}p(s',r \\vert s,a)\\big(r + v_\\pi(s')\\big) \\Big] \\quad (\\because q_\\pi(s, a) \\doteq \\sum_{s', r} p(s', r \\vert s, a) \\Big[r + \\gamma v_\\pi(s') \\Big]) \\\\ &amp;= \\sum_a \\Big[ \\nabla \\pi(a \\vert s) q_\\pi(s,a) + \\pi(a \\vert s) \\sum_{s'}p(s' \\vert s,a) \\nabla v_\\pi(s') \\Big] \\quad (\\because p(s' \\vert s, a) \\doteq \\sum_{r \\in \\mathcal{R}}p(s', r \\vert s, a)) \\\\ &amp;= \\sum_a \\Big[ \\nabla \\pi(a \\vert s) q_\\pi(s,a) + \\pi(a \\vert s) \\sum_{s'}p(s' \\vert s,a) \\quad \\text{(unrolling)} \\\\ &amp;\\quad\\quad \\sum_{a'} \\big[\\nabla \\pi(a' \\vert s') q_\\pi(s',a') + \\pi(a' \\vert s')\\sum_{s''}p(s'' \\vert s',a') \\nabla v_\\pi(s'') \\big] \\Big] \\\\ &amp;= \\sum_{x \\in \\mathcal{S}} \\sum_{k=0}^\\infty \\Pr(s \\rightarrow x, k, \\pi) \\sum_a \\nabla \\pi(a \\vert x) q_\\pi(x,a)\\end{align}\\]$p(s’ \\vert s, a)$2와 $q_\\pi(s, a)$3의 변환 원리는 MDP에 관한 포스트를 참고하기 바란다. unrolling을 반복한 이후의 $\\Pr(s \\rightarrow x, k, \\pi)$는 policy $\\pi$를 따를 때 $k$ time step에서의 state $s$에서 $x$로 transition이 발생할 확률이다.앞서 우리의 목적함수 $J(\\mathbf{\\theta}) = v_\\pi(s_0)$였다. 위에서 구한 $\\nabla v_\\pi(s)$를 통해 $\\nabla v_\\pi(s_0)$를 구해보자.\\[\\begin{align} \\nabla J(\\mathbf{\\theta}) &amp;= \\nabla v_\\pi(s_0) \\\\ &amp;= \\sum_s \\Bigg( \\sum_{k=0}^\\infty \\Pr(s_0 \\rightarrow s, k, \\pi) \\Bigg) \\sum_a \\nabla \\pi(a \\vert s) q_\\pi(s,a) \\\\ &amp;= \\sum_s \\eta(s) \\sum_a \\nabla \\pi(a \\vert s) q_\\pi(s,a) \\quad (\\text{$\\eta(s)$ is the expected number of visits}) \\\\ &amp;= \\sum_{s'} \\eta(s') \\sum_s \\dfrac{\\eta(s)}{\\sum_{s'}\\eta(s')} \\sum_a \\nabla \\pi(a \\vert s) q_\\pi(s,a) \\\\ &amp;= \\sum_{s'} \\eta(s') \\sum_s \\mu(s) \\sum_a \\nabla \\pi(a \\vert s) q_\\pi(s, a) \\\\ &amp;\\propto \\sum_s \\mu(s) \\sum_a \\nabla \\pi(a \\vert s) q_\\pi(s, a) \\quad (\\text{Q.E.D.})\\end{align}\\]드디어 모든 증명이 끝났다. 기대 방문 횟수 $\\eta$와 state distribution $\\mu$의 관계는 sutton 책에 자세히 기술되어 있으니 참고하기 바란다.4REINFORCE: Monte Carlo Policy Gradient자, 이제 본격적으로 실제 사용되는 policy gradient method를 알아보려고 한다. policy gradient theorem의 우측항은 target policy $\\pi$를 따를 때 얼마나 자주 state들이 나타나는지에 의한 weighted sum이다. 따라서 이를 실제 state $S_t$에서의 $\\pi$에 관한 기대값으로 표현할 수 있다.\\[\\begin{align} \\nabla J(\\mathbf{\\theta}) &amp;\\propto \\sum_s \\mu(s) \\sum_a q_\\pi(s,a) \\nabla \\pi(a \\vert s, \\mathbf{\\theta}) \\\\ &amp;= \\mathbb{E}_\\pi\\Bigg[\\sum_a q_\\pi(S_t,a) \\nabla \\pi(a \\vert S_t, \\mathbf{\\theta}) \\Bigg]\\end{align}\\]이를 통해 stochastic gradient-ascent를 수행할 수 있다.\\[\\mathbf{\\theta}_{t+1} \\doteq \\mathbf{\\theta}_t + \\alpha \\sum_a \\hat{q}(S_t, a, \\mathbf{w}) \\nabla \\pi(a \\vert S_t, \\mathbf{\\theta})\\]$\\hat{q}$은 $q_\\pi$의 학습된 근사치이다. 위 방법은 모든 action을 포함하기 때문에 all-actions method라고 부른다.그러나 우리는 모든 action을 고려하기 보다는, 실제 선택된 action에 대해 고려하고 싶다. 이를 수행하는 가장 간단하면서도 유명한 REINFORCE 알고리즘이 있다. 아래는 위 수식으로부터 REINFORCE 알고리즘을 유도하는 과정이다.\\[\\begin{align} \\nabla J(\\mathbf{\\theta}) &amp;\\propto \\mathbb{E}_\\pi \\Bigg[ \\sum_a \\pi(a \\vert S_t, \\mathbf{\\theta}) q_\\pi(S_t, a) \\dfrac{\\nabla \\pi(a \\vert S_t, \\mathbf{\\theta})}{\\pi(a \\vert S_t, \\mathbf{\\theta})} \\Bigg] \\\\ &amp;= \\mathbb{E}_\\pi \\Big[ q_\\pi(S_t, A_t) \\dfrac{\\nabla \\pi(A_t \\vert S_t, \\mathbf{\\theta})}{\\pi(A_t \\vert S_t, \\mathbf{\\theta})} \\Big] \\quad \\text{(replacing $a$ by the sample $A_t \\sim \\pi$)} \\\\ &amp;= \\mathbb{E}_\\pi \\Big[G_t \\dfrac{\\nabla \\pi(A_t \\vert S_t, \\mathbf{\\theta})}{\\pi(A_t \\vert S_t, \\mathbf{\\theta})}] \\quad \\text{(because $\\mathbb{E}_\\pi[G_t \\vert S_t, A_t] = q_\\pi(S_t, A_t)$)} \\\\ &amp;= \\mathbb{E}_\\pi \\Big[G_t \\nabla \\ln \\pi(A_t \\vert S_t, \\mathbf{\\theta}) \\Big] \\quad \\text{(logarithmic derivative)}\\end{align}\\]위 과정을 차례차례 보자. action $a$에 대한 합을, policy $\\pi$를 따를 때의 기대값에 의해 실제 선택된 action $A_t$로 대체하고 싶다. 그러기 위해서는 action $a$를 policy $\\pi(a \\vert S_t, \\mathbf{\\theta})$에 의해 가중치를 부여하면 된다. 이렇게 되면 policy $\\pi$를 따를 때 실제 선택된 action에 대한 기대값과 동일하다. 수식의 동등성을 유지하기 위해 $\\pi(a \\vert S_t, \\mathbf{\\theta})$를 곱하고 나눈다.마지막 라인이 우리가 얻고 싶었던 수식이다. $G_t$는 일반적인 return이며, $G_t \\nabla \\ln \\pi(A_t \\vert S_t, \\mathbf{\\theta})$는 각 time step에서 sampling된 값으로, 이것의 기대값은 gradient에 비례한다. 이를 통해 stochastic gradient ascent 알고리즘, REINFORCE 업데이트를 수행한다.\\[\\mathbf{\\theta}_{t+1} \\doteq \\mathbf{\\theta}_t + \\alpha G_t \\nabla \\ln \\pi(A_t \\vert S_t, \\mathbf{\\theta}_t)\\]REINFORCE는 time step $t$에서의 실제 return을 사용한다. 따라서 REINFORCE는 Monte Carlo (MC) method로, episode가 종료되어야만 모든 업데이트가 수행된다. 아래는 REINFORCE 알고리즘이다. $\\text{Algorithm: REINFORCE: Monte-Carlo Policy-Gradient Control (episodic) for $\\pi_\\ast$}$ \\(\\begin{align*}&amp; \\textstyle \\text{Input: a differentiable policy parameterization $\\pi(a \\vert s, \\mathbf{\\theta})$} \\\\&amp; \\textstyle \\text{Algorithm parameter: step size $\\alpha &gt; 0$} \\\\&amp; \\textstyle \\text{Initialize policy parameter $\\mathbf{\\theta} \\in \\mathbb{R}^{d'}$ (e.g., to $\\mathbf{0}$)} \\\\\\\\&amp; \\textstyle \\text{Loop forever (for each episode):} \\\\&amp; \\textstyle \\qquad \\text{Generate an episode $S_0, A_0, R_1, \\dots, S_{T-1}, A_{T-1}, R_T$, following $\\pi(\\cdot \\vert \\cdot, \\mathbf{\\theta})$} \\\\&amp; \\textstyle \\qquad \\text{Loop for each step of the episode $t = 0, 1, \\dots, T - 1$:} \\\\&amp; \\textstyle \\qquad\\qquad G \\leftarrow \\sum_{k = t+1}^T \\gamma^{k-t-1}R_k \\\\&amp; \\textstyle \\qquad\\qquad \\mathbf{\\theta} \\leftarrow \\mathbf{\\theta} + \\alpha \\gamma^t G \\nabla \\ln \\pi(A_t \\vert S_t, \\mathbf{\\theta}) \\\\\\end{align*}\\)한 가지 차이점은, REINFORCE 업데이트 시의 discount factor $\\gamma^t$이다. 앞서 policy gradient theorem에서 non-discounted case ($\\gamma=1$)을 가정했었지만, 여기서는 일반적인 discounted setting이기 때문에 추가되었다.REINFORCE는 $\\alpha$가 시간에 따라 감소한다고 할 때 local optimum으로의 수렴성이 보장된다. 그러나 MC method 특성 상 분산이 크고, 학습 속도가 느리다.REINFORCE with Baselinepolicy gradient theorem은 임의의 action value와 baseline $b(s)$의 비교를 포함하도록 일반화 될 수 있다.\\[\\nabla J(\\mathbf{\\theta}) \\propto \\sum_s \\mu(s) \\sum_a \\Big(q_\\pi(s,a) - b(s) \\Big) \\nabla \\pi(a \\vert, \\mathbf{\\theta})\\]baseline은 어떤 것도 가능하며, 심지어 난수도 가능하다. 위 수식이 성립하는 이유는 baseline에 의해 빼는 값이 0이기 때문이다.\\[\\sum_a b(s) \\nabla \\pi(a \\vert s, \\mathbf{\\theta}) = b(s) \\nabla \\sum_a \\pi(a \\vert s, \\mathbf{\\theta}) = b(s) \\nabla 1 = 0\\]위에 따라 REINFORCE에도 baseline을 적용하는 것이 가능하다.\\[\\mathbf{\\theta}_{t+1} \\doteq \\mathbf{\\theta}_t + \\alpha \\Big(G_t - b(S_t) \\Big) \\nabla \\ln \\pi(A_t \\vert S_t, \\mathbf{\\theta}_t)\\]baseline은 업데이트의 기대값을 변화시키지는 않는다. 하지만 분산에는 큰 영향을 미친다. 어떤 state들에서는 모든 action들이 높은 값을 가지고, 다른 어떤 state들에서는 모든 action들이 낮은 값을 가질 수 있다. 이 때 baseline을 적절히 사용하면 모든 action들의 value와 baseline의 차이의 평균을 0으로 조정해 분산을 낮출 수 있다.Actor-Critic MethodsREINFORCE는 앞서 봤듯이 MC method이기 때문에 여러 문제점을 가지고 있다. 우리는 이미 tabular method에서 봤듯이 MC method를 TD method로 개선할 수 있음을 알고 있다. 여기서도 마찬가지로 적용할 수 있다.REINFORCE의 실제 return을 bootstrapping이 사용된 one-step return $G_{t:t+1}$으로 대체한다. one-step return은 획득한 reward와 next state에서의 discounted state value를 더한 값이다. baseline은 현재 state value로 설정한다. action을 평가하는데 state-value function이 이러한 방식으로 사용될 때 이를 critic이라고 부르며, policy 부분은 actor라고 부른다. 따라서 이러한 policy gradient method를 actor-critic method라고 부른다.one-step actor-critic method는 one-step return과 baseline으로 학습된 state-value function을 통해 REINFORCE를 아래와 같이 완전히 대체할 수 있다.\\[\\begin{align} \\mathbf{\\theta}_{t+1} &amp;\\doteq \\mathbf{\\theta}_t + \\alpha \\Big( G_{t:t+1} - \\hat{v}(S_t, \\mathbf{w}) \\Big) \\nabla \\ln \\pi(A_t \\vert S_t, \\mathbf{\\theta}_t) \\\\ &amp;= \\mathbf{\\theta}_t + \\alpha \\Big( R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\mathbf{w}) - \\hat{v}(S_t, \\mathbf{w}) \\Big) \\nabla \\ln \\pi(A_t \\vert S_t, \\mathbf{\\theta}_t)\\end{align}\\]state-value function을 학습하는 가장 자연스러운 방법은 semi-gradient TD(0)이다. actor-critic은 REINFORCE와 달리 bootstrapping을 이용하기 때문에 완전히 online으로 학습할 수 있다. 아래는 one-step actor-critic 알고리즘이다. $\\text{Algorithm: One-step Actor-Critic (episodic), for estimating $\\pi_{\\mathbf{\\theta}} \\approx \\pi_\\ast$}$ \\(\\begin{align*}&amp; \\textstyle \\text{Input: a differentiable policy parameterization $\\pi(a \\vert s, \\mathbf{\\theta})$} \\\\&amp; \\textstyle \\text{Input: a differentiable state-value function parameterization $\\hat{v}(s, \\mathbf{w})$} \\\\&amp; \\textstyle \\text{Parameters: step sizes $\\alpha^\\mathbf{\\theta} &gt; 0$, $\\alpha^\\mathbf{w} &gt; 0$} \\\\&amp; \\textstyle \\text{Initialize policy parameters $\\mathbf{\\theta} \\in \\mathbb{R}^{d'}$ and state-value weights $\\mathbf{w} \\in \\mathbb{R}^d$ (e.g., to $\\mathbf{0}$)} \\\\\\\\&amp; \\textstyle \\text{Loop forever (for each episode)} \\\\&amp; \\textstyle \\qquad \\text{Initialize $S$ (first state of episode)} \\\\&amp; \\textstyle \\qquad I \\leftarrow 1 \\\\&amp; \\textstyle \\qquad \\text{Loop while $S$ is not terminal (for each time step):} \\\\&amp; \\textstyle \\qquad\\qquad A \\sim \\pi(\\cdot \\vert S, \\mathbf{\\theta}) \\\\&amp; \\textstyle \\qquad\\qquad \\text{Take action $A$, observe $S', R$} \\\\&amp; \\textstyle \\qquad\\qquad \\delta \\leftarrow R + \\gamma \\hat{v}(S', \\mathbf{w}) - \\hat{v}(S, \\mathbf{w}) \\qquad \\text{(if $S'$ is terminal, then $\\hat{v}(S', \\mathbf{w}) \\doteq 0$)} \\\\&amp; \\textstyle \\qquad\\qquad \\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha^\\mathbf{w} \\delta \\nabla \\hat{v}(S, \\mathbf{w}) \\\\&amp; \\textstyle \\qquad\\qquad \\mathbf{\\theta} \\leftarrow \\mathbf{\\theta} + \\alpha^\\mathbf{\\theta} I \\delta \\nabla \\ln \\pi(A \\vert S, \\mathbf{\\theta}) \\\\&amp; \\textstyle \\qquad\\qquad I \\leftarrow \\gamma I \\\\&amp; \\textstyle \\qquad\\qquad S \\leftarrow S' \\\\\\end{align*}\\)Policy Parameterization for Continuous Actionspolicy-based method는 큰 action space를 다루기에 적합하며 심지어 action이 무한개인 continuous action space를 다룰 수 있다. 수많은 action들의 각각의 확률을 학습하기 보다는, 확률 분포를 학습한다. 예를 들면 action이 실수 집합에서 정의될 때 action을 정규분포로부터 sampling 할 수 있다.정규분포의 확률밀도함수는 일반적으로 아래와 같다.\\[p(x) \\doteq \\dfrac{1}{\\sigma \\sqrt{2\\pi}} \\exp \\bigg(- \\dfrac{(x - \\mu)^2}{2 \\sigma^2} \\bigg)\\]$\\mu$는 평균, $\\sigma$는 표준편차이며, 여기서 $\\pi$는 당연하지만 실수 $\\pi \\approx 3.14159$이다. $p(x)$는 확률이 아닌 $x$에서의 확률밀도이며, $x$가 어떤 범위 안에 있을 확률은 확률밀도함수의 integral이다. 아래 그림은 $\\mu$와 $\\sigma$의 값에 따른 정규분포 확률밀도함수이다.Fig 2. Normal distribution.(Image source: Sec 13.7 Sutton &amp; Barto (2020).)policy를 정규분포에 대해 매개변수화하기 위해 평균 $\\mu$와 표준편차 $\\sigma$를 아래와 같이 매개변수화한다.\\[\\pi(a \\vert s, \\mathbf{\\theta}) \\doteq \\dfrac{1}{\\sigma(s, \\mathbf{\\theta}) \\sqrt{2\\pi}} \\exp \\bigg(- \\dfrac{(a - \\mu(s, \\mathbf{\\theta}))^2}{2 \\sigma(s, \\mathbf{\\theta})^2} \\bigg)\\]$\\mu : \\mathcal{S} \\times \\mathbb{R}^{d’} \\rightarrow \\mathbb{R}$와 $\\sigma : \\mathcal{S} \\times \\mathbb{R}^{d’} \\rightarrow \\mathbb{R}^+$는 매개변수화된 function approximator이다. 이를 위해 policy의 parameter vector를 $\\mathbf{\\theta} = [\\mathbf{\\theta} _\\mu, \\mathbf{\\theta} _\\sigma]^\\top$와 같이 두 파트로 나누어야한다.Summary길고 긴 여정이 끝이 났다. 강화학습의 기초부터 policy gradient method까지 오는데 많은 시간이 걸렸다. 이 summary를 끝으로 RL Fundamental은 끝이다. policy gradient method는 policy 자체를 매개변수화하는 방법 action을 선택하는 구체적인 확률을 학습할 수 있음 discrete action 뿐만 아니라 continuous action에도 적용 가능 policy gradient theorem에 의해 state distribution의 미분을 포함하지 않고 policy parameter에 의해 얼마나 성능이 영향을 받는지에 대한 기술이 가능 policy gradient method에 baseline을 추가하면 분산을 낮추는데 상당히 도움이 됨 REINFORCE는 Monte Carlo policy gradient method Actor-Critic은 bootstrapping을 통해 online 학습이 가능 actor는 policy, critic은 state-value function을 학습References[1] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction; 2nd Edition. 2020.Footnotes DevSlem. On-policy Prediction with Approximation. The Prediction Objective ($\\overline{\\text{VE}}$). On-policy Control with Approximation. Average Reward: A New Problem Setting for Continuing Tasks. Ergodicity. &#8617; DevSlem. Finite Markov Decision Processes. What is MDPs. &#8617; DevSlem. Finite Markov Decision Processes. Bellman Expectation Equation. &#8617; Reinforcement Learning: An Introduction; 2nd Edition. 2020. Sec. 9.2, p.199. &#8617; " }, { "title": "Exploration by Random Network Distillation", "url": "/reinforcement-learning/exploration-methods/exploration-by-random-network-distillation/", "categories": "Reinforcement Learning, Exploration Methods", "tags": "RL, AI, DRL", "date": "2022-09-21 00:00:00 +0900", "snippet": "이 포스트에서는 exploration을 쉽고 효과적으로 수행할 수 있는 방법인 Exploration by Random Network Distillation 논문을 소개한다.Abstract exploration bonus는 observation feature의 예측 error임 고정 랜덤 초기화 신경망이 사용됨 extrinsic reward와 intrinsic reward를 유연하게 결합 Montezuma’s Revenge에서 SOTA를 달성IntroductionReinforcement Learning (RL) method는 dense reward 환경에서는 잘 작동한다. dense reward 환경은 랜덤한 action sequence로부터 쉽게 reward를 찾을 수 있는 환경을 의미한다. 반대로 reward를 찾기 어려운 spare reward 환경에서는 학습에 종종 실패한다.dense reward 환경을 모델링하는 것은 어려우며 부적절하다. dense reward 환경은 일종의 cheating 행위가 될 수도 있다. 따라서 spare reward 환경에서 적절히 환경을 탐색할 수 있는 방법이 필요하다.이 논문에서 제시한 방법의 특징은 아래와 같다. 간단한 구현 high-dimensional observation에서도 잘 작동 어떤 policy optimization 알고리즘에도 적용 가능 experience batch에 대한 신경망의 단일 forward pass만 요구하기 때문에 효율적신경망은 학습된 예제와 비슷한 것들에 대해 상당히 낮은 prediction error를 가지는 경향이 있다. 이러한 특징을 기반으로 새로운 experience의 novelty를 정량화함으로써 exploration bonus를 정의할 수 있다.prediction error를 최대화하려는 agent는 대체로 확률적 transition을 추구하는 경향이 있다. 가장 대표적인 예시가 TV noise이다. TV noise는 계속 확률적으로 변하기 때문에 agent는 항상 새롭다고 느끼게 된다. 그 결과 쓸모없는 noise로 가득찬 TV 화면만 계속 쳐다보게 된다.이 논문은 위와 같은 문제를 입력에 대한 결정론적 함수를 사용한 exploration bonus를 정의함으로써 해결한다. 이 결정론적 함수는 observation에 대한 고정 랜덤 초기화 신경망이다.exploration bonus를 extrinsic reward와 결합하기 위해 PPO algorithm을 두 reward stream에 대한 두 개의 state value function을 사용하도록 변형한다. 이를 통해 각 reward에 대해 서로 다른 discount rate 적용이 가능해지며, episodic과 non-episodic return을 결합할 수 있게 해준다.Exploration Bonus환경으로부터 획득하는 reward를 $e_t$라고 하자. 이 때 $e_t$는 sparse하다. exploration bonus $i_t$는 agent가 spare reward 환경을 적절히 탐색하도록 돕는 역할을 한다. 최종적으로 reward function은 $r_t = e_t + i_t$로 정의된다. agent가 새로운 state를 탐색하도록 돕기 위해서는 당연히 자주 방문했던 state보다 새로운 state에서 $i_t$가 높아야 할 것이다.Random Network Distillation이 논문에서 exploration bonus $i_t$를 어떻게 정의하는지 알아보자. 먼저, 이 논문에서는 2개의 network를 사용한다. target - 고정 랜덤 초기화 신경망 (fixed randomly initialized network) predictor - observation을 예측target network $f : \\mathcal{O} \\rightarrow \\mathbb{R}^k$는 observation을 임베딩한다. predictor network $\\hat{f} : \\mathcal{O} \\rightarrow \\mathbb{R}^k$는 expected MSE $\\lVert \\hat{f}(x;\\theta) - f(x) \\rVert^2$를 최소화하도록 학습된다. prediction error는 predictor가 학습했던 것과 비슷하지 않은 새로운 state에 대해서 높을 것이다. 이를 통해 exploration을 도울 수 있다.Prediction Errorprediction error의 요인은 아래와 같다. Amount of training data - 비슷한 example을 적게 관찰 했을 때 Stochasticity - target function이 stochastic할 때 Model misspecification - 반드시 필요한 정보를 놓쳤거나 target function의 복잡성에 맞추기 어려울 때 Learning dynamics - target function을 가장 잘 근사하는 predictor를 찾는데 실패할 때위 첫번째 요소는 prediction error를 exploration bonus로 사용하게 하는 근본적 요인이다. 만약 예측 문제가 forward dynamics ($s_t$와 $a_t$를 통해 $s_{t+1}$을 예측하는 모델) 일 경우 두번째 요소는 ‘noisy-TV’ 문제를 일으킨다. deterministic한 transition보다 stochastic한 transition 예측이 어려운 건 너무나 당연하다. 또한 세번째 요소 역시 부적절하다.RND는 target network가 deterministic하게 선택되고, predictor network의 model-class 내에 있기 때문에 두번째와 세번째 요소를 피할 수 있다.Combining Intrinsic and Extrinsic Returns이 논문에서는 intrinsic reward를 non-episodic return으로 좋다고 주장한다. 그 이유는 아래와 같다. agent의 intrinsic return은 미래에 발견할 수도 있는 모든 새로운 상태와 관련됨 episodic intrinsic reward는 정보 누락을 발생시킴 이 접근법은 인간이 게임을 탐색할 때와 유사함 episodic하다면 탐색 도중 game over 시 return이 0이 되기 때문에 risk 감수를 꺼리게 됨그러나 extrinsic reward에 대해서는 episodic return으로 다루는 것이 좋다고 주장한다. 그 이유는 만약 게임 시작 근처에서 reward를 발견할 경우, 그 reward를 계속 획득하기 위해 의도적으로 game over를 반복적으로 당하도록 악용할 것이다.그렇다면 어떻게 intrinsic reward $i_t$의 non-episodic stream과 extrinsic reward $e_t$의 episodic stream을 적절히 결합할 수 있을까? 이 논문에서는 extrinsic return $R_E$와 intrinsic return $R_I$ 각각을 더한 $R = R_E + R_I$를 관찰한다. 즉, 각 return에 대한 value $V_E$와 $V_I$를 구한 뒤 value function $V = V_E + V_I$로 결합한다. 이러한 아이디어로 서로 다른 discount factor를 사용한 reward stream을 결합할 수도 있다.episodic과 non-episodic reward stream의 결합 혹은 서로 다른 discount factor를 가진 reward stream의 결합을 하지 않더라도, value function에 대한 추가적인 supervisory signal의 존재 때문에 여전히 value function을 분리하는 것에 이점이 있다. 이는 특히 exploration bonus에 중요한데 extrinsic reward function은 stationary한 반면 intrinsic reward function은 non-stationary하기 때문이다.Reward and Observation Normalizationprediction error를 exploration bonus로 사용할 때, 환경이 달라지거나 다른 순간에 있을 때 reward 크기가 너무 달라진다는 문제가 있다. reward를 일관된 크기로 유지하기 위해 intrinsic return의 표준편차 추정치로 나눔으로써 정규화를 수행한다.observation도 정규화를 수행한다. observation을 정규화하지 않을 경우 임베딩의 분산이 극도로 낮아 입력에 대한 정보가 전혀 전달되지 않을 수 있다. 이를 위해 observation에 평균을 빼고 표준편차로 나눈 뒤 -5와 5 사이의 범위로 clipping한다. 정규화 파라미터 (평균, 표준편차)는 최적화 시작 전에 random agent로 약간의 step을 통해 초기화된다." }, { "title": "Eligibility Traces", "url": "/reinforcement-learning/rl-fundamental/eligibility-traces/", "categories": "Reinforcement Learning, RL Fundamental", "tags": "RL, AI, Function Approximation RL", "date": "2022-09-01 00:00:00 +0900", "snippet": "이번 포스트에서는 TD와 Monte Carlo method를 통합 및 일반화하는 eligibility traces에 대해 다룰 것이다.Introductioneligibility traces는 TD와 Monte Carlo (MC) method를 통합 및 일반화하는 방법으로 스펙트럼에 걸쳐 있다. 스펙트럼의 양 끝에는 MC method ($\\lambda=1$)와 1-step TD method ($\\lambda=0$)가 있다. 또한 eligibility traces는 MC method를 online 학습과 continuing task에서의 학습을 가능하게 한다.eligibility traces와 비슷한 방법으로 $n$-step TD method가 존재한다.1 그러나 eligibility traces는 $n$-step TD method보다 우아한 알고리즘적 메커니즘을 지니고, 상당한 계산적 이점을 가진다.eligibility traces는 단기 기억 vector인 eligibility trace $\\mathbf{z}_t \\in \\mathbb{R}^d$와 동시에 장기 기억 weight vector $\\mathbf{w}_t \\in \\mathbb{R}^d$를 사용한다. 이 둘이 무슨 역할을 하는 지 곧 알아볼 것이다.eligibility traces가 $n$-step method에 비해 가지는 주요한 계산적 이점은 마지막 $n$개의 feature vector를 저장하는 대신, 단 하나의 trace vector만을 사용한다는 것에서 비롯된다. 또한 $n$-step method는 $n-1$ time step만큼 학습이 지연되고 episode 종료를 포착해야하는 반면, eligibility traces는 학습이 지속적이고 균일하게 수행된다.MC method와 $n$-step method는 각각 모든 미래 reward와 $n$개의 reward에 기반해 업데이트가 수행된다. 이렇게 업데이트되는 state로부터 앞 혹은 미래를 바라보는 것에 기반하는 방법을 forward view라고 부른다. 그러나 eligibility trace를 사용할 경우, 업데이트되는 state로부터 최근 방문했던 state를 향해 뒤 혹은 과거를 바라보는데, 이러한 방법을 backward view라고 한다.이 내용들을 이제 차근차근 알아볼 것이다. 평소처럼 먼저 state value에 대한 prediction을 알아본 뒤, action value와 control로 확장한다.The $\\lambda$-return먼저, $n$-step return에 대해 리뷰를 하자. $n$-step return $G_{t:t+n}$은 아래와 같이 $n$개의 discounted reward와 $n$-step에 도달된 state의 discounted 추정치를 더한 값으로 정의된다.\\[G_{t:t+n} \\doteq R_{t+1} + \\gamma R_{t+2} + \\cdots + \\gamma^{n-1}R_{t+n} + \\gamma^n\\hat{v}(S_{t+n}, \\mathbf{w}_{t+n-1}), \\quad 0 \\leq t \\leq T - n\\]$\\hat{v}(s,\\mathbf{w})$는 weight vector $\\mathbf{w}$가 주어졌을 때 state $s$에서의 추정치이다. $T$는 episode의 terminal time step이며 $n \\geq 1$이다.우리의 이번 목적인 $\\lambda$-return은 모든 $n$에 대한 $n$-step return들의 평균으로 TD($\\lambda$) 알고리즘에 사용된다. 각 가중치는 $\\lambda^{n-1} \\text{ (where $\\lambda \\in [0, 1)$)}$에 비례하며, 가중치의 총합을 1로 설정하기 위해 $1 - \\lambda$에 의해 정규화된다. 아래는 $\\lambda$-return의 정의이다.\\[G_t^\\lambda \\doteq (1 - \\lambda) \\sum_{n=1}^\\infty \\lambda^{n-1}G_{t:t+n}\\]위 수식을 아래와 같이 episode 종료 전후로 분리할 수 있다.\\[G_t^\\lambda = (1 - \\lambda)\\sum_{n=1}^{T-t-1} \\lambda^{n-1}G_{t:t+n} + \\lambda^{T-t-1}G_t\\]이 수식은 $\\lambda = 1$일 때의 상황을 더 명확히 해준다. $\\lambda = 1$일 때 왼쪽 main sum은 0이 되며 기본적인 return $G_t$만 남게 된다. 따라서 $\\lambda = 1$일 때 $\\lambda$-return은 MC method이다. 반대로 $\\lambda = 0$일 때 오직 one-step return $G_{t:t+1}$만 남게 되기 때문에 one-step TD method이다. $\\lambda$-return은 $n$-step return과 비교했을 때 MC method와 one-step TD method 사이를 조금 더 부드럽게 이동할 수 있다. 아래는 $\\lambda$-return에서 $n$-step return sequence에 가중치를 부여하는 것을 나타낸 backup diagram이다.Fig 1. The backup diagram for TD($\\lambda$).(Image source: Sec 12.1 Sutton &amp; Barto (2020).)아래는 각 $n$-step return에 부여되는 가중치의 변화 추이를 나타내는 그림이다. terminal time step 이후의 $n$-step return은 실제 return $G_t$이다.Fig 2. Weighting given in the $\\lambda$-return to each of the $n$-step returns.(Image source: Sec 12.1 Sutton &amp; Barto (2020).)이제 $\\lambda$-return에 기반한 첫번째 알고리즘을 정의하자. 굉장히 naive한 알고리즘으로 off-line $\\lambda$-return algorithm이라고 부른다. off-line 알고리즘인 이유는 episode 동안 weight vector가 변하지 않기 때문이다. episode 종료 후에, 전체 off-line update sequence가 아래 일반적인 semi-gradient rule을 따라 수행된다. 이 때 target은 $\\lambda$-return $G_t^\\lambda$이다.\\[\\mathbf{w}_{t+1} \\doteq \\mathbf{w}_t + \\alpha \\Big[G_t^\\lambda - \\hat{v}(S_t, \\mathbf{w}_t) \\Big] \\nabla \\hat{v}(S_t, \\mathbf{w}_t), \\quad t = 0, \\dots, T - 1\\]그렇다면 왜 episode 종료까지 기다려야 할까? 이유는 비교적 간단하다. $n$-step TD method는 $n$-step return을 계산하기 위해 $n$번의 transition이 발생할 때까지 기다려야 했다. $\\lambda$-return은 기본적으로 $G_{t:t+1}$부터 $G_t$까지 모든 return을 포함한다. $G_t$를 계산하기 위해서는 episode 종료를 기다려야만 한다.우리가 지금까지 알아본 방법은 forward view이다. time step $t$에서 어떤 state $S_t$를 update할 때 우리는 $t+1, t+2, \\dots$와 같이 미래의 보상과 state를 본다. 이 state를 업데이트한 후 다음 state로 넘어가면, 우리는 이전 state를 결코 다시 보지 않는다. 반대로 미래의 것들은 반복적으로 처리된다. 아래는 이러한 forward view의 관계를 나타내는 그림이다.Fig 3. The forward view.(Image source: Sec 12.1 Sutton &amp; Barto (2020).)TD($\\lambda$)TD($\\lambda$)는 off-line $\\lambda$-return을 아래와 같이 3가지 방식으로 개선하였다. episode의 모든 step에서 weight vector를 업데이트할 수 있음 1의 이유로 계산이 동등하게 분배됨 1의 이유로 continuing problem에 적용 가능eligibility trace는 vector $\\mathbf{z}_t \\in \\mathbb{R}^d$는 weight vector $\\mathbf{w}_t$와 동일한 차원이다. weight vector는 시스템의 lifetime 동안 누적되는 장기기억인 반면, eligibility trace는 한 episode 길이보다 적게 지속되는 단기기억이다. 이러한 eligibility trace는 weigh vector에 영향을 미친다.TD($\\lambda$)에서 eligibility trace vector는 episode 시작 시에 zero vector로 초기화되며 아래와 같이 업데이트 된다.\\[\\begin{align*} &amp; \\mathbf{z}_{-1} \\doteq \\mathbf{0}, \\\\ &amp; \\mathbf{z}_t \\doteq \\gamma \\lambda \\mathbf{z}_{t-1} + \\nabla \\hat{v}(S_t, \\mathbf{w}_t), \\quad 0 \\leq t \\leq T\\end{align*}\\]$\\lambda$는 이전 section에서 소개된 parameter로 이제 trace-decay parameter로 부를 것이다. eligibility trace는 weight vector의 각 원소가 최근 state value에 어떻게 기여하는지를 추적한다. 여기서 “최근”은 $\\gamma\\lambda$에 의해 정의된다.이제 eligibility trace를 사용해 weight vector를 업데이트하는 방법을 살펴보자. 먼저, state value에 대한 TD error는 아래와 같다.\\[\\delta_t \\doteq R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\mathbf{w}_t) - \\hat{v}(S_t, \\mathbf{w}_t)\\]TD($\\lambda$)에서 weight vector는 scalar TD error와 vector eligibility trace에 비례하여 업데이트 된다.\\[\\mathbf{w}_{t+1} \\doteq \\mathbf{w}_t + \\alpha \\delta_t \\mathbf{z}_t\\]아래는 알고리즘이다. $\\text{Algorithm: Semi-gradient TD($\\lambda$) for estimating $\\hat{v} \\approx v_\\pi$}$ \\(\\begin{align*}&amp; \\textstyle \\text{Input: the policy $\\pi$ to be evaluated} \\\\&amp; \\textstyle \\text{Input: a differentiable function $\\hat{v} : \\mathcal{S}^+ \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}$ such that $\\hat{v}(\\text{terminal}, \\cdot) = 0$} \\\\&amp; \\textstyle \\text{Algorithm parameters: step size $\\alpha &gt; 0$, trace decay rate $\\lambda \\in [0,1]$} \\\\&amp; \\textstyle \\text{Initialize value-function weights $\\mathbf{w}$ arbitrarily (e.g., $\\mathbf{w} = \\mathbf{0}$)} \\\\\\\\&amp; \\textstyle \\text{Loop for each episode:} \\\\&amp; \\textstyle \\qquad \\text{Initialize $S$} \\\\&amp; \\textstyle \\qquad \\mathbf{z} \\leftarrow \\mathbf{0} \\qquad \\text{(a $d$-dimensional vector)} \\\\&amp; \\textstyle \\qquad \\text{Loop for each step of episode:} \\\\&amp; \\textstyle \\qquad\\qquad \\text{Choose } A \\sim \\pi(\\cdot \\vert S) \\\\&amp; \\textstyle \\qquad\\qquad \\text{Take action $A$, observe $R, S'$} \\\\&amp; \\textstyle \\qquad\\qquad \\mathbf{z} \\leftarrow \\gamma \\lambda \\mathbf{z} + \\nabla \\hat{v}(S, \\mathbf{w}) \\\\&amp; \\textstyle \\qquad\\qquad \\delta \\leftarrow R + \\gamma \\hat{v}(S', \\mathbf{w}) - \\hat{v}(S, \\mathbf{w}) \\\\&amp; \\textstyle \\qquad\\qquad \\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha \\delta \\mathbf{z} \\\\&amp; \\textstyle \\qquad\\qquad S \\leftarrow S' \\\\&amp; \\textstyle \\qquad \\text{until $S'$ is terminal} \\\\\\end{align*}\\)TD($\\lambda$)는 시간을 거꾸로 향한다. $\\lambda$에 의해 현재 time step으로부터 시간적으로 더 멀리 떨어진 이전 state일 수록 더 적게 update된다. 더 멀리 떨어질 수록 더 많이 discount되기 때문이다. 즉, 더 이전의 state에게 TD error에 대한 더 낮은 신용을 준다. 아래는 이에 대한 그림이다.Fig 4. The backward view.(Image source: Sec 12.2 Sutton &amp; Barto (2020).)$\\lambda = 0$일 때 $\\mathbf{z}_t = \\nabla\\hat{v}(S_t, \\mathbf{w}_t)$로 TD($\\lambda$)의 update는 one-step semi-gradient TD update가 된다. 이를 TD(0)라 부른다. 반대로 $\\lambda$가 1일 때는 신용은 오직 $\\gamma$에 의해서만 감소한다. 이는 결국 MC method로 여길 수 있다. 이를 TD(1)이라고 부른다. 그러나 TD(1)은 MC method와 다르게 continuing task에도 적용할 수 있으며, online으로 학습할 수 있다.$n$-step Truncated $\\lambda$-return Methods앞서 봤던 off-line $\\lambda$-return 알고리즘을 개선시켜보자. $\\lambda$-return의 근본적인 문제는 episode 종료 전까지 실제 return $G_t$를 모른다는 것이다. 우리는 이 문제에 대해 이미 수없이 다뤄왔다. 실제 return $G_t$를 bootstrapping을 통해 근사화하면 된다. 데이터가 이후 horizon $h$까지 주어졌을 때, $\\lambda$-return을 아래와 같이 변경할 것이며 이를 truncated $\\lambda$-return이라고 한다.\\[G_{t:h}^\\lambda \\doteq (1 - \\lambda) \\sum_{n=1}^{h-t-1} \\lambda^{n-1} G_{t:t+n} + \\lambda^{h-t-1}G_{t:h}, \\quad 0 \\leq t &lt; h \\leq T\\]기존 $\\lambda$-return에서 terminal time step $T$가 horizon $h$로 대체되었으며, 실제 return $G_t$가 $n$-step return $G_{t:h}$로 대체되었다. 기존 $n$-step method에서는 $n$-step return만 사용했지만 여기서는 $1 \\leq k \\leq n$에 대해 모든 $k$-step return이 포함된다. truncated $\\lambda$-return을 사용한 TD update를 Truncated TD($\\lambda$) (TTD($\\lambda$))라고 하며 아래는 이에 대한 backup diagram이다.Fig 5. The backup diagram for Truncated TD($\\lambda$).(Image source: Sec 12.3 Sutton &amp; Barto (2020).)기존 TD($\\lambda$)와 비교했을 때 TTD($\\lambda$)는 가장 긴 요소가 episode 종료가 아닌, 최대 $n$-step까지 diagram이 이어진다. TTD($\\lambda$)는 아래와 같이 정의된다.\\[\\mathbf{w}_{t+n} \\doteq \\mathbf{w}_{t+n-1} + \\alpha \\big[G_{t:t+n}^\\lambda - \\hat{v}(S_t, \\mathbf{w}_{t+n-1}) \\big] \\nabla\\hat{v}(S_t, \\mathbf{w}_{t+n-1}), \\quad 0 \\leq t &lt; T\\]Sarsa($\\lambda$)이제 action-value method로 확장하자. off-line $\\lambda$-return algorithm의 action-value 형식은 단순히 $\\hat{v}$을 $\\hat{q}$으로 대체하기만 하면 된다.\\[\\mathbf{w}_{t+1} \\doteq \\mathbf{w}_t + \\alpha \\Big[G_t^\\lambda - \\hat{q}(S_t,A_t,\\mathbf{w}_t) \\Big] \\nabla \\hat{q}(S_t,A_t,\\mathbf{w}_t), \\quad t = 0,\\dots,T-1\\]$G_t^\\lambda \\doteq G_{t:\\infty}^\\lambda$이다.action value에 대한 TD method는 이러한 forward view를 근사화한다. 이를 Sarsa($\\lambda$)라고 하며 TD($\\lambda$)와 동일한 update rule을 가진다.\\[\\begin{align*} &amp; \\mathbf{w}_{t+1} \\doteq \\mathbf{w}_t + \\alpha \\delta_t \\mathbf{z}_t, \\\\ &amp; \\delta_t \\doteq R_{t+1} + \\gamma \\hat{q}(S_{t+1},A_{t+1},\\mathbf{w}_t) - \\hat{q}(S_t,A_t,\\mathbf{w}_t), \\\\ &amp; \\mathbf{z}_{-1} \\doteq \\mathbf{0}, \\\\ &amp; \\mathbf{z_t} \\doteq \\gamma \\lambda \\mathbf{z}_{t-1} + \\nabla \\hat{q}(S_t,A_t,\\mathbf{w}_t), \\quad 0 \\leq t \\leq T\\end{align*}\\]아래는 Sarsa($\\lambda$)의 backup diagram이다.Fig 6. Sarsa($\\lambda$)’s backup diagram.(Image source: Sec 12.7 Sutton &amp; Barto (2020).)아래는 그동안 알아보았던 Sarsa를 비교하는 좋은 그림이다. 왜 eligibility trace가 one-step과 $n$-step method보다도 상당히 효율적인지 알 수 있다.Fig 7. Comparison of control algorithms in Gridworld.(Image source: Sec 12.7 Sutton &amp; Barto (2020).)eligibility trace method는 episode의 시작까지 모든 action value를 업데이트 하지만 최근으로부터 멀리 떨어질 수록 더 적은 정도로 업데이트한다. 가장 매력적이고 종종 가장 강력한 방법이다.Summary $\\lambda$-return은 $n$-step return의 모든 $n$에 대한 평균으로 더 일반화된 형식 weight vector는 장기 기억, eligibility trace vector는 단기 기억 TD($\\lambda$)는 backward view로 이전 state들을 추적 eligibility trace는 episode의 시작까지 모든 value를 최근성에 따라 다른 정도로 업데이트References[1] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction; 2nd Edition. 2020.Footnotes DevSlem. n-step Bootstrapping.. &#8617; " }, { "title": "Off-policy Methods with Approximation", "url": "/reinforcement-learning/rl-fundamental/off-policy-methods-with-approximation/", "categories": "Reinforcement Learning, RL Fundamental", "tags": "RL, AI, Function Approximation RL", "date": "2022-08-30 00:00:00 +0900", "snippet": "이 포스트에서는 on-policy function approximation을 off-policy로의 확장과 이로 인해 발생하는 문제들에 대해 다룰 것이다.Introductionoff-policy method는 behavior policy $b$에 의해 생성된 experience로부터 target policy $\\pi$에 대한 value function을 학습하는 방법이다. 반대로 on-policy method는 behavior policy와 target policy가 동일한 방법이다. off-policy method에서는 일반적으로 target policy $\\pi$는 greedy policy이고, behavior policy $b$는 좀 더 효과적으로 exploration을 수행할 수 있는 policy (e.g., $\\epsilon$-greedy policy) 이다.off-policy method의 문제는 크게 두가지로 나뉜다. 첫 번째는 update target (target policy 아님)과 관련된 이슈로, 이전 tabular off-policy method에서 다뤘었던 importance sampling과 관련되어 있다. 두 번째는 update distribution과 관련된 이슈로, update distribution이 on-policy distribution을 따르지 않는 다는 점이다. on-policy distribution은 semi-gradient method의 안정성에 있어 중요하다. 이 두 이슈에 대해 이번 포스트에서 다룰 것이다.Semi-gradient Methods이 섹션에서는 off-policy method를 간략히 semi-gradient method로 확장한다. 이 확장된 방법은 off-policy method의 첫 번째 이슈에 대해서만 다루며, 수렴이 안되어 발산할 때도 있다.$n$-step tabular method에서 사용했던 방법을 단순히 function approximation을 사용한 weight vector $\\mathbf{w}$에 대한 update로 변경한다. off-policy method는 target policy와 behavior policy의 distribution이 다르기 때문에 importance sampling 기법을 사용한다.1 그 중 off-policy TD(0)와 같은 알고리즘은 per-step importance sampling ratio를 사용한다.\\[\\rho_t \\doteq \\rho_{t:t} = \\dfrac{\\pi(A_t \\vert S_t)}{b(A_t \\vert S_t)}\\]아래는 off-policy semi-gradient TD(0)로 semi-gradient on-policy TD(0)에 $\\rho_t$만 추가되었다.\\[\\mathbf{w}_{t+1} \\doteq \\mathbf{w}_t + \\alpha \\rho_t \\delta_t \\nabla \\hat{v}(S_t, \\mathbf{w}_t)\\]$\\delta_t$는 TD error로 아래는 각각 episodic, discounted setting과 average reward를 사용하는 continuing, undiscounted setting에서의 TD error이다.\\[\\begin{align*} &amp; \\delta_t \\doteq R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\mathbf{w}_t) - \\hat{v}(S_t, \\mathbf{w}_t) \\text{, or} \\tag{episodic} \\\\ &amp; \\delta_t \\doteq R_{t+1} + \\bar{R}_t + \\hat{v}(S_{t+1}, \\mathbf{w}_t) - \\hat{v}(S_t, \\mathbf{w}_t) \\tag{continuing}\\end{align*}\\]$\\bar{R}_t$는 average reward의 추정치이다.2action value에 대해서도 쉽게 전환할 수 있다. 아래는 off-policy semi-gradient Expected Sarsa이다.\\[\\begin{align*} &amp; \\mathbf{w}_{t+1} \\doteq \\mathbf{w}_t + \\alpha \\delta_t \\nabla \\hat{q}(S_t, A_t, \\mathbf{w_t}) \\text{, with} \\\\ \\\\ &amp; \\delta_t \\doteq R_{t+1} + \\gamma \\sum_a \\pi(a \\vert S_{t+1}) \\hat{q}(S_{t+1}, a, \\mathbf{w}_t) - \\hat{q}(S_t, A_t, \\mathbf{w}_t) \\text{, or} \\tag{episodic} \\\\ &amp; \\delta_t \\doteq R_{t+1} - \\bar{R}_t + \\sum_a \\pi(a \\vert S_{t+1}) \\hat{q}(S_{t+1}, a, \\mathbf{w}_t) - \\hat{q}(S_t, A_t, \\mathbf{w}_t) \\tag{continuing}\\end{align*}\\]이 알고리즘은 importance sampling을 사용하지 않는다. tabular case에서는 action value에 대한 1-step TD method에 importance sampling을 사용하지 않는 이유가 명확했다.3 그러나 function approximation에서는 명확하지 않다. 그 이유를 Sutton 책에서는 아래와 같이 설명한다. (솔직히 내가 이해 못했음) With function approximation it is less clear because we might want to weight different state-action pairs differently once they all contribute to the same overall approximation.4아래는 off-policy semi-gradient Sarsa의 $n$-step version이다.\\[\\begin{align*} &amp; \\mathbf{w}_{t+n} \\doteq \\mathbf{w}_{t+n-1} + \\alpha \\rho_{t+1} \\cdots \\rho_{t+n} [G_{t:t+n} - \\hat{q}(S_t, A_t, \\mathbf{w}_{t+n-1})] \\nabla \\hat{q}(S_t,A_t,\\mathbf{w}_{t+n-1}) \\text{, with} \\\\ \\\\ &amp; G_{t:t+n} \\doteq R_{t+1} + \\cdots + \\gamma^{n-1}R_{t+n} + \\gamma^n \\hat{q}(S_{t+n}, A_{t+n}, \\mathbf{w}_{t+n-1}) \\text{, or} \\tag{episodic} \\\\ &amp; G_{t:t+n} \\doteq R_{t+1} - \\bar{R}_t + \\cdots + R_{t+n} - \\bar{R}_{t+n-1} + \\hat{q}(S_{t+n}, A_{t+n}, \\mathbf{w}_{t+n-1}) \\tag{continuing}\\end{align*}\\]위 첫 번째 수식에서 $\\rho_k$는 $k \\geq T$ ($T$는 episode의 terminal step) 일 때 $1$이다. $t + n \\geq T$일 때 $G_{t:t+n} \\doteq G_t$이다. 세 번째 수식에서 on-policy semi-gradient $n$-step Sarsa의 경우 average reward가 오직 $\\bar{R}_{t+n-1}$이었는데 여기서는 다르다. off-policy 여서 다른 건지, 책이 오류인 건지 모르겠다.The Deadly Triadoff-policy learning에서 발생하는 불안정성과 발산 이슈는 아래 3가지 요소를 모두 결합할 때 발생한다. 이를 the deadly triad라고 부른다. Function approximation Bootstrapping Off-policy learningthe deadly triad 중 2가지 요소만 결합한다면 불안정성 이슈를 피할 수 있다. 따라서 위 3개 중 하나를 포기할 수 있다면 포기하는 것이 바람직하다.먼저, function approximation은 당연히 포기할 수 없다. function approximation은 매우 큰 state space를 효과적으로 처리할 수 있는 강력한 도구이기 때문이다.bootstrapping 없이 학습이 가능하다는 것은 이미 알려진 사실이다. 가장 대표적인 게 Monte Carlo (MC) Method이다. 그러나 bootstrapping method는 MC method에 비해 훨씬 강력하다. online으로 학습할 수 있기 때문에 훨씬 빠르고, 데이터를 효율적으로 사용하며, continuing task에 적용 가능하다. 또한 MC method는 일반적인 bootstrapping method에 비해 분산이 훨씬 크다. 따라서 bootstrapping 역시 포기하기는 어렵다.마지막으로 off-policy learning이다. 포기할 수 있을까? off-policy method는 target policy와 behavior policy가 다르다. 이는 어디까지나 편리한 거지, 반드시 필요한 것은 아니다. 그러나 더 큰 목표를 가진 강력한 AI를 만들기 위해서는 off-policy learning은 필수적이다. agent가 하나의 behavior policy로부터 선택된 action을 바탕으로 생성된 experience를 통해 여러 개의 target policy를 병렬적으로 학습할 수 있기 때문이다.Linear Value-function Geometry이 섹션에서는 value function approximation을 조금 더 추상적으로 이해해 볼 것이다. 먼저, state-value function $v : \\mathcal{S} \\rightarrow \\mathbb{R}$이 있다고 하자. 또한 function approximation을 사용하는 대부분의 경우, state 개수보다 weight vector $\\mathbf{w}$의 weight 개수가 훨씬 작다.예를 들어 3개의 state $\\mathcal{S} = \\{ s_1, s_2, s_3 \\}$와 2개의 weight $\\mathbf{w} = (w_1, w_2)^\\top$가 있다고 하자. 이 경우 value function $v = [v(s_1), v(s_2), v(s_3)]^\\top$를 3차원 공간 내의 점들로 표현되며, weight vector는 하위 공간인 2차원 공간으로 표현된다. 아래는 이에 대한 그림이다.Fig 1. The geometry of linear value-function approximation.(Image source: Sec 11.4 Sutton &amp; Barto (2020).)위 그림에서 true value function $v_\\pi$는 weight vector $\\mathbf{w}$에 의해 표현되는 2차원 공간보다 더 큰 공간에 (여기서는 3차원) 표현된다.어떤 고정된 policy $\\pi$를 고려해보자. true value function $v_\\pi$는 위 그림처럼 매우 복잡해서 weight vector $\\mathbf{w}$로 정확히 근사하는 것은 불가능하다. 즉, $v_\\pi$는 하위 공간에 존재하지 않는다. 그렇다면 표현 가능한 value function 중 어떤 것이 true value function에 가까울까?먼저, 두 value function 사이의 거리를 측정할 필요가 있다. 두 value function $v_1$, $v_2$가 주어졌졌으며 $v = v_1 - v_2$라고 하자. 또한 특정 state를 조금 더 정확히 추정하기 위해 그 state에 얼마나 집중할 지를 나타내는 state distribution $\\mu : \\mathcal{S} \\rightarrow [0,1]$를 (주로 on-policy distribution이 사용됨) 사용하자. 이 때 value function 사이의 distance를 아래와 같이 norm을 사용해 정의할 수 있다.\\[\\big\\lVert v \\big\\rVert_\\mu^2 \\doteq \\sum_{s \\in \\mathcal{S}} \\mu(s) v(s)^2\\]위 정의를 사용할 때 mean square value error $\\overline{\\text{VE}}(\\mathbf{w}) \\doteq \\sum_{s \\in \\mathcal{S}}\\mu(s)\\Big[v_\\pi(s) - \\hat{v}(s,\\mathbf{w}) \\Big]^2$를 $\\overline{\\text{VE}}(\\mathbf{w}) = \\lVert v_{\\mathbf{w}} - v_\\pi \\rVert_\\mu^2$와 같은 단순한 형태로 나타낼 수 있다.5 어떤 value function의 하위 공간에서의 가장 가까운 표현 가능한 value function은 projection 연산 $\\Pi$을 통해 찾을 수 있다.\\[\\Pi v \\doteq v_\\mathbf{w} \\text{ where } \\mathbf{w} = \\underset{\\mathbf{w} \\in \\mathbb{R}^d}{\\arg\\min} \\ \\big\\lVert v - v_\\mathbf{w} \\big\\rVert_\\mu^2\\]true value function $v_\\pi$에 가장 가까운 표현 가능한 value function은 $v_\\pi$의 projection $\\Pi v_\\pi$로 Fig 1에서 확인할 수 있다.여기서는 이정도 컨셉만 이해하고 넘어간다. 더 자세한 내용은 여기서 소개하기에는 너무 복잡해 궁금하다면 Sutton 책을 참조하기 바란다.6References[1] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction; 2nd Edition. 2020.Footnotes DevSlem. Monte Carlo Methods in RL. Importance Sampling. &#8617; DevSlem. On-policy Control with Approximation. Average Reward: A New Problem Setting for Continuing Tasks. Conversion to Average Reward Setting. &#8617; DevSlem. n-step Bootstrapping. $n$-step Off-policy Learning. &#8617; Reinforcement Learning: An Introduction; 2nd Edition. 2020. Sec. 11.1, p.259. &#8617; DevSlem. On-policy Prediction with Approximation. The Prediction Objective ($\\overline{\\text{VE}}$). &#8617; Reinforcement Learning: An Introduction; 2nd Edition. 2020. Sec. 11.4, p.266. &#8617; " }, { "title": "On-policy Control with Approximation", "url": "/reinforcement-learning/rl-fundamental/on-policy-control-with-approximation/", "categories": "Reinforcement Learning, RL Fundamental", "tags": "RL, AI, Function Approximation RL", "date": "2022-08-23 00:00:00 +0900", "snippet": "이 포스트에서는 function approximation을 사용한 prediction을 control로 확장할 것이다. 이를 위해 state-value function이 아닌 action-value function을 추정한다. 그 후 on-policy GPI의 일반적인 패턴을 따라 학습을 진행하는 방법을 알아본다. 이 포스트에서는 특히 semi-gradient TD(0)의 가장 기본적인 확장인 semi-gradient Sarsa 알고리즘을 다룰 것이다.Episodic Semi-gradient Controlfunction approximation을 통해 매개변수화된 action-value function은 $\\hat{q}(s,a,\\mathbf{w}) \\approx q_\\pi(s,a)$이며, 이 때 $\\mathbf{w} \\in \\mathbb{R}^d$는 $d$차원 weight 벡터이다. 또한 $S_t \\mapsto U_t$가 아닌 $S_t, A_t \\mapsto U_t$ 형식의 training example을 고려한다. $U_t$는 $q_\\pi(S_t,A_t)$의 어떤 근사값이든 가능하다. 이를 바탕으로 action-value prediction에 대한 일반적인 gradient-descent update는 아래와 같다.\\[\\mathbf{w}_{t+1} \\doteq \\mathbf{w}_t + \\alpha \\Big[ U_t - \\hat{q}(S_t,A_t,\\mathbf{w}_t) \\Big] \\nabla \\hat{q}(S_t,A_t,\\mathbf{w}_t)\\]위 update rule을 바탕으로 한 one-step Sarsa의 update rule은 아래와 같다.\\[\\mathbf{w}_{t+1} \\doteq \\mathbf{w}_t + \\alpha \\Big[ R_{t+1} + \\gamma \\hat{q}(S_{t+1},A_{t+1},\\mathbf{w}_t) - \\hat{q}(S_t,A_t,\\mathbf{w}_t) \\Big] \\nabla \\hat{q}(S_t,A_t,\\mathbf{w}_t)\\]위 방법을 episodic semi-gradient one-step Sarsa라고 부른다. continuing task에는 다른 방법이 사용된다.이제 control을 수행하자. 여기서는 continuous action이나 매우 많은 discrete action에 대해서는 다루지 않는다. 이는 아직까지 연구중인 굉장히 어려운 문제이기 때문이다. action이 discrete하고 그리 많지 않다면 $\\epsilon$-greedy policy와 같은 방법을 통해 policy improvement를 수행할 수 있다. 아래 박스는 전체 알고리즘이다. $\\text{Algorithm: Episodic Semi-gradient Sarsa for Estimating } \\hat{q} \\approx q_\\ast$ \\(\\begin{align*}&amp; \\textstyle \\text{Input: a differentiable action-value function parameterization } \\hat{q} : \\mathcal{S} \\times \\mathcal{A} \\times \\mathbb{R}^d \\rightarrow \\mathbb{R} \\\\&amp; \\textstyle \\text{Algorithm parameters: step size $\\alpha &gt; 0$, small $\\epsilon &gt; 0$} \\\\&amp; \\textstyle \\text{Initialize value-function weights $\\mathbf{w} \\in \\mathbb{R}^d$ arbitrarily (e.g., $\\mathbf{w} = \\mathbf{0}$)} \\\\\\\\&amp; \\textstyle \\text{Loop for each episode:} \\\\&amp; \\textstyle \\qquad S,A \\leftarrow \\text{initial state and action of episode (e.g., $\\epsilon$-greedy)} \\\\&amp; \\textstyle \\qquad \\text{Loop for each step of episode:} \\\\&amp; \\textstyle \\qquad\\qquad \\text{Take action $A$, observe } R, S' \\\\&amp; \\textstyle \\qquad\\qquad \\text{If $S'$ is terminal:} \\\\&amp; \\textstyle \\qquad\\qquad\\qquad \\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha \\big[R - \\hat{q}(S,A,\\mathbf{w}) \\big] \\nabla \\hat{q}(S,A,\\mathbf{w}) \\\\&amp; \\textstyle \\qquad\\qquad\\qquad \\text{Go to next episode} \\\\&amp; \\textstyle \\qquad\\qquad \\text{Choose $A'$ as a function of $\\hat{q}(S',\\cdot,\\mathbf{w})$ (e.g., $\\epsilon$-greedy)} \\\\&amp; \\textstyle \\qquad\\qquad \\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha \\big[R + \\gamma \\hat{q}(S',A',\\mathbf{w}) - \\hat{q}(S,A,\\mathbf{w}) \\big] \\nabla \\hat{q}(S,A,\\mathbf{w}) \\\\&amp; \\textstyle \\qquad\\qquad S \\leftarrow S' \\\\&amp; \\textstyle \\qquad\\qquad A \\leftarrow A' \\\\\\end{align*}\\)Semi-gradient $n$-step Sarsaepisodic semi-gradient Sarsa의 $n$-step version이다. 아래는 update target에 사용되는 $n$-step return으로 function approximation 형식으로 정의되었다.\\[G_{t:{t+n}} \\doteq R_{t+1} + \\gamma R_{t+2} + \\cdots + \\gamma^{n-1}R_{t+n} + \\gamma^n \\hat{q}(S_{t+n}, A_{t+n}, \\mathbf{w}_{t+n-1}), \\quad t + n &lt; T\\]$t+n \\geq T$일 때 $G_{t:t+n} \\doteq G_t$이다. $n$-step update rule은 아래와 같다.\\[\\mathbf{w}_{t+n} \\doteq \\mathbf{w}_{t+n-1} + \\alpha \\Big[ G_{t:t+n} - \\hat{q}(S_t,A_t,\\mathbf{w}_{t+n-1}) \\Big] \\nabla \\hat{q}(S_t,A_t,\\mathbf{w}_{t+n-1}), \\quad 0 \\leq t &lt; T\\]아래 박스는 전체 알고리즘이다. $\\text{Algorithm: Episodic semi-gradient $n$-step Sarsa for estimating } \\hat{q} \\approx q_\\ast \\text{ or } q_\\pi$ \\(\\begin{align*}&amp; \\textstyle \\text{Input: a differentiable action-value function parameterization } \\hat{q}: \\mathcal{S} \\times \\mathcal{A} \\times \\mathbb{R}^d \\rightarrow \\mathbb{R} \\\\&amp; \\textstyle \\text{Input: a policy $\\pi$ (if estimating $q_\\pi$)} \\\\&amp; \\textstyle \\text{Algorithm parameters: step size $\\alpha &gt; 0$, small $\\epsilon &gt; 0$, a positive integer $n$} \\\\&amp; \\textstyle \\text{Initialize value-function weights $\\mathbf{w} \\in \\mathbb{R}^d$ arbitrarily (e.g., $\\mathbf{w} = \\mathbf{0}$)} \\\\&amp; \\textstyle \\text{All store and access operations $(S_t, A_t, R_t)$ can take their index mod $n+1$} \\\\\\\\&amp; \\textstyle \\text{Loop for each episode:} \\\\&amp; \\textstyle \\qquad \\text{Initialize and store $S_0 \\neq$ terminal} \\\\&amp; \\textstyle \\qquad \\text{Select and store an action $A_0 \\sim \\pi(\\cdot \\vert S_0)$ or $\\epsilon$-greedy wrt $\\hat{q}(S_0,\\cdot,\\mathbf{w})$} \\\\&amp; \\textstyle \\qquad T \\leftarrow \\infty \\\\&amp; \\textstyle \\qquad \\text{Loop for } t = 0, 1, 2, \\ldots : \\\\&amp; \\textstyle \\qquad\\vert\\qquad \\text{If $t &lt; T$, then:} \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad \\text{Take action } A_t \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad \\text{Observe and store the next reward as $R_{t+1}$ and the next state as $S_{t+1}$} \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad \\text{If $S_{t+1}$ is terminal, then:} \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad\\qquad T \\leftarrow t + 1 \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad \\text{else:} \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad\\qquad \\text{Select and store $A_{t+1} \\sim \\pi(\\cdot \\vert S_{t+1})$ or $\\epsilon$-greedy wrt $\\hat{q}(S_{t+1},\\cdot,\\mathbf{w})$} \\\\&amp; \\textstyle \\qquad\\vert\\qquad \\tau \\leftarrow t - n + 1 \\qquad \\text{($\\tau$ is the time whose estimate is being updated)} \\\\&amp; \\textstyle \\qquad\\vert\\qquad \\text{If $\\tau \\geq 0$:} \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad G \\leftarrow \\sum_{i = \\tau + 1}^{\\min(\\tau + n, T)} \\gamma^{i - \\tau - 1}R_i \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad \\text{If $\\tau + n &lt; T$, then } G \\leftarrow G + \\gamma^n \\hat{q}(S_{\\tau + n}, A_{\\tau + n}, \\mathbf{w}) \\qquad (G_{\\tau : \\tau + n}) \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad \\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha \\big[G - \\hat{q}(S_\\tau, A_\\tau, \\mathbf{w}) \\big] \\nabla \\hat{q}(S_\\tau, A_\\tau, \\mathbf{w}) \\\\&amp; \\textstyle \\qquad \\text{Until } \\tau = T - 1 \\\\\\end{align*}\\)Average Reward: A New Problem Setting for Continuing Tasks우리가 그동안 다뤘던 setting은 discounted setting으로 delayed reward에 discount를 적용하는 setting이다. 이 setting은 function approximation을 continuing task에서 사용할 때 문제가 발생한다. 이로 인해 새로운 setting을 도입할 필요가 있다. 사실 tabular 방법을 사용할 때는 오히려 discounted setting이 효과적이다.1Average Rewardcontinuing task에 적용하기 위해 새롭게 도입할 setting은 average reward setting이다. 이 setting은 discounted reward가 존재하지 않으며 즉각적인 reward와 delayed reward를 동일하게 취급한다. 이 setting에서 policy $\\pi$에 대한 quality는 average reward $r(\\pi)$에 의해 정의된다.\\[\\begin{align*} r(\\pi) &amp;\\doteq \\lim_{h \\rightarrow \\infty} \\dfrac{1}{h} \\sum_{t=1}^h \\mathbb{E}[R_t \\ \\vert \\ S_0, A_{0:t-1} \\sim \\pi] \\\\ &amp;= \\lim_{t \\rightarrow \\infty} \\mathbb{E}[R_t \\ \\vert \\ S_0, A_{0:t-1} \\sim \\pi] \\\\ &amp;= \\sum_s \\mu_\\pi(s) \\sum_a \\pi(a \\vert s) \\sum_{s',r} p(s',r \\vert s,a)r\\end{align*}\\]기대값은 시작 state $S_0$와 policy $\\pi$에 따라 선택된 subsequent action $A_0, A_1, \\dots, A_{t-1}$을 조건으로 한다. MDP가 ergodic하다면 위 두번째와 세번째 수식이 성립한다. ergodic하다는게 무슨 의미일까? 이에 대해 알아보자.Ergodicityergodicity는 시간이 충분할 때 선택된 action들에 상관없이 모든 state가 방문됨을 의미한다. 따라서 $S_0$에 독립적인 steady-state distribution $\\mu_\\pi(s) \\doteq \\lim_{t \\rightarrow \\infty} \\Pr \\{ S_t=s \\ \\vert \\ A_{0:t-1} \\sim \\pi \\}$가 존재한다. distribution $\\mu_\\pi(s)$가 존재한다는 것은, 모든 state에 대해 어떤 action들이 선택되었든 간에 어떤 state $s$에 방문할 확률이 수렴했음을 의미이다.위 정의에 따라 ergodic MDP에서는 시작 state와 초기에 결정된 action들은 오직 일시적인 효과만 가진다. 장기적 관점에서는 어떤 state에 방문할 확률은 오직 policy와 MDP transition probability에 의해 결정된다. ergodicity는 $r(\\pi)$의 수렴에 대한 충분조건이지만 필요조건은 아니다.steady-state distribution $\\mu_\\pi$ 하에서 policy $\\pi$에 따라 action을 선택할 때, 어떤 state $s’$이 방문될 확률 $\\mu_\\pi(s’)$에 대해 아래와 같은 수식이 성립한다.\\[\\sum_s \\mu_\\pi(s) \\sum_a \\pi(a \\vert s)p(s' \\vert s,a) = \\mu_\\pi(s')\\]위 수식이 성립하는 이유는 앞서 언급했듯이 어떤 state에 방문할 확률은 오직 policy $\\pi$와 transition probability $p$에 의해 결정되기 때문임을 직관적으로 알 수 있다.Conversion to Average Reward Settingaverage reward setting에서 return은 reward와 average reward의 차이의 관점에서 정의된다.\\[G_t \\doteq R_{t+1} - r(\\pi) + R_{t+2} - r(\\pi) + R_{t+3} - r(\\pi) + \\cdots\\]이 return을 differential return이라고 하며, 이에 대응되는 value function을 differential value function이라고 한다. differential value function은 새로운 return만 사용할 뿐, 기존 value function과 동일한 관점에서 정의된다. 즉, differential value function $v_\\pi(s) \\doteq \\mathbb{E}_ \\pi[G_t \\vert S_t=s]$이고, $q_\\pi(s,a) \\doteq \\mathbb{E}_\\pi[G_t \\vert S_t=s, A_t=a]$이다. differential value function 역시 bellman equation을 가지나 약간의 차이가 있다. 먼저 discount rate $\\gamma$를 제거하고, 모든 reward를 reward와 실제 average reward와의 차이로 대체한다.\\[\\begin{align*} &amp; v_\\pi(s) = \\sum_a \\pi(a \\vert s) \\sum_{r,s'} p(s',r \\vert s,a)\\Big[r - r(\\pi) + v_\\pi(s')\\Big] \\\\ &amp; q_\\pi(s,a) = \\sum_{r,s'}p(s',r \\vert s,a)\\Big[r - r(\\pi) + \\sum_{a'} \\pi(a' \\vert s')q_\\pi(s', a')\\Big] \\\\ &amp; v_\\ast(s) = \\max_a \\sum_{r,s'} p(s',r \\vert s,a)\\Big[r - \\max_\\pi r(\\pi) + v_\\ast(s')\\Big] \\\\ &amp; q_\\ast(s,a) = \\sum_{r,s'} p(s',r \\vert s,a) \\Big[r - \\max_\\pi r(\\pi) + \\max_{a'} q_\\ast(s',a')\\Big]\\end{align*}\\]또한 TD error에 대한 differential한 형식 역시 정의할 수 있다.\\[\\begin{align*} &amp; \\delta_t \\doteq R_{t+1} - \\bar{R}_t + \\hat{v}(S_{t+1}, \\mathbf{w}_t) - \\hat{v}(S_t, \\mathbf{w}_t) \\\\ &amp; \\delta_t \\doteq R_{t+1} - \\bar{R}_t + \\hat{q}(S_{t+1}, A_{t+1}, \\mathbf{w}_t) - \\hat{q}(S_t, A_t, \\mathbf{w}_t)\\end{align*}\\]$\\bar{R}_t$는 average reward $r(\\pi)$의 time step $t$에서의 추정치이다. 이 정의들을 통해 기존 대부분의 알고리즘과 이론적 결과를 특별한 변화 없이 average reward setting으로 전환할 수 있다. 예를 들어 semi-gradient Sarsa의 average reward version은 단순히 TD error을 differential TD error로 전환하기만 하면 된다.\\[\\mathbf{w}_{t+1} \\doteq \\mathbf{w}_t + \\alpha \\delta_t \\nabla \\hat{q}(S_t, A_t, \\mathbf{w}_t)\\]$\\delta_t$는 action value에 대한 differential TD error이다. differential semi-gradient Sarsa에 대한 전체 알고리즘은 아래 박스와 같다. $\\text{Algorithm: Differential semi-gradient Sarsa for estimating } \\hat{q} \\approx q_\\ast$ \\(\\begin{align*}&amp; \\textstyle \\text{Input: a differentiable action-value function parameterization } \\hat{q} : \\mathcal{S} \\times \\mathcal{A} \\times \\mathbb{R}^d \\rightarrow \\mathbb{R} \\\\&amp; \\textstyle \\text{Algorithm parameters: step size $\\alpha, \\beta &gt; 0$, small $\\epsilon &gt; 0$} \\\\&amp; \\textstyle \\text{Initialize value-function weights $\\mathbf{w} \\in \\mathbb{R}^d$ arbitrarily (e.g., $\\mathbf{w}=\\mathbf{0}$)} \\\\&amp; \\textstyle \\text{Initialize average reward estimate $\\bar{R} \\in \\mathbb{R}$ arbitrarily (e.g., $\\bar{R} = 0$)} \\\\\\\\&amp; \\textstyle \\text{Initialize state $S$, and action $A$} \\\\&amp; \\textstyle \\text{Loop for each step:} \\\\&amp; \\textstyle \\qquad \\text{Take action $A$, observe $R, S'$} \\\\&amp; \\textstyle \\qquad \\text{Choose $A'$ as a function of $\\hat{q}(S', \\cdot, \\mathbf{w})$ (e.g., $\\epsilon$-greedy)} \\\\&amp; \\textstyle \\qquad \\delta \\leftarrow R - \\bar{R} + \\hat{q}(S',A',\\mathbf{w}) - \\hat{q}(S,A,\\mathbf{w}) \\\\&amp; \\textstyle \\qquad \\bar{R} \\leftarrow \\bar{R} + \\beta \\delta \\\\&amp; \\textstyle \\qquad \\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha \\delta \\nabla \\hat{q}(S,A,\\mathbf{w}) \\\\&amp; \\textstyle \\qquad S \\leftarrow S' \\\\&amp; \\textstyle \\qquad A \\leftarrow A' \\\\\\end{align*}\\)위 알고리즘은 differential value가 아닌 differential value에 임의의 offset이 더해진 값으로 수렴한다는 이슈가 있다. 그러나 bellman equation이나 TD error는 모든 값이 같은 양만큼 이동하더라도 (즉, 같은 offset이 더해지더라도) 영향을 받지 않는다. 따라서 실제로는 문제가 되지 않는다.Deprecating the Discounted Setting곧 추가될 예정.Differential Semi-gradient $n$-step Sarsa$n$-step return을 function approximation이 사용된 differential 형식으로 아래와 같이 바꿀 수 있다.\\[G_{t:t+n} \\doteq R_{t+1} - \\bar{R}_{t+n-1} + \\cdots + R_{t+n} - \\bar{R}_{t+n-1} + \\hat{q}(S_{t+n}, A_{t+n}, \\mathbf{w}_{t+n-1})\\]$\\bar{R}$는 $r(\\pi)$의 추정치이며, $n \\geq 1$, $t+n &lt; T$이다. $t+n \\geq T$이면 $G_{t:t+n} \\doteq G_t$이다. differential $n$-step TD error는 아래와 같다.\\[\\delta_t \\doteq G_{t:t+n} - \\hat{q}(S_t, A_t, \\mathbf{w})\\]아래 박스는 전체 알고리즘이다. $\\text{Algorithm: Differential semi-gradient $n$-step Sarsa for estimating $\\hat{q} \\approx q_\\pi$ or $q_\\ast$}$ \\(\\begin{align*}&amp; \\textstyle \\text{Input: a differentiable function $\\hat{q} : \\mathcal{S} \\times \\mathcal{A} \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}$, a policy $\\pi$} \\\\&amp; \\textstyle \\text{Initialize value-function weights $\\mathbf{w} \\in \\mathbb{R}^d$ arbitrarily (e.g., $\\mathbf{w} = \\mathbf{0}$)} \\\\&amp; \\textstyle \\text{Initialize average-reward estimate $\\bar{R} \\in \\mathbb{R}$ arbitrarily (e.g., $\\bar{R} = 0$)} \\\\&amp; \\textstyle \\text{Algorithm parameters: step size $\\alpha, \\beta &gt; 0$, small $\\epsilon &gt; 0$, a positive integer $n$} \\\\&amp; \\textstyle \\text{All store and access operations $(S_t,A_t,R_t)$ can take their index mode $n+1$} \\\\\\\\&amp; \\textstyle \\text{Initialize and store $S_0$, and $A_0$} \\\\&amp; \\textstyle \\text{Loop for each step, } t = 0, 1, 2, \\ldots : \\\\&amp; \\textstyle \\qquad \\text{Take action $A_t$} \\\\&amp; \\textstyle \\qquad \\text{Observe and store the next reward as $R_{t+1}$ and the next state as $S_{t+1}$} \\\\&amp; \\textstyle \\qquad \\text{Select and store an action $A_{t+1} \\sim \\pi(\\cdot \\vert S_{t+1})$, or $\\epsilon$-greedy wrt $\\hat{q}(S_{t+1}, \\cdot, \\mathbf{w})$} \\\\&amp; \\textstyle \\qquad \\tau \\leftarrow t - n + 1 \\qquad \\text{($\\tau$ is the time whose estimate is being updated)} \\\\&amp; \\textstyle \\qquad \\text{If $\\tau \\geq 0$:} \\\\&amp; \\textstyle \\qquad\\qquad \\delta \\leftarrow \\sum_{i=\\tau+1}^{\\tau+n} (R_i - \\bar{R}) + \\hat{q}(S_{\\tau+n}, A_{\\tau+n}, \\mathbf{w}) - \\hat{q}(S_\\tau, A_\\tau, \\mathbf{w}) \\\\&amp; \\textstyle \\qquad\\qquad \\bar{R} \\leftarrow \\bar{R} + \\beta \\delta \\\\&amp; \\textstyle \\qquad\\qquad \\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha \\delta \\nabla \\hat{q}(S_\\tau, A_\\tau, \\mathbf{w}) \\\\\\end{align*}\\)References[1] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction; 2nd Edition. 2020.Footnotes DevSlem. Finite Markov Decision Processes. Return. &#8617; " }, { "title": "On-policy Prediction with Approximation", "url": "/reinforcement-learning/rl-fundamental/on-policy-prediction-with-approximation/", "categories": "Reinforcement Learning, RL Fundamental", "tags": "RL, AI, Function Approximation RL", "date": "2022-08-18 00:00:00 +0900", "snippet": "이 포스트에서는 reinforcement learning을 수행하는 새로운 방법인 function approximation에 대한 소개와 이를 바탕으로 on-policy method에서 prediction을 수행하는 방법을 소개한다.What is Function Approximation and Why needed?지금까지 알아본 기존 Reinforcement Learning (RL) 방법들은 모두 tabular 기반이었다. tabular 기반 방법들은 작은 state space에서는 잘 작동하지만 매우 큰 state space에 적용할 때는 치명적인 몇가지 문제가 발생한다.먼저, state space가 매우 클 경우 각 state를 모두 mapping하는 데 너무 많은 메모리가 요구된다. 대부분의 RL task들은 가능한 state의 경우의 수가 우주에 존재하는 모든 원자 개수보다 많거나 실수 전체 집합에서 정의되어 무한개이다.두 번째는 이러한 많은 state space에 대해 완벽히 계산할 시간이 없다는 것이다. state space가 클 수록 state에 mapping되는 table은 더 커질 것이고 이에 따라 더 많은 데이터가 필요하고 계산되어야 한다. 이를 수행할만한 시간은 당연히 턱없이 부족하다. 따라서 대부분의 state는 아예 발견조차 하지 못할 때가 많다.더 좋은 학습을 위해선 아직 발견하지 못한 state에 대해서도 적절히 처리할 수 있어야 한다. 이를 위해 state를 일반화할 필요성이 있다. 즉, 주요 이슈는 generalization이다. 일반화를 달성할 수 있는 방법으로 function approximation을 사용한다. function approximation은 말 그대로 함수를 근사하는 방법이다. function approximation은 원래 supervised learning에서 사용되던 방법이다. 이를 RL에 적용해 state에 대한 function을 (e.g., value function, policy) 근사하는 것이 앞으로의 주요 과제이다.function approximation을 RL에 적용할 때 전통적인 supervised learning에서는 나타나지 않던 몇가지 문제가 발생한다. 대표적으로 nonstationarity, bootstrapping, delayed target이 있다. 이 문제를 포함한 function approximation을 RL에 적용할 때 발생하는 여러 문제들을 우리는 앞으로 다룰 것이다.Introduction이 포스트에서는 function approximation을 적용한 on-policy method에 집중할 것이다. on-policy method는 policy $\\pi$로부터 생성된 experience로부터 $\\pi$에 대한 함수를 추정하는 방법이었다. 여기서는 $\\pi$에 대한 state-value function $v_\\pi$를 근사하는 방법을 알아볼 것이다.기존 tabular 기반 방법과의 가장 주요한 차이점은 추정된 value function이 table이 아닌 weight vector $\\mathbf{w} \\in \\mathbb{R}^d$로 구성된 매개변수화된 함수로써 표현된다는 것이다. 따라서 어떤 weight vector $\\mathbf{w}$가 주어졌을 때 state $s$에 대한 근사값을 $\\hat{v}(s,\\mathbf{w}) \\approx v_\\pi(s)$로 나타낸다. 요즘 대부분의 RL은 weight vector $\\mathbf{w}$에 주로 신경망 매개변수를 사용한다. weight의 개수 ($\\mathbf{w}$의 차원)는 거의 대부분의 경우 state의 개수보다 훨씬 작다. 즉, $d \\ll \\vert \\mathcal{S} \\vert$이다.weight 하나를 변경하면 많은 state에 대한 추정치도 변하게 된다. 결과적으로 어떤 state 하나가 업데이트되면 다른 수많은 state의 value에도 영향을 미친다. 이를 통해 generalization이 가능해진다. 발견한 state에 대해 업데이트하면 아직 발견하지 못한 state의 value도 변경되기 때문이다. 그러나 다른 수많은 state의 value가 변경되기 때문에 관리하고 이해하기 어렵다는 점도 있다.Value-function Approximationvalue function을 업데이트하기 위해서는 이에 대한 target이 필요하다. target은 추정치가 다가가려는 지향점으로 생각할 수 있다. 우리는 앞선 여러 tabular 기반 방법에서 각 방법에 대한 target들을 알아봤었다. state $s$에서의 update target을 $u$라고 할 때 update를 아래와 같이 표현한다.\\[\\Large s \\mapsto u\\]Monte Carlo (MC) update는 $S_t \\mapsto G_t$, TD(0) update는 $S_t \\mapsto R_{t+1} + \\gamma \\hat{v}(S_{t+1},\\mathbf{w}_ t)$, $n$-step TD update는 $S_t \\mapsto G_{t:t+n}$이다. 또한 Dynamic Programming (DP)의 policy-evaluation update는 $s \\mapsto \\mathbb{E}_ \\pi [R_{t+1} + \\gamma \\hat{v}(S_{t+1},\\mathbf{w}_t) \\ \\vert \\ S_t=s]$ 이다.이러한 update를 training example로 사용함으로써 value prediction에 대한 function approximation을 수행할 것이다. 각 update를 training example로 간주하는 것은 이미 존재하는 여러 function approximation 방법을 사용할 수 있게 해준다. 하지만 모든 function approximation이 RL에 적합하지는 않다. 대부분의 신경망 기반 방법들은 static한 여러 개의 training example로 구성된 training set을 가정한다. RL에서는 agent가 environment와 상호작용하면서 online으로 학습하는 것이 중요하다. 따라서 점진적으로 획득한 data로부터 효과적으로 학습할 수 있는 방법이 필요하다. 게다가 RL에서는 일반적으로 function approximation 방법이 nonstationary한 target function을 (시간이 지남에 따라 target function이 변함) 다룰 수 있어야 한다. 일반적인 supervised learning은 target인 정답 레이블이 고정되어 있기 때문에 이러한 문제가 발생하지 않는다.The Prediction Objective ($\\overline{\\text{VE}}$)function approximation을 사용할 경우 발생하는 문제는 어떤 state에서의 update가 다른 state에도 영향을 미친다는 것이다. state의 개수는 weight의 개수보다 훨씬 많으며 이로 인해 한 state의 추정치를 정확하게 만들면 다른 나머지는 덜 정확해진다. 따라서 모든 state에 대한 정확한 값을 얻는 것은 불가능하다.위와 같은 이유로 우리는 어떤 state에 조금 더 집중할 지를 고민할 필요가 있다. 즉, 상대적으로 중요하다고 여겨지는 state를 더 정확히 추정한다. 어떤 state를 정확히 추정할 수록 그 state의 추정치 $\\hat{v}(s,\\mathbf{w})$와 실제값 $v_\\pi(s)$의 오차는 작을 것이다. 상대적으로 중요하다고 여겨지는 state의 오차를 더 많이 줄이고 싶다. 이를 위해 얼마나 그 state에 집중할 지를 나타내는 state distribution을 $\\mu(s) \\geq 0, \\sum_s \\mu(s) = 1$라고 하자. $\\mu$에 의해 state space에 가중치를 부여함으로써 오차에 대한 objective function을 아래와 같이 정의할 수 있다.\\[\\overline{\\text{VE}}(\\mathbf{w}) \\doteq \\sum_{s \\in \\mathcal{S}}\\mu(s)\\Big[v_\\pi(s) - \\hat{v}(s,\\mathbf{w}) \\Big]^2\\]위 objective function을 mean square value error라고 한다. 그러나 RL에서 $\\overline{\\text{VE}}$를 minimize한다고 해서 좋은 성능을 낸다고 말할 수는 없다. RL의 궁극적인 목적은 더 좋은 policy를 찾는 것이고, 이를 위해 value function을 학습하는 것이다.function approximation을 사용할 경우 $\\overline{\\text{VE}}$의 global optimum을 보장할 수 있을까? 간단한 linear method에 대해서는 보장되지만, 신경망과 같은 복잡한 function approximator의 경우 global optimum이 아닌 대게 weight vector $\\mathbf{w}^\\ast$의 근처에 있는 모든 $\\mathbf{w}$에 대해서만 $\\overline{\\text{VE}}(\\mathbf{w}^\\ast) \\leq \\overline{\\text{VE}}(\\mathbf{w})$를 만족하는 local optimum에 수렴하려고 한다. 그러나 이마저도 대부분의 강화학습에서는 수렴성에 대한 보장이 없으며 오히려 $\\overline{\\text{VE}}$가 발산하는 경우도 생긴다.Stochastic-gradient and Semi-gradient Methods이번 포스트의 핵심이다. Stochastic Gradient Descent (SGD)는 function approximation을 위한 가장 대표적인 학습 방법으로 online RL에도 잘 작동한다. SGD를 사용해 value prediction을 수행해보자.weight vector는 $d$개의 실수로 구성되어 있는 열벡터로 $\\mathbf{w} \\doteq (w_1,w_2,\\dots,w_d)^\\top$이다. approximate value function $\\hat{v}(s,\\mathbf{w})$는 모든 state $s \\in \\mathcal{S}$에서 $\\mathbf{w}$에 대해 미분가능한 함수이다. time step $t$마다 $\\mathbf{w}$를 업데이트하며 각 time step에서의 weight vector를 $\\mathbf{w}_t$로 나타낸다.각 time step마다 새로운 example $S_t \\mapsto v_\\pi(S_t)$를 관찰한다고 하자. 또한 동일한 distribution $\\mu$를 가진 state가 example에 나타난다고 하자. 이 때 관찰된 example에 대해 error $\\overline{\\text{VE}}$를 minimize하기 위해 SGD method를 사용해 weight vector를 error를 줄이는 방향으로 조정한다. SGD는 gradient descent method로 기울기의 반대 방향으로 약간 이동해 함수를 minimize하는 최적화 방법이다. 업데이트가 확률적으로 선택된 단일 example에 대해 수행될 때 gradient descent method를 “stochastic”하다고 부른다. 아래는 이에 대한 수식이다.\\[\\begin{align} \\mathbf{w}_{t+1} &amp;\\doteq \\mathbf{w}_t - \\dfrac{1}{2}\\alpha \\nabla \\Big[ v_\\pi(S_t) - \\hat{v}(S_t, \\mathbf{w}_t) \\Big]^2 \\\\ &amp;= \\mathbf{w}_t + \\alpha \\Big[v_\\pi(S_t) - \\hat{v}(S_t, \\mathbf{w}_t) \\Big] \\nabla \\hat{v}(S_t, \\mathbf{w}_t)\\end{align}\\]$\\alpha$는 양수인 step-size parameter이다. $\\nabla f(\\mathbf{w})$는 gradient vector로 함수 $f(\\mathbf{w})$를 편미분한 열벡터이다.\\[\\nabla f(\\mathbf{w}) \\doteq \\bigg(\\dfrac{\\partial f(\\mathbf{w})}{\\partial w_1}, \\dfrac{\\partial f(\\mathbf{w})}{\\partial w_2}, \\cdots , \\dfrac{\\partial f(\\mathbf{w})}{\\partial w_d} \\bigg)^\\top\\]기울기의 반대 방향으로 약간만 이동하는 이유는 각기 다른 state들에 대한 error의 균형을 맞추는 근사를 하기 위해서이다. error를 완전히 없애는 value function을 찾는 것이 목적이 아닐 뿐더러, 모든 state에 대한 정확한 실제값을 알고 있더라도, $d \\ll \\vert \\mathcal{S} \\vert$인 weight vector $\\mathbf{w}$의 제한된 차원으로 인해 error를 완전히 없애는 것은 불가능하다. 또한 SGD는 $\\alpha$가 시간에 따라 감소한다고 가정하며 이 경우 local optimum으로의 수렴이 보장된다. 따라서 약간만 이동하는 것이 바람직하다.General-gradient MethodRL은 supervised learning과 다르게 정답 레이블이 존재하지 않는 unsupervised learning이다. 따라서 target의 실제값 $v_\\pi(S_t)$를 알지 못한다. t번째 training example의 실제 값이 아닌 target을 $U_t \\in \\mathbb{R}$라고 하자. 즉, $S_t \\mapsto U_t$이다. $v_\\pi(S_t)$를 $U_t$로 대체하더라도 여전히 SGD를 통해 근사화가 가능하다.\\[\\mathbf{w}_{t+1} \\doteq \\mathbf{w}_t + \\alpha \\Big[U_t - \\hat{v}(S_t, \\mathbf{w}_t) \\Big] \\nabla \\hat{v}(S_t, \\mathbf{w}_t)\\]만약 $U_t$가 unbiased 추정치라면, 즉, $\\mathbb{E}[U_t \\vert S_t=s] = v_\\pi(s)$라면 $\\mathbf{w}_ t$는 local optimum으로 수렴됨이 보장된다. 대표적으로 Monte Carlo target $U_t \\doteq G_t$는 $v_\\pi(S_t)$의 unbiased 추정치이기 때문에 위 조건을 만족한다. 아래 박스는 Monte Carlo 방법을 사용한 value prediction 알고리즘이다. $\\text{Algorithm: Gradient Monte Carlo Algorithm for Estimating $\\hat{v} \\approx v_\\pi$}$ \\(\\begin{align*}&amp; \\textstyle \\text{Input: the policy $\\pi$ to be evaluated} \\\\&amp; \\textstyle \\text{Input: a differentiable function } \\hat{v} : \\mathcal{S} \\times \\mathbb{R}^d \\rightarrow \\mathbb{R} \\\\&amp; \\textstyle \\text{Algorithm parameter: step size } \\alpha &gt; 0 \\\\&amp; \\textstyle \\text{Initialize value-function weights $\\mathbf{w} \\in \\mathbb{R}^d$ arbitrarily (e.g., $\\mathbf{w}=\\mathbf{0}$)} \\\\\\\\&amp; \\textstyle \\text{Loop forever (for each episode):} \\\\&amp; \\textstyle \\qquad \\text{Generate an episode } S_0, A_0, R_1, S_1, A_1, \\dots, R_T, S_T \\text{ using } \\pi \\\\&amp; \\textstyle \\qquad \\text{Loop for each step of episode, } t = 0, 1, \\dots, T-1 \\text{:} \\\\&amp; \\textstyle \\qquad\\qquad \\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha [G_t - \\hat{v}(S_t, \\mathbf{w})] \\nabla \\hat{v}(S_t, \\mathbf{w}) \\\\\\end{align*}\\)Semi-gradient MethodMC method와는 다르게 TD method나 DP는 bootstrapping을 통해 target을 근사화한다. bootstrapping target은 weight vector $\\mathbf{w}_ t$에 의한 현재 추정치에 의존한다. 따라서 bootstrapping target은 biased해 unbiased target과 같은 local optimum으로의 수렴이 보장되지 않는다.bootstrapping 방법은 추정치에 대해서 weight vector $\\mathbf{w}_ t$를 변경하는 것은 고려하지만, target에 대해서는 고려하지 않는다. 즉, gradient의 일부분만을 포함하기 때문에 semi-gradient method라고 부른다.수렴성에 대한 보장이 없음에도 불구하고 bootstrapping 방법은 MC method보다 더 선호된다. MC method와 달리 episode의 종료를 기다릴 필요가 없어 continuing task에도 적용할 수 있고, online으로 학습이 가능해 훨씬 빠른 학습 속도를 가진다.아래 박스는 가장 대표적인 semi-gradient method인 semi-gradient TD(0)의 알고리즘이다. $\\text{Algorithm: Semi-gradient TD(0) for estimating } \\hat{v} \\approx v_\\pi$ \\(\\begin{align*}&amp; \\textstyle \\text{Input: the policy $\\pi$ to be evaluated} \\\\&amp; \\textstyle \\text{Input: a differntiable function $\\hat{v} : \\mathcal{S}^+ \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}$ such that } \\hat{v}(\\text{terminal}, \\cdot) = 0 \\\\&amp; \\textstyle \\text{Algorithm parameter: step size } \\alpha &gt; 0 \\\\&amp; \\textstyle \\text{Initialize value-function weights $\\mathbf{w} \\in \\mathbb{R}^d$ arbitrarily (e.g., $\\mathbf{w}=\\mathbf{0}$)} \\\\\\\\&amp; \\textstyle \\text{Loop for each episode:} \\\\&amp; \\textstyle \\qquad \\text{Initialize } S \\\\&amp; \\textstyle \\qquad \\text{Loop for each step of episode:} \\\\&amp; \\textstyle \\qquad\\qquad \\text{Choose } A \\sim \\pi(\\cdot \\vert S) \\\\&amp; \\textstyle \\qquad\\qquad \\text{Take action $A$, observe } R, S' \\\\&amp; \\textstyle \\qquad\\qquad \\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha [R + \\gamma \\hat{v}(S', \\mathbf{w}) - \\hat{v}(S, \\mathbf{w})] \\nabla \\hat{v}(S,\\mathbf{w}) \\\\&amp; \\textstyle \\qquad\\qquad S \\leftarrow S' \\\\&amp; \\textstyle \\qquad \\text{until $S$ is terminal} \\\\\\end{align*}\\)위 알고리즘을 보면 왜 semi-gradient method인지 알 수 있다. TD(0)의 target은 $U_t \\doteq R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\mathbf{w_t})$이다. weight vector $\\mathbf{w}_ t$에 의한 next state의 value 추정치를 사용해 target이 구성되고 error를 구하지만, gradient vector는 target이 아닌 현재 state의 추정치만을 고려하고 있음을 알 수 있다.Nonlinear Function Approximation: Artificial Neural NetworksArtificial Neural Network (ANN)은 nonlinear function approximation에 널리 사용되는 방법이다. ANN은 뉴런이 서로 연결되어있는 네트워크이다. 아래 그림은 일반적인 feedforward ANN을 나타낸다. 4개의 입력 유닛으로 구성된 input layer와 2개의 출력 유닛으로 구성된 output layer, 2개의 “hidden layer”로 구성되어있다.Fig 1. A generic feedforward ANN with 4 input units, 2 output units, and 2 hidden layers.(Image source: Sec 9.7 Sutton &amp; Barto (2020).)각각의 weight는 각 링크(그림에서 화살표)에 대응된다. 각 유닛은 semi-linear 유닛으로 입력 신호의 weighted sum을 구하는 linear 연산 후, nonlinear function인 activation function에 적용하여 유닛의 출력인 activation을 얻는다. 중요한 점은 activation function은 반드시 nonlinear function이어야 한다.feedforward ANN의 각 출력 유닛의 activation은 nonlinear function이다. 이 함수는 network의 연결 weight에 의해 매개변수화된다. hidden layer가 없는 ANN은 가능한 함수의 굉장히 작은 부분만 표현할 수 있다. 그러나 충분히 큰 개수의 유닛을 가진 hidden layer가 있을 경우 network의 input space에 대해 좁은 영역에서 어떤 연속 함수든지 근사화할 수 있다.ANN은 일반적으로 stochastic gradient method에 의해 학습되며, 각 weight는 minimize하거나 maximize할 objective function에 의해 측정된 네트워크의 전체적인 성능을 향상시키는 방향으로 조정된다. 일반적인 supervised learning의 경우 objective function은 training example에 대한 expected error이다. 반면 RL에서는 value function을 학습하기 위한 TD error, maximizing expected reward, policy-gradient algorithm 등 여러 종류의 objective function을 사용한다.복잡한 구조의 ANN을 효과적으로 미분하는 방법은 그 유명한 backpropagation algorithm이다. backpropagation algorithm은 1개나 2개의 hidden layer로 구성된 얕은 network에 대해서는 상당히 좋은 결과를 낸다. 그러나 보다 더 깊은 deep ANN에서는 한계가 있다. 그 이유는 먼저, 굉장히 많은 weight로 구성된 deep ANN은 overfitting 문제를 피하기 어렵다는 점이다. overfitting은 아직 훈련하지 않은 case에 대해 일반화하는 데 실패하는 문제이다. 두번째는, backpropagation을 통해 계산된 편미분 값이 입력층에 도달할 수록 가파르게 감소해 학습이 극도로 느려지거나, 가파르게 증가해 학습을 불안정하게 만들기 때문이다. 그러나 최근에는 이 문제를 다루는 여러 방법들이 많이 나와 hidden layer가 굉장히 많은 매우 깊은 network도 잘 학습한다.References[1] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction; 2nd Edition. 2020." }, { "title": "Planning and Learning with Tabular Methods", "url": "/reinforcement-learning/rl-fundamental/planning-and-learning-with-tabular-methods/", "categories": "Reinforcement Learning, RL Fundamental", "tags": "RL, AI", "date": "2022-08-16 00:00:00 +0900", "snippet": "이 포스트는 나중에 업데이트 될 예정입니다." }, { "title": "n-step Bootstrapping", "url": "/reinforcement-learning/rl-fundamental/n-step-bootstrapping/", "categories": "Reinforcement Learning, RL Fundamental", "tags": "RL, AI", "date": "2022-07-19 00:00:00 +0900", "snippet": "이 포스트에서는 TD method의 확장된 형태인 $n$-step TD methods를 간략히 소개한다.What is $n$-step TD method$n$-step TD method는 1-step TD method와 Monte Carlo (MC) method를 통합한 방법이다. $n$-step TD method는 일종의 스펙트럼으로 양 끝단에 각각 1-step TD와 MC method가 존재한다. 실제 1-step TD나 MC method가 항상 좋은 performance를 내는건 아니다. 따라서 보다 일반적인 n-step TD method에 대해 알아둘 필요가 있다.이전 포스트에서 TD method는 MC method와 Dynamic Programming (DP)의 아이디어를 결합한 방법이라고 소개했었다. $n$-step method 역시 동일하다. 즉, sampling과 bootstrapping을 통해 training이 이루어진다. 다만 1-step TD와의 차이점은 bootstrapping이 이루어지는 time step이 1개가 아니라 여러 개일 뿐이다.$n$-step TD PredictionMC method는 episode를 완전히 고려하기 때문에 전체 reward를 알고 있어 실제 return을 구할 수 있었다.\\[G_t \\doteq R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots + \\gamma^{T-t-1}R_T\\]$T$는 episode의 last time step이다. 그런데 1-step TD method는 episode 전체가 아니라 sampling된 단 하나의 transition 고려하기 때문에 next reward $R_{t+1}$만 구할 수 있다. 따라서 아직 sampling 되지 않은 time step들의 discounted reward들을 아래와 같이 next state에서의 추정치로 근사해 처리한다.\\[G_{t:t+1} \\doteq R_{t+1} + \\gamma V_t(S_{t+1})\\]$G_{t:t+1}$은 time step $t$에서 $t+1$까지의 transition이 발생했을 때의 target으로 1-step return이라고 한다. 참고로 MC method의 target은 실제 return $G_t$이다. 우리는 위 1-step TD method의 target을 $n$-step으로 확장할 것이다. 먼저 아래 backup diagram을 보자.Fig 1. Backup diagrams of $n$-step methods.(Image source: Sec 7.1 Sutton &amp; Barto (2018).)위 backup diagram을 보면 알 수 있지만 1-step TD 수행 시 실제 reward는 $R_{t+1}$만 획득할 수 있다. 2-step TD 수행 시 $R_{t+1}$과 $R_{t+2}$만 획득할 수 있다. 획득하지 못한 나머지 reward는 기존에 학습된 추정치로 대체한다. 즉, 2-step TD 수행 시 $S_{t+2}$에 위치할 것이기 때문에 $V(S_{t+2})$를 사용한다. 아래는 2-step TD method의 target이다.\\[G_{t:t+2} \\doteq R_{t+1} + \\gamma R_{t+2} + \\gamma^2 V_{t+1}(S_{t+2})\\]이를 $n$-step TD method로 일반화하면 아래와 같다.\\[G_{t:t+n} \\doteq R_{t+1} + \\gamma R_{t+2} + \\cdots + \\gamma^{n-1} R_{t+n} + \\gamma^n V_{t+n-1}(S_{t+n})\\]위 $n$-step TD method의 target을 $n$-step return이라고 한다. 정리하면 $n$-step return은 실제 return의 근사치로, $n$-step 이후 잘려진 뒤 $V_{t+n-1}(S_{t+n})$에 의해 보정된 값이다. 이 때 $t + n \\geq T$일 경우, episode의 termination을 넘어갔기 때문에 보정된 값은 0이 되어 실제 return이 된다.\\[G_{t:t+n} \\doteq G_t, \\quad \\text{if } t + n \\geq T\\]$n$-step return은 $n$-step 이후의 $R_{t+n}$과 이전에 계산된 $V_{t+n-1}$이 발견이 되어야만 계산할 수 있다. 따라서 $t+n$ time step 이후에만 $G_{t:t+n}$을 이용할 수 있다. 아래는 $n$-step return을 사용해 state value를 추정하는 prediction update rule이다.\\[V_{t+n}(S_t) \\doteq V_{t+n-1}(S_t) + \\alpha [G_{t:t+n} - V_{t+n-1}(S_t)]\\]이 때 $s \\neq S_t$인 모든 state의 value는 변하지 않는다. 위 update rule이 $n$-step TD이다. episode의 첫 $n-1$ step까지는 어떤 변화도 발생하지 않는다는 사실을 꼭 기억하길 바란다. $\\text{Algorithm: $n$-step TD for estimating } V \\approx v_\\pi$ \\(\\begin{align*}&amp; \\textstyle \\text{Input: a policy } \\pi \\\\&amp; \\textstyle \\text{Algorithm parameters: step size } \\alpha \\in (0, 1] \\text{, a positive integer } n \\\\&amp; \\textstyle \\text{Initialize } V(s) \\text{ arbitrarily, for all } s \\in \\mathcal{S} \\\\&amp; \\textstyle \\text{All store and access operations (for } S_t \\text{ and } R_t \\text{) can take their index mod } n+1 \\\\\\\\&amp; \\textstyle \\text{Loop for each episode:} \\\\&amp; \\textstyle \\qquad \\text{Initialize and store } S_0 \\neq \\text{terminal} \\\\&amp; \\textstyle \\qquad T \\leftarrow \\infty \\\\&amp; \\textstyle \\qquad \\text{Loop for } t = 0, 1, 2, \\dotso : \\\\&amp; \\textstyle \\qquad\\qquad \\text{If } t &lt; T \\text{, then:} \\\\&amp; \\textstyle \\qquad\\qquad\\qquad \\text{Take an action according to } \\pi(\\cdot \\vert S_t) \\\\&amp; \\textstyle \\qquad\\qquad\\qquad \\text{Observe and store the next reward as } R_{t+1} \\text{ and the next state as } S_{t+1} \\\\&amp; \\textstyle \\qquad\\qquad\\qquad \\text{If } S_{t+1} \\text{ is terminal, then } T \\leftarrow t + 1 \\\\&amp; \\textstyle \\qquad\\qquad \\tau \\leftarrow t - n + 1 \\qquad \\text{($\\tau$ is the time whose state's estimate is being updated)} \\\\&amp; \\textstyle \\qquad\\qquad \\text{If $\\tau \\geq 0$:} \\\\&amp; \\textstyle \\qquad\\qquad\\qquad G \\leftarrow \\sum_{i=\\tau + 1}^{\\min(\\tau + n, T)} \\gamma^{i - \\tau - 1}R_i \\\\&amp; \\textstyle \\qquad\\qquad\\qquad \\text{If } \\tau + n &lt; T \\text{, then: } G \\leftarrow G + \\gamma^n V(S_{\\tau + n}) \\qquad (G_{\\tau : \\tau + n}) \\\\&amp; \\textstyle \\qquad\\qquad\\qquad V(S_\\tau) \\leftarrow V(S_\\tau) + \\alpha [G - V(S_\\tau)] \\\\&amp; \\textstyle \\qquad \\text{until } \\tau = T - 1\\end{align*}\\)위 알고리즘을 보면 $n$-step transition이 발생할 때는 $t = n-1$일 때로 이 때 부터 value function의 update가 수행된다.$n$-step Sarsa이제 $n$-step method의 control에 대해 알아보자. 가장 먼저 알아볼 것은 n-step Sarsa이다. Sarsa는 on-policy method이며 여기서는 단지 1-step이 아닌 $n$-step으로 확장했을 뿐이다.핵심 아이디어는 MC나 1-step TD method와 동일하게 state value 추정을 action value 추정으로 전환하는 것이다. 이에 따라 모든 시작과 끝은 state가 아니라 action이 된다. 아래는 $n$-step Sarsa에 대한 backup diagram이다.Fig 2. Backup diagrams of $n$-step Sarsa.(Image source: Sec 7.2 Sutton &amp; Barto (2018).)이제 $n$-step Sarsa에서 사용하기 위한 $n$-step return을 action value에 관해 정의해보자. 단지, $n$-step TD Prediction의 $n$-step return에서 $V$를 $Q$로 바꿔주기만 하면 된다.\\[G_{t:t+n} \\doteq R_{t+1} + \\gamma R_{t+2} + \\cdots + \\gamma^{n-1} R_{t+n} + \\gamma^n Q_{t+n-1}(S_{t+n}, A_{t+n}), \\quad n \\geq 1, 0 \\leq t &lt; T - n\\]단, $t + n \\geq T$일 때 $G_{t:t+n} \\doteq G_t$이다. 위 $n$-step return for Sarsa를 바탕으로 $n$-step Sarsa의 update rule을 정의해보자.\\[Q_{t+n}(S_t,A_t) \\doteq Q_{t+n-1}(S_t,A_t) + \\alpha [G_{t:t+n} - Q_{t+n-1}(S_t, A_t)], \\quad 0 \\leq t &lt; T\\]역시 마찬가지로 $s \\neq S_t$ or $a \\neq A_t$인 모든 $s, a$에 대한 value function은 변하지 않는다.아래는 1-step Sarsa와 $n$-step Sarsa를 비교하는 그림이다. 파란색 화살표는 목표 G에 도달했을 떄 증가하는 action value를 나타낸다.Fig 3. Comparison of 1-step and $n$-step Sarsa.(Image source: Sec 7.2 Sutton &amp; Barto (2018).)위 그림을 보면 알 수 있지만 목표 G에 도달했을 때 1-step Sarsa는 바로 직전 state에서의 action value만 증가하지만 $n$-step Sarsa는 sequence의 마지막 $n$개의 action만큼 증가한다. 이로 인해 하나의 episode로부터 더 많은 학습이 가능해진다.아래는 $n$-step Sarsa의 알고리즘이다. $\\text{Algorithm: $n$-step Sarsa for estimating } Q \\approx q_\\ast \\text{ or } q_\\pi$ \\(\\begin{align*}&amp; \\textstyle \\text{Initialize } Q(s,a) \\text{ arbitrarily, for all } s \\in \\mathcal{S}, a \\in \\mathcal{A} \\\\&amp; \\textstyle \\text{Initialize $\\pi$ to be $\\epsilon$-greedy with respect to $Q$, or to a fixed given policy} \\\\&amp; \\textstyle \\text{Algorithm parameters: step size } \\alpha \\in (0, 1] \\text{, small $\\epsilon &gt; 0$, a positive integer $n$} \\\\&amp; \\textstyle \\text{All store and access operations (for $S_t$, $A_t$, and $R_t$) can take their index mod $n+1$} \\\\\\\\&amp; \\textstyle \\text{Loop for each episode}: \\\\&amp; \\textstyle \\qquad \\text{Initialize and store } S_0 \\neq \\text{terminal} \\\\&amp; \\textstyle \\qquad \\text{Select and store an action } A_0 \\sim \\pi(\\cdot \\vert S_0) \\\\&amp; \\textstyle \\qquad T \\leftarrow \\infty \\\\&amp; \\textstyle \\qquad \\text{Loop for } t = 0, 1, 2, \\dotso \\text{:} \\\\&amp; \\textstyle \\qquad\\vert\\qquad \\text{If $t &lt; T$, then:} \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad \\text{Take action } A_t \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad \\text{Observe and store the next reward as $R_{t+1}$ and the next state as $S_{t+1}$} \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad \\text{If $S_{t+1}$ is terminal, then:} \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad\\qquad T \\leftarrow t + 1 \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad \\text{else:} \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad\\qquad \\text{Select and store an action } A_{t+1} \\sim \\pi(\\cdot \\vert S_{t+1}) \\\\&amp; \\textstyle \\qquad\\vert\\qquad \\tau \\leftarrow t - n + 1 \\qquad (\\tau \\text{ is the time whose estimate is being updated}) \\\\&amp; \\textstyle \\qquad\\vert\\qquad \\text{If $\\tau \\geq 0$:} \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad G \\leftarrow \\sum_{i = \\tau + 1}^{\\min(\\tau + n, T)} \\gamma^{i - \\tau - 1}R_i \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad \\text{If } \\tau + n &lt; T \\text{, then } G \\leftarrow G + \\gamma^n Q(S_{\\tau + n}, A_{\\tau + n}) \\qquad (G_{\\tau : \\tau + n}) \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad Q(S_\\tau, A_\\tau) \\leftarrow Q(S_\\tau, A_\\tau) + \\alpha [G - Q(S_\\tau, A_\\tau)] \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad \\text{If $\\pi$ is being learned, then ensure that $\\pi(\\cdot \\vert S_\\tau)$ is $\\epsilon$-greedy wrt $Q$} \\\\&amp; \\textstyle \\qquad \\text{until } \\tau = T - 1 \\\\\\end{align*}\\)$n$-step Expected Sarsa$n$-step Expected Sarsa 역시 어렵지 않게 구할 수 있다.. Fig.2를 보면 알 수 있지만 Sarsa와 동일하게 진행되나 마지막 $n$-step에서 모든 action value에 대한 expected value를 고려한다. 아래는 임의의 state $s$에서 action value 추정치를 통해 획득할 수 있는 expected approximate value이다.\\[\\bar{V}_t(s) \\doteq \\sum_a \\pi(a \\vert s)Q_t(s,a), \\quad \\text{for all } s \\in \\mathcal{S}\\]이 때 $\\pi$는 target policy이다. 위 수식을 바탕으로 Expected Sarsa에 대한 $n$-step return을 구할 수 있다.\\[G_{t:t+n} \\doteq R_{t+1} + \\cdots + \\gamma^{n-1}R_{t+n} + \\gamma^n \\bar{V}_{t+n-1}(S_{t+n}), \\quad t + n &lt; T\\]위 수식을 바탕으로 $n$-step Sarsa 알고리즘을 약간만 변형하면 $n$-step Expected Sarsa 알고리즘을 쉽게 구할 수 있다.$n$-step Off-policy Learningoff-policy learning은 behavior policy $b$에 따라 sampling 한 뒤, target policy $\\pi$에 대한 value function을 학습 하는 방법이다. off-policy method에서 behavior policy와 target policy의 distribution이 다르기 때문에 importance-sampling 기법을 사용했었다. importance-sampling ratio는 어떤 trajectory의 action들을 두 policy에 따라 선택할 상대적 확률이다.1\\[\\rho_{t:h} \\doteq \\prod_{k=t}^{\\min(h, T-1)} \\dfrac{\\pi(A_k \\vert S_k)}{b(A_k \\vert S_k)}\\]아래는 $S_t$가 주어져있을 때 $n$-step transition으로 생성된 trajectory이다.\\[A_t, S_{t+1}, A_{t+1}, \\dots, A_{t+n-1}, S_{t+n}\\]이 때 off-policy에서는 아래와 같이 importance-sampling ratio $\\rho_{t:t+n-1}$을 통해 $n$-step TD error에 가중치를 부여하여 update를 수행할 수 있다.\\[V_{t+n}(S_t) \\doteq V_{t+n-1}(S_t) + \\alpha \\rho_{t:t+n-1}[G_{t:t+n} - V_{t+n-1}(S_t)], \\quad 0 \\leq t &lt; T\\]이를 바탕으로 off-policy $n$-step Sarsa를 정의할 수 있다. 마찬가지로 state value를 action value로 전환하면 된다. 다만 importance-sampling ratio을 고려할 때 주의할 점이 있다. state value 추정과 action value 추정 시의 trajectory가 달라 importance-sampling ratio가 다르다. action value를 추정할 때는 state action pair $S_t, A_t$가 주어져 있으며, Sarsa이기 때문에 trajectory의 마지막 state에서도 action을 선택한다. 아래는 Sarsa의 $n$-step transition trajectory이다.\\[S_{t+1}, A_{t+1}, \\dots, A_{t+n-1}, S_{t+n}, A_{t+n}\\]따라서 우리가 고려해야할 importance-sampling ratio는 $\\rho_{t+1:t+n}$이다. 아래는 off-policy Sarsa의 update rule이다.\\[Q_{t+n}(S_t, A_t) \\doteq Q_{t+n-1}(S_t, A_t) + \\alpha \\rho_{t+1:t+n}[G_{t:t+n} - Q_{t+n-1}(S_t, A_t)], \\quad 0 \\leq t &lt; T\\]importance-sampling ratio는 subsequent action에 대해서만 고려한다는 사실을 꼭 기억하길 바란다. 아래는 off-policy $n$-step Sarsa 알고리즘이다. $\\text{Algorithm: Off-policy $n$-step Sarsa for estimating } Q \\approx q_\\ast \\text{ or } q_\\pi$ \\(\\begin{align*}&amp; \\textstyle \\text{Input: an arbitrary behavior policy $b$ such that $b(a \\vert s) &gt; 0$, for all } s \\in \\mathcal{S}, a \\in \\mathcal{A} \\\\&amp; \\textstyle \\text{Initialize } Q(s,a) \\text{ arbitrarily, for all } s \\in \\mathcal{S}, a \\in \\mathcal{A} \\\\&amp; \\textstyle \\text{Initialize $\\pi$ to be greedy with respect to $Q$, or as a fixed given policy} \\\\&amp; \\textstyle \\text{Algorithm parameters: step size } \\alpha \\in (0, 1] \\text{, a positive integer $n$} \\\\&amp; \\textstyle \\text{All store and access operations (for $S_t$, $A_t$, and $R_t$) can take their index mod $n+1$} \\\\\\\\&amp; \\textstyle \\text{Loop for each episode}: \\\\&amp; \\textstyle \\qquad \\text{Initialize and store } S_0 \\neq \\text{terminal} \\\\&amp; \\textstyle \\qquad \\text{Select and store an action } A_0 \\sim b(\\cdot \\vert S_0) \\\\&amp; \\textstyle \\qquad T \\leftarrow \\infty \\\\&amp; \\textstyle \\qquad \\text{Loop for } t = 0, 1, 2, \\dotso \\text{:} \\\\&amp; \\textstyle \\qquad\\vert\\qquad \\text{If $t &lt; T$, then:} \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad \\text{Take action } A_t \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad \\text{Observe and store the next reward as $R_{t+1}$ and the next state as $S_{t+1}$} \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad \\text{If $S_{t+1}$ is terminal, then:} \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad\\qquad T \\leftarrow t + 1 \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad \\text{else:} \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad\\qquad \\text{Select and store an action } A_{t+1} \\sim b(\\cdot \\vert S_{t+1}) \\\\&amp; \\textstyle \\qquad\\vert\\qquad \\tau \\leftarrow t - n + 1 \\qquad (\\tau \\text{ is the time whose estimate is being updated}) \\\\&amp; \\textstyle \\qquad\\vert\\qquad \\text{If $\\tau \\geq 0$:} \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad \\rho \\leftarrow \\prod_{i = \\tau + 1}^{\\min(\\tau + n, T-1)} \\frac{\\pi(A_i \\vert S_i)}{b(A_i \\vert S_i)} \\qquad (\\rho_{\\tau + 1:\\tau + n}) \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad G \\leftarrow \\sum_{i = \\tau + 1}^{\\min(\\tau + n, T)} \\gamma^{i - \\tau - 1}R_i \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad \\text{If } \\tau + n &lt; T \\text{, then: } G \\leftarrow G + \\gamma^n Q(S_{\\tau + n}, A_{\\tau + n}) \\qquad (G_{\\tau : \\tau + n}) \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad Q(S_\\tau, A_\\tau) \\leftarrow Q(S_\\tau, A_\\tau) + \\alpha \\rho [G - Q(S_\\tau, A_\\tau)] \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad \\text{If $\\pi$ is being learned, then ensure that $\\pi(\\cdot \\vert S_\\tau)$ is greedy wrt $Q$} \\\\&amp; \\textstyle \\qquad \\text{until } \\tau = T - 1 \\\\\\end{align*}\\) 참고로 위 알고리즘은 Reinforcement: An Introduction (2020)에 소개된 알고리즘이다. 2018에 있는 알고리즘과 살짝 다른데 2020 버전이 올바른 알고리즘으로 보인다.off-policy Expected Sarsa의 경우 위 알고리즘에서 importance-sampling ratio $\\rho_{t+1:t+n}$ 대신에 $\\rho_{t+1:t+n-1}$을 사용하고 $n$-step return Expected Sarsa version을 사용하면 된다. importance-sampling ratio에서 마지막 action을 고려하지 않는 이유는 아래와 같다. This is because in Expected Sarsa all possible actions are taken into account in the last state; the one actually taken has no e↵ect and does not have to be corrected for.2$n$-step Tree Backupimportance sampling은 off-policy learning을 가능하게 하지만 분산이 커질 수 있다는 단점이 있다. importance sampling을 사용하지 않고 off-policy learning을 가능하게 해주는 방법이 있는데 바로 tree-backup algorithm이다. 아래는 3개의 sample transition과 2개의 sample action을 나타낸 backup diagram이다. 각 state node의 사이드에 달려 있는 action node들은 sampling 시에 선택되지 않은 action을 나타낸다.Fig 4. 3-step tree-backup diagram.(Image source: Sec 7.5 Sutton &amp; Barto (2018).)지금까지 diagram에서 top node의 action value를 추정할 때 아래 node를 쭉 따라 획득한 discounted reward들과 가장 아래 node의 value를 결합한 값을 사용했었다. tree-backup algorithm은 이러한 것들에 추가적으로 위 backup diagram에서 각 state들의 사이드에 달려 있는 action node에 해당하는 value 추정치 또한 고려한다.좀 더 구체적으로 보자. 먼저, 각 sample transition에서 선택되지 않은 action $a \\neq A_{t+1}$들은 target policy $\\pi$에 의해 추정된 action value에 가중치 $\\pi(a \\vert S_{t+1})$을 부여한다. 실제로 선택된 action $A_{t+1}$은 다음 단계의 모든 action에 $\\pi(A_{t+1} \\vert S_{t+1})$을 가중치로 부여한다. 이는 재귀적으로 수행된다.이제 구체적인 $n$-step tree-backup algorithm에 대한 수식을 보자. 먼저 1-step return은 아래와 같이 정의된다. 이 때 1-step return은 Expected Sarsa의 return과 동일하다.\\[G_{t:t+1} \\doteq R_{t+1} + \\gamma \\sum_a \\pi(a \\vert S_{t+1})Q_t(S_{t+1}, a), \\quad t &lt; T-1\\]2-step tree-backup return은 아래와 같이 정의된다.\\[\\begin{align*} G_{t:t+2} &amp;\\doteq R_{t+1} + \\gamma \\sum_{a \\neq A_{t+1}} \\pi(a \\vert S_{t+1})Q_{t+1}(S_{t+1}, a) \\\\ &amp;+ \\gamma \\pi (A_{t+1} \\vert S_{t+1})\\Big(R_{t+2} + \\gamma \\sum_a \\pi(a \\vert S_{t+2})Q_{t+1}(S_{t+2},a) \\Big) \\\\ &amp;= R_{t+1} + \\gamma \\sum_{a \\neq A_{t+1}} \\pi(a \\vert S_{t+1})Q_{t+1}(S_{t+1},a) + \\gamma \\pi(A_{t+1} \\vert S_{t+1})G_{t+1:t+2}, \\quad t &lt; T - 2\\end{align*}\\]$n$-step tree-backup return (target)은 아래와 같이 정의된다. 단지 위 수식을 $n$-step에 대한 재귀적 형태로 바꿔주면 된다.\\[G_{t:t+n} \\doteq R_{t+1} + \\gamma \\sum_{a \\neq A_{t+1}} \\pi(a \\vert S_{t+1})Q_{t+n-1}(S_{t+1}, a) + \\gamma \\pi(A_{t+1} \\vert S_{t+1})G_{t+1:t+n}, \\quad t &lt; T - 1, n \\geq 2\\]참고로 위 수식에서 $t &lt; T-1$인 이유는 $G_{t+1:t+n}$에서 sub-sequence가 처리되기 때문이다. $n=1$인 경우는 $G_{t:t+1}$을 사용하면 된다. 또한 $G_{T-1:t+n} \\doteq R_T$로 정의된다. 위에서 정의한 target을 일반적인 $n$-step Sarsa update rule에 적용하면 아래와 같다.\\[Q_{t+n}(S_t,A_t) \\doteq Q_{t+n-1}(S_t,A_t) + \\alpha [G_{t:t+n} - Q_{t+n-1}(S_t,A_t)], \\quad 0 \\leq t &lt; T\\]참고로 위 수식은 on-policy $n$-step Sarsa와 동일한 수식이지만 $n$-step return을 정의하는 방식이 다르다. 지금 보이는 수식은 $n$-step tree-backup return을 사용한 $n$-step tree-backup Sarsa로 off-policy learning이다. 아래는 이에 대한 알고리즘이다. $\\text{Algorithm: $n$-step Tree Backup for estimating } Q \\approx q_\\ast \\text{ or } q_\\pi$ \\(\\begin{align*}&amp; \\textstyle \\text{Initialize } Q(s,a) \\text{ arbitrarily, for all } s \\in \\mathcal{S}, a \\in \\mathcal{A} \\\\&amp; \\textstyle \\text{Initialize $\\pi$ to be greedy with respect to $Q$, or as a fixed given policy} \\\\&amp; \\textstyle \\text{Algorithm parameters: step size } \\alpha \\in (0, 1] \\text{, a positive integer $n$} \\\\&amp; \\textstyle \\text{All store and access operations (for $S_t$, $A_t$, and $R_t$) can take their index mod $n+1$} \\\\\\\\&amp; \\textstyle \\text{Loop for each episode}: \\\\&amp; \\textstyle \\qquad \\text{Initialize and store } S_0 \\neq \\text{terminal} \\\\&amp; \\textstyle \\qquad \\text{Choose an action $A_0$ arbitrarily as a function of $S_0$; Store $A_0$} \\\\&amp; \\textstyle \\qquad T \\leftarrow \\infty \\\\&amp; \\textstyle \\qquad \\text{Loop for } t = 0, 1, 2, \\dotso \\text{:} \\\\&amp; \\textstyle \\qquad\\vert\\qquad \\text{If $t &lt; T$:} \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad \\text{Take action } A_t \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad \\text{Observe and store the next reward as $R_{t+1}$ and the next state as $S_{t+1}$} \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad \\text{If $S_{t+1}$ is terminal:} \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad\\qquad T \\leftarrow t + 1 \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad \\text{else:} \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad\\qquad \\text{Choose an action $A_{t+1}$ arbitrarily as a function of $S_{t+1}$; Store $A_{t+1}$} \\\\&amp; \\textstyle \\qquad\\vert\\qquad \\tau \\leftarrow t - n + 1 \\qquad (\\tau \\text{ is the time whose estimate is being updated}) \\\\&amp; \\textstyle \\qquad\\vert\\qquad \\text{If $\\tau \\geq 0$:} \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad \\text{If } t + 1 \\geq T \\text{:} \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad\\qquad G \\leftarrow R_T \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad \\text{else:} \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad\\qquad G \\leftarrow R_{t+1} + \\gamma \\sum_a \\pi(a \\vert S_{t+1})Q(S_{t+1},a) \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad \\text{Loop for } k = \\min(t,T-1) \\text{ down through $\\tau + 1$:} \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad\\qquad G \\leftarrow R_k + \\gamma \\sum_{a \\neq A_k} \\pi(a \\vert S_k)Q(S_k,a) + \\gamma \\pi(A_k \\vert S_k)G \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad Q(S_\\tau, A_\\tau) \\leftarrow Q(S_\\tau, A_\\tau) + \\alpha [G - Q(S_\\tau, A_\\tau)] \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad \\text{If $\\pi$ is being learned, then ensure that $\\pi(\\cdot \\vert S_\\tau)$ is greedy wrt $Q$} \\\\&amp; \\textstyle \\qquad \\text{until } \\tau = T - 1 \\\\\\end{align*}\\)A Unifying Algorithm: $n$-step $Q(\\sigma)$지금까지 아래 3개의 backup diagram이 나타내는 방법을 알아봤었다. 4번째 알고리즘 $n$-step $Q(\\sigma)$는 이러한 알고리즘들을 통합한 방법이다.Fig 5. Backup diagram of $n$-step $Q(\\sigma)$.(Image source: Sec 7.6 Sutton &amp; Barto (2018).)$n$-step $Q(\\sigma)$ 알고리즘은 $\\sigma = 1$일 떄는 sampling을, $\\sigma = 0$일 때는 expectation을 나타낸다. 아래는 $n$-step $Q(\\sigma)$의 $n$-step return에 대한 정의이다.\\[G_{t:h} \\doteq R_{t+1} + \\gamma \\Big(\\sigma_{t+1}\\rho_{t+1} + (1 - \\sigma_{t+1})\\pi(A_{t+1} \\vert S_{t+1})\\Big)\\Big(G_{t+1:h} - Q_{h-1}(S_{t+1},A_{t+1})\\Big) + \\gamma \\bar{V}_{h-1}(S_{t+1}), \\quad t &lt; h \\leq T\\]이 때 $h = t + n$이다. 재귀호출은 $G_{h:h} \\doteq Q_{h-1}(S_h,A_h) \\text{ if } h &lt; T$이거나 $G_{T-1:T} \\doteq R_T \\text{ if } h = T$일 때 끝난다. 알고리즘은 아래와 같다. $\\text{Algorithm: Off-policy $n$-step $Q(\\sigma)$ for estimating } Q \\approx q_\\ast \\text{ or } q_\\pi$ \\(\\begin{align*}&amp; \\textstyle \\text{Input: an arbitrary behavior policy $b$ such that $b(a \\vert s) &gt; 0$, for all } s \\in \\mathcal{S}, a \\in \\mathcal{A} \\\\&amp; \\textstyle \\text{Initialize } Q(s,a) \\text{ arbitrarily, for all } s \\in \\mathcal{S}, a \\in \\mathcal{A} \\\\&amp; \\textstyle \\text{Initialize $\\pi$ to be greedy with respect to $Q$, or else it is a fixed given policy} \\\\&amp; \\textstyle \\text{Algorithm parameters: step size } \\alpha \\in (0, 1] \\text{, a positive integer $n$} \\\\&amp; \\textstyle \\text{All store and access operations can take their index mod $n+1$} \\\\\\\\&amp; \\textstyle \\text{Loop for each episode}: \\\\&amp; \\textstyle \\qquad \\text{Initialize and store } S_0 \\neq \\text{terminal} \\\\&amp; \\textstyle \\qquad \\text{Select and store an action } A_0 \\sim b(\\cdot \\vert S_0) \\\\&amp; \\textstyle \\qquad T \\leftarrow \\infty \\\\&amp; \\textstyle \\qquad \\text{Loop for } t = 0, 1, 2, \\dotso \\text{:} \\\\&amp; \\textstyle \\qquad\\vert\\qquad \\text{If $t &lt; T$:} \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad \\text{Take action } A_t \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad \\text{Observe and store the next reward as $R_{t+1}$ and the next state as $S_{t+1}$} \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad \\text{If $S_{t+1}$ is terminal:} \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad\\qquad T \\leftarrow t + 1 \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad \\text{else:} \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad\\qquad \\text{Choose and store an action } A_{t+1} \\sim b(\\cdot \\vert S_{t+1}) \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad\\qquad \\text{Select and store } \\sigma_{t+1} \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad\\qquad \\text{Store } \\frac{\\pi(A_{t+1} \\vert S_{t+1})}{b(A_{t+1 \\vert S_{t+1}})} \\text{ as } \\rho_{t+1} \\\\&amp; \\textstyle \\qquad\\vert\\qquad \\tau \\leftarrow t - n + 1 \\qquad (\\tau \\text{ is the time whose estimate is being updated}) \\\\&amp; \\textstyle \\qquad\\vert\\qquad \\text{If $\\tau \\geq 0$:} \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad \\text{If } t + 1 &lt; T \\text{:} \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad\\qquad G \\leftarrow Q(S_{t+1}, A_{t+1}) \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad \\text{Loop for } k = \\min(t + 1,T) \\text{ down through $\\tau + 1$:} \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad\\qquad \\text{if } k = T \\text{:} \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad\\qquad\\qquad G \\leftarrow R_T \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad\\qquad \\text{else:} \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad\\qquad\\qquad \\bar{V} \\leftarrow \\sum_a \\pi(a \\vert S_k)Q(S_k,a) \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad\\qquad\\qquad G \\leftarrow R_k + \\gamma\\big(\\sigma_k \\rho_k + (1-\\sigma_k)\\pi(A_k \\vert S_k)\\big) \\big(G - Q(S_k,A_k)\\big) + \\gamma \\bar{V} \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad Q(S_\\tau, A_\\tau) \\leftarrow Q(S_\\tau, A_\\tau) + \\alpha [G - Q(S_\\tau, A_\\tau)] \\\\&amp; \\textstyle \\qquad\\vert\\qquad\\qquad \\text{If $\\pi$ is being learned, then ensure that $\\pi(\\cdot \\vert S_\\tau)$ is greedy wrt $Q$} \\\\&amp; \\textstyle \\qquad \\text{until } \\tau = T - 1 \\\\\\end{align*}\\)Summary지금까지 $n$-step method에 대해 알아보았다. $n$-step method는 1-step TD와 MC method 사이에 해당하는 방법으로 보다 일반적인 TD method이다. 1-step TD나 MC method는 양극단에 있는 방법이기 떄문에 항상 잘 동작하지는 않는다. 특히 $n$-step TD는 1-step TD보다 복잡하고 더 많은 계산과 메모리를 요구하지만, 단 하나의 time step이 지배하는 현상에서 벗어날 수 있다는 점에서 지불할만한 가치가 있다.$n$-step method는 $n$-step transition을 고려하는 방법이다. 모든 $n$-step method는 update를 수행하기 위해 $n$ time step만큼 기다려야 한다. $n$-step method 역시 on-policy와 off-policy method가 있다. 특히 off-policy method에는 2가지 접근법이 있다. 하나는 importance sampling을 사용해 처리하는 방법이다. 이 방법은 단순하지만 높은 분산을 가진다. 다른 하나는 tree-backup algorithm이다.References[1] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction; 2nd Edition. 2018.[2] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction; 2nd Edition. 2020.[3] Towards Data Science. Sagi Shaier. N-step Bootstrapping.Footnotes DevSlem. Importance Sampling. &#8617; Reinforcement Learning: An Introduction; 2nd Edition. 2018. Sec. 7.3, p.172. &#8617; " }, { "title": "Temporal-Difference Learning", "url": "/reinforcement-learning/rl-fundamental/temporal-difference-learning/", "categories": "Reinforcement Learning, RL Fundamental", "tags": "RL, AI", "date": "2022-07-07 00:00:00 +0900", "snippet": "이 포스트에서는 RL에서 반드시 알아야 하는 RL의 핵심인 Temporal-Difference learning method를 소개한다.What is TD learningTemporal-Difference (TD) learning method는 Monte Carlo (MC) method와 Dynamic Programming (DP)의 아이디어를 결합한 방법이다. TD method는 아래와 같은 특징을 가지고 있다. MC method처럼 model 없이 experience로부터 학습이 가능하다. (agent는 environment dynamics $p(s’,r \\vert s,a)$를 모른다.) DP처럼 다른 학습된 추정치를 기반으로 추정치를 update한다. 즉, bootstrap이다. Generalized Policy Iteration (GPI)를 따른다.TD method는 DP와 MC method의 치명적 단점들을 극복한 방법이다. GPI의 control에서는 약간의 차이만 있을 뿐 거의 비슷하다. 핵심은 GPI의 prediction 부분이며 여기서 큰 차이를 보인다. TD method가 어떻게 prediction을 다루는지 알아보자.TD PredictionTD와 MC method의 공통점은 prediction problem을 해결하기 위해 sample experience를 사용한다는 점이다. MC method의 가장 큰 단점은 value function을 추정하기 위해 return을 구해야 했기 때문에 episode의 종료를 기다려야 한다는 문제가 있었다. every-visit MC method의 value function을 추정하는 단순한 update rule은 아래와 같다.\\[\\begin{align} V(S_t) &amp;\\leftarrow V(S_t) + \\alpha \\Big[G_t - V(S_t) \\Big] \\\\ V(S_t) &amp;\\leftarrow (1 - \\alpha)V(S_t) + \\alpha G_t\\end{align}\\]$G_t$는 time step $t$에 대한 return으로 MC method의 target이다. $\\alpha$는 step-size parameter 혹은 weight이다. 위 update rule을 constant-$\\alpha$ MC라고도 부른다. 참고로 이 포스트에서 일반적인 value function은 소문자로 (e.g. $v$) 표기하며, value function의 추정치임을 명확하게 나타낼 때는 대문자로 (e.g. $V$) 표기한다.위의 첫 번째 update rule은 incremental한 형식으로 일반적인 형태는 아래와 같다.\\[\\textit{NewEstimate} \\leftarrow \\textit{OldEstimate} + \\textit{StepSize} \\Big[\\textit{Target} - \\textit{OldEstimate} \\Big]\\]TD method는 앞서 언급했듯이 DP의 bootstrap 속성을 가져왔다. TD method는 MC method처럼 episode의 종료를 기다릴 필요 없이 next time step까지만 기다리면 된다. next time step $t+1$에서 TD method는 즉시 target을 형성할 수 있다. 아래는 TD method의 간단한 update rule이다.\\[V(S_t) \\leftarrow V(S_t) + \\alpha \\Big[R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) \\Big]\\]$R_{t+1}$은 획득한 reward, $\\gamma$는 discount factor이다. TD method는 reward $R_{t+1}$과 이미 존재하는 next state value 추정치 $V(S_{t+1})$을 통해 현재 state의 value function을 즉시 업데이트 한다. 특히 업데이트에 사용되는 TD method의 target을 TD target이라고 하고, TD target과 현재 state의 value 추정치와의 차이를 TD error라고 한다. 특히 TD error는 RL에서 중요한 형식으로 RL 전반에 걸쳐 다양한 형태로 나타난다.\\[\\textit{TD target} \\doteq R_{t+1} + \\gamma V(S_{t+1})\\]\\[\\textit{TD error } \\delta_t \\doteq R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)\\]이러한 TD method를 TD(0) 혹은 one-step TD라고 하는데 TD($\\lambda$)나 $n$-step TD method의 특수 case이다. 아래는 TD(0)에 대한 알고리즘이다. $\\text{Algorithm: Tabular TD(0) for estimating } v_\\pi$ \\(\\begin{align*}&amp; \\textstyle \\text{Input: the policy } \\pi \\text{ to be evaluated} \\\\&amp; \\textstyle \\text{Algorithm parameter: step size } \\alpha \\in (0, 1] \\\\&amp; \\textstyle \\text{Initialize } V(s) \\text{, for all } s \\in \\mathcal{S}^+ \\text{, arbitrarily except that } V(\\textit{terminal}) = 0 \\\\\\\\&amp; \\textstyle \\text{Loop for each episode: } \\\\&amp; \\textstyle \\qquad \\text{Initialize } S \\\\&amp; \\textstyle \\qquad \\text{Loop for each step of episode:} \\\\&amp; \\textstyle \\qquad\\qquad A \\leftarrow \\text{action given by } \\pi \\text{ for } S \\\\&amp; \\textstyle \\qquad\\qquad \\text{Take action } A \\text{, observe } R, S' \\\\&amp; \\textstyle \\qquad\\qquad V(S) \\leftarrow V(S) + \\alpha[R + \\gamma V(S') - V(S)] \\\\&amp; \\textstyle \\qquad\\qquad S \\leftarrow S' \\\\&amp; \\textstyle \\qquad \\text{until } S \\text{ is terminal} \\\\\\end{align*}\\)아래는 TD(0)에 대한 backup diagram이다.Fig 1. TD(0) backup diagram.(Image source: Robotic Sea Bass. An Intuitive Guide to Reinforcement Learning.)backup diagram의 맨 위 state node에 대한 value 추정치는 next state로의 딱 1번의 sample transition을 통해 즉각적으로 update된다.TD Control우리는 앞서 TD prediction을 통해 value function을 추정하는 방법을 알아보았다. 이제 GPI에 따라 TD control을 통해 policy를 update할 것이다. MC method와 마찬가지로 sampling을 통해 학습하기 때문에 exploration과 exploitation에 대한 trade off 관계를 고려해야 하며, 이를 수행하는 방법으로 TD method 역시 on-policy와 off-policy method가 있다.MC method에서 state value $v_\\pi$를 추정할 경우 environment에 대한 지식인 state transition probability distribution을 알아야 policy improvement를 수행할 수 있었다.1 이는 TD method에도 동일하게 적용된다. 다행이 이러한 문제는 $v_\\pi$ 대신 action value $q_\\pi$를 directly하게 추정하면 해결할 수 있다. $q_\\pi$를 추정하는 것의 장점은 environment에 대한 지식이 필요가 없어지는 것 뿐만 아니라 TD Prediction에서의 state value 추정과 본질적으로 같기 때문에 아래 그림과 같이 단지 state에서 state-action pair sequence로 대체하기만 하면 된다.Fig 2. State-action pair sequence.(Image source: Sec 6.4 Sutton &amp; Barto (2018).)앞으로 알아볼 TD method algorithm은 모두 action value $q_\\pi$를 추정한다. 이 때 TD method는 bootstrap하기 때문에 다른 학습된 next state에서의 action value 추정치를 기반으로 현재 state-action pair의 $Q(s,a)$를 추정한다. 따라서 TD method algorithm들은 다른 학습된 action value 추정치를 고려하는 방식에 따라 구분된다. 조금 더 구체적으로 얘기하자면, TD method를 target policy와 behavior policy 관점에서 볼 때 현재 update하려는 state-action pair는 behavior policy에 의해 선택되고, 다른 학습된 action value 추정치에 대한 선택은 target policy에 의해 이루어진다. 이 target policy를 어떻게 설정하느냐에 따라 algorithm들이 구분된다.SarsaSarsa는 가장 기본적인 TD on-policy method이다. 현재 state-action pair의 action value $Q(S_t,A_t)$를 추정할 때 다른 학습된 next state-action pair에 대한 $Q(S_{t+1},A_{t+1})$을 현재 policy $\\pi$에 따라 선택한다. 이에 대한 update rule은 아래와 같다.\\[Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\Big[R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \\Big]\\]당연하지만 $S_{t+1}$이 terminal state일 경우 $Q(S_{t+1}, A_{t+1})$은 0이다. 위 update rule을 TD Prediction에서 보았던 TD method의 state value에 대한 update rule과 비교해볼 때 단지 state value 추정치 $V(S)$를 action value $Q(S, A)$로 대체했을 뿐임을 확인할 수 있다.Sarsa를 target policy와 behavior policy 관점에서 살펴보자. Sarsa는 on-policy method이기 때문에 target policy와 behavior policy가 동일하다.2 따라서 next state-action pair에 대한 $Q(S_{t+1}, A_{t+1})$을 현재 behavior policy $\\pi$에 따라 선택한다. 원래 behavior policy는 $b$로 나타내지만 on-policy method이기 떄문에 $\\pi = b$이다. 아래는 위 update rule의 backup diagram이다.Fig 3. Sarsa backup diagram.(Image source: Sec 6.4 Sutton &amp; Barto (2018).)모든 on-policy method에서는 experience 생성에 사용된 behavior policy $\\pi$에 대한 $q_\\pi$를 추정함과 동시에, 추정된 $q_\\pi$에 관해 behavior policy $\\pi$를 greedy한 방향으로 update한다. Sarsa가 수렴하기 위해서는 exploration이 잘 수행되어야 하기 때문에 주로 $\\epsilon$-soft policy류의 방법을 사용한다. 아래는 Sarsa algorithm이다. $\\text{Algorithm: Sarsa (on-policy TD control) for estimating } Q \\approx q_\\ast$ \\(\\begin{align*}&amp; \\textstyle \\text{Algorithm parameters: step size } \\alpha \\in (0,1], \\text{ small } \\epsilon &gt; 0 \\\\&amp; \\textstyle \\text{Initialize } Q(s,a) \\text{, for all } s \\in \\mathcal{S}^+, a \\in \\mathcal{A}(s) \\text{, arbitrarily except that } Q(\\textit{terminal},\\cdot) = 0 \\\\\\\\&amp; \\textstyle \\text{Loop for each episode:} \\\\&amp; \\textstyle \\qquad \\text{Initialize } S \\\\&amp; \\textstyle \\qquad \\text{Choose } A \\text{ from } S \\text{ using policy derived from } Q \\text{ (e.g., } \\epsilon \\text{-greedy)} \\\\&amp; \\textstyle \\qquad \\text{Loop for each step of episode:} \\\\&amp; \\textstyle \\qquad\\qquad \\text{Take action } A \\text{, observe } R, S' \\\\&amp; \\textstyle \\qquad\\qquad \\text{Choose } A' \\text{ from } S' \\text{ using policy derive from } Q \\text{ (e.g., } \\epsilon \\text{-greedy)} \\\\&amp; \\textstyle \\qquad\\qquad Q(S,A) \\leftarrow Q(S,A) + \\alpha [R + \\gamma Q(S',A') - Q(S,A)] \\\\&amp; \\textstyle \\qquad\\qquad S \\leftarrow S'; \\ A \\leftarrow A'; \\\\&amp; \\textstyle \\qquad \\text{until } S \\text{ is terminal}\\end{align*}\\)아래는 위 algorithm을 구현한 소스 코드이다. Windy Gridworld3 training with Sarsa: DevSlem/rl-algorithm (Github)Q-learningQ-learning은 RL에서 가장 기본적이면서도 가장 중요한 알고리즘 중 하나이다. Q-learning은 off-policy TD method이며 아래와 같이 정의된다.\\[Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\Big[R_{t+1} + \\gamma \\max_a Q(S_{t+1},a) - Q(S_t,A_t) \\Big]\\]Q-learning과 Sarsa의 가장 주요한 차이는 TD error를 구성할 때 next state-action pair의 value function $Q$를 선택하는 기준이다. Sarsa는 next state $S_{t+1}$에서 현재 policy $\\pi$를 따라 action value를 선택했다면, Q-learning은 현재 policy와 상관 없이 next state에서의 maximum action value $\\max_a Q(S_{t+1},a)$를 선택한다. Q-learning의 backup diagram은 아래와 같으며 Sarsa의 backup diagram과 비교해보길 바란다.Fig 4. Q-learning backup diagram.(Image source: Sec 6.5 Sutton &amp; Barto (2018).)위 backup diagram에서 화살표 사이를 이어주는 선은 greedy selection을 의미한다.Q-learning을 target policy와 behavior policy 관점에서 살펴보자. Q-learning은 off-policy method로 target policy와 behavior policy가 분리된다.2 Q-learning에서 next state-action pair에 대한 value function $Q$를 고려할 때 greedy하게 고려하기 때문에 target policy는 greedy policy이다. behavior policy는 exploration을 충분히 수행할 수 있는 임의의 policy (e.g. $\\epsilon$-soft policy)이다. 아래는 Q-learning algorithm이다. $\\text{Algorithm: Q-learning (off-policy TD control) for estimating } \\pi \\approx \\pi_\\ast$ \\(\\begin{align*}&amp; \\textstyle \\text{Algorithm parameters: step size } \\alpha \\in (0,1] \\text{, small } \\epsilon &gt; 0 \\\\&amp; \\textstyle \\text{Initialize } Q(s,a) \\text{, for all } s \\in \\mathcal{S}^+, a \\in \\mathcal{A}(s) \\text{, arbitrarily except that } Q(\\textit{terminal}, \\cdot) = 0 \\\\\\\\&amp; \\textstyle \\text{Loop for each episode:} \\\\&amp; \\textstyle \\qquad \\text{Initialize } S \\\\&amp; \\textstyle \\qquad \\text{Loop for each step of episode:} \\\\&amp; \\textstyle \\qquad\\qquad \\text{Choose } A \\text{ from } S \\text{ using policy derived from } Q \\text{ (e.g., } \\epsilon \\text{-greedy)} \\\\&amp; \\textstyle \\qquad\\qquad \\text{Take action, observe } R,S' \\\\&amp; \\textstyle \\qquad\\qquad Q(S,A) \\leftarrow Q(S,A) + \\alpha [R + \\gamma \\max_a Q(S',a) - Q(S,A)] \\\\&amp; \\textstyle \\qquad\\qquad S \\leftarrow S' \\\\&amp; \\textstyle \\qquad \\text{until } S \\text{ is terminal}\\end{align*}\\)아래는 Q-learning과 Sarsa algorithm을 구현한 뒤 비교하는 소스 코드이다. Cliff Walking4 training with both Q-learning and Sarsa: DevSlem/rl-algorithm (Github)Expected SarsaExpected Sarsa는 Q-learning과 유사한 알고리즘이다. Q-learning이 TD error를 구성할 때 next state-action pair들의 maximum action value를 고려했다면 Expected Sarsa는 target policy $\\pi$를 따랐을 때의 next state-action pair에 대한 expected value를 고려한다. 아래 update rule을 보면 조금 더 쉽게 이해할 수 있다.\\[\\begin{align} Q(S_t, A_t) &amp;\\leftarrow Q(S_t, A_t) + \\alpha \\Big[R_{t+1} + \\gamma \\mathbb{E}_\\pi[Q(S_{t+1},A_{t+1}) \\ \\vert \\ S_{t+1}] - Q(S_t,A_t) \\Big] \\\\ &amp;\\leftarrow Q(S_t, A_t) + \\alpha \\Big[R_{t+1} + \\gamma \\sum_a \\pi(a \\vert S_{t+1}) Q(S_{t+1},a) - Q(S_t,A_t) \\Big]\\end{align}\\]Expected Sarsa는 on-policy일까 off-policy method일까? 정답은 둘다 될 수 있다이다. target policy $\\pi$를 어떻게 설정하느냐에 따라 달라진다. target policy와 behavior policy가 다르면 off-policy이고 같으면 on-policy method이다. 단지 그 뿐이다. 예를 들면 behavior policy $b$가 $\\epsilon$-greedy policy라고 할 때 target policy $\\pi$도 $\\epsilon$-greedy policy이면 $\\pi = b$인 on-policy method이며 expected value는 $\\epsilon$-greedy policy에 관해 계산된다. 반대로 $\\pi \\neq b$인 off-policy method이며 target policy가 greedy policy라면 어떨까? greedy action을 제외한 나머지 action들의 확률은 0이기 때문에 expected value는 next state-action pair에 대한 maximum action value이다. 즉, Q-learning과 동일해진다. 이처럼 Expected Sarsa는 flexible하다는 것을 알 수 있으며 모든 action들을 고려하는 expected value이기 떄문에 Sarsa에 비해 분산이 작다.아래는 Expected Sarsa의 backup diagram이다. Sarsa와 비교했을 때 모든 action을 고려하고 있음을 확인할 수 있다.Fig 5. Expected Sarsa backup diagram.(Image source: Sec 6.6 Sutton &amp; Barto (2018).)Expected Sarsa는 위 Q-learning과 거의 구조가 동일하기 때문에 따로 algorithm을 올리지는 않겠다. 대신 아래에 소스코드를 첨부한다. 여기서는 target policy와 behavior policy가 동일한 on-policy Expected Sarsa를 구현했다. Expected Sarsa의 update rule은 update() 메서드에 구현되어 있다. Expected Sarsa source code: DevSlem/rl-algorithm (Github)Double Q-learning기존 Q-learning의 가장 큰 문제는 biased하다는 점이다. 이로 인해 특히 stochastic한 environment에서 action value들에 대한 overestimation으로 인해 매우 나쁜 performance를 보인다. 아래 stochastic environment에 대한 예제를 먼저 살펴보자.Fig 6. Simple stochastic environment.(Image source: Sec 6.7 Sutton &amp; Barto (2018).)agent는 항상 state A에서 시작한다. A에서 right action을 선택하면 reward 0과 함께 즉시 episode는 종료된다. left action을 선택하면 reward 0과 함께 state B로 전이된다. state B에서는 episode를 즉시 종료할 수 있는 수 많은 action들이 존재한다. 이 때 각 action들을 선택함으로써 얻게 되는 reward는 normal distribution $N(-0.1,1)$을 따른다. 즉, stochastic한 environment이다.그렇다면 위 environment에서 왜 Q-learning은 매우 나쁜 performance를 보일까? state B에서 획득할 수 있는 reward는 $N(-0.1, 1)$을 따르기 떄문에 state A에서 left action을 선택했을 때의 expected return은 -0.1이 될 것이다. 반대로 right action의 expected return은 0이다. 그런데 Q-learning의 training 초기에 state B에서 action을 선택했을 떄 $N(-0.1, 1)$에 따라 reward를 어떤 큰 양수 (e.g. $+2$) 값으로 주로 획득했다면 어떻게 될까? training 초기이기 때문에 아직 sampling된 data가 부족해 획득된 reward의 expected value는 양수일 것이다. Q-learning은 maximum next state-action pair value를 선택한다. 이는 training 초기에 state A에서 실제 optimal action인 right가 아닌 left action을 선택하도록 유도할 것이다. 즉, training 속도는 저하될 것이다.위 문제를 해결하도록 고안된 것이 Double Q-learning algorithm이다. 기존 Q-learning과 다르게 Double Q-learning은 action value를 2개로 나누어 추정한다. 즉, $Q_1, Q_2$를 추정하는 방법이다. 아래는 Q-learning과 Double Q-learning의 performance 비교이다. state A에서 left action을 선택하는 비율이 $y$값이며 이 값이 작을 수록 optimal하다.Fig 7. Comparison of Q-learning and Double Q-learning.(Image source: Sec 6.7 Sutton &amp; Barto (2018).)위 그림을 보면 알겠지만 Q-learning은 training 초기에 left action을 overestimation하여 left action 쪽으로 편향된 모습을 확인할 수 있다. 반대로 Double Q-learning은 training 초기부터 안정적이며 Q-learning에 비해 훨씬 빠르게 optimal에 도달한다. 이에 대한 자세한 직관적 설명은 첨부된 블로그를5, 수식적 증명은 논문6을 찾아보길 바란다.앞서 Double Q-learning은 두 개의 action value $Q_1, Q_2$를 추정한다고 언급했다. 이 둘은 일종의 서로를 보완하는 역할을 한다. $Q_1$을 update하고 싶다고 할 때 TD error를 구성하기 위해 next state-action pair value가 필요하다. Double Q-learning 역시 Q-learning이기 때문에 target policy는 greedy policy로, next state에서 고려할 action은 $Q_1$에 대한 greedy action $A^\\ast = \\arg\\max_aQ_1(S_{t+1},a)$이다. 기존 Q-learning에서는 TD error를 구성할 때 $A^\\ast$에 대한 action value $Q_1(S_{t+1}, A^\\ast)$를 고려했었다. Double Q-learning에서는 $A^\\ast$에 대해 $Q_1$이 아닌 $Q_2(S_{t+1},A^\\ast)$를 고려한다. 즉, $Q_1$을 update하기 위해서 $Q_2$ 추정치를 고려한다. $Q_2$를 update할 때는 반대이다. 이를 정리한 $Q_1$에 대한 update rule은 아래와 같다.\\[Q_1(S_t,A_t) \\leftarrow Q_1(S_t,A_t) + \\alpha \\Big[R_{t+1} + \\gamma Q_2\\big(S_{t+1}, \\underset{a}{\\arg\\max} \\ Q_1(S_{t+1},a) \\big) - Q_1(S_t,A_t)]\\]$Q_2$를 update할 때는 위 update rule에서 $Q_1$과 $Q_2$를 서로 바꿔주기만 하면 된다. $Q_1$과 $Q_2$는 당연하지만 둘이 같은 값을 가지도록 수렴할 것이다. behavior policy는 보통 $Q_1$과 $Q_2$를 모두 고려한다. 가장 간단한 방법은 behavior policy가 $Q_1 + Q_2$에 대해 action을 선택하는 것이다. $Q_1$과 $Q_2$의 update 역시 여러 가지 방법이 있겠지만 가장 간단한 방법은 각 episode의 time step $t$마다 0.5의 확률로 랜덤하게 update하는 것이다. 이에 대한 algorithm은 아래와 같다. $\\text{Algorithm: Double Q-learning, for estimating } Q_1 \\approx Q_2 \\approx q_\\ast$ \\(\\begin{align*}&amp; \\textstyle \\text{Algorithm parameters: step size } \\alpha \\in (0,1] \\text{, small } \\epsilon &gt; 0 \\\\&amp; \\textstyle \\text{Initialize } Q_1(s,a) \\text{ and } Q_2(s,a) \\text{, for all } s \\in \\mathcal{S}^+, a \\in \\mathcal{A}(s) \\text{, such that } Q(\\textit{terminal}, \\cdot) = 0 \\\\\\\\&amp; \\textstyle \\text{Loop for each episode:} \\\\&amp; \\textstyle \\qquad \\text{Initialize } S \\\\&amp; \\textstyle \\qquad \\text{Loop for each step of episode:} \\\\&amp; \\textstyle \\qquad\\qquad \\text{Choose } A \\text{ from } S \\text{ using the policy } \\epsilon \\text{-greedy in } Q_1 + Q_2 \\\\&amp; \\textstyle \\qquad\\qquad \\text{Take action } A \\text{, observe } R, S' \\\\&amp; \\textstyle \\qquad\\qquad \\text{With 0.5 probability:} \\\\&amp; \\textstyle \\qquad\\qquad\\qquad Q_1(S,A) \\leftarrow Q_1(S,A) + \\alpha \\Big(R + \\gamma Q_2 \\big(S', \\arg\\max_a Q_1(S',a) \\big) - Q_1(S,A) \\Big) \\\\&amp; \\textstyle \\qquad\\qquad \\text{else:} \\\\&amp; \\textstyle \\qquad\\qquad\\qquad Q_2(S,A) \\leftarrow Q_2(S,A) + \\alpha \\Big(R + \\gamma Q_1 \\big(S', \\arg\\max_a Q_2(S',a) \\big) - Q_2(S,A) \\Big) \\\\&amp; \\textstyle \\qquad\\qquad S \\leftarrow S' \\\\&amp; \\textstyle \\qquad \\text{until } S \\text{ is terminal}\\end{align*}\\)아래는 위 update rule을 구현한 소스 코드이다. update() 메서드에 구현되어있다. Double Q-learning source code: DevSlem/rl-algorithm (Github)Summary지금까지 TD method에 대해 알아보았다. TD method는 MC method와 DP의 아이디어를 결합한 방식이다. MC method 처럼 environment에 대한 지식 없이 sampling을 통해 학습하며 DP와 같이 bootstrap한 속성을 가진다. TD method 역시 GPI를 따른다. TD prediction에서 TD error는 굉장히 중요한 수식으로 RL 전반에 걸쳐 등장한다. TD method 역시 on-policy와 off-policy로 구분되며 on-policy에는 Sarsa, off-policy에는 Q-learning이 있다. 특히 Q-learning은 굉장히 중요한 algorithm이다. 그 외에도 위 두 algorithm을 개선한 Expected Sarsa, Double Q-learning을 알아보았다.References[1] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction; 2nd Edition. 2018.Footnotes DevSlem. Monte Carlo Estimation of Action Values. &#8617; DevSlem. Off-policy methods. &#8617; &#8617;2 Reinforcement Learning: An Introduction; 2nd Edition. 2018. Sec. 6.4, p.152; Example 6.5: Windy Gridworld. &#8617; Reinforcement Learning: An Introduction; 2nd Edition. 2018. Sec. 6.5, p.154; Example 6.6: Cliff Walking. &#8617; Towards Data Science. Ziad SALLOUM. Double Q-Learning, the Easy Way. &#8617; Hado van Hasselt. Double Q-learning. &#8617; " }, { "title": "Monte Carlo Methods in RL", "url": "/reinforcement-learning/rl-fundamental/monte-carlo-methods/", "categories": "Reinforcement Learning, RL Fundamental", "tags": "RL, AI", "date": "2022-07-03 00:00:00 +0900", "snippet": "이 포스트에서는 RL에서 environment에 대한 지식을 완전히 알 수 없을 때 experience를 통해 문제를 해결하는 Monte Carlo methods를 소개한다.IntroductionMonte Carlo (MC) methods는 반복된 random sampling을 통해 numerical한 근사해를 얻는 방법으로 일종의 simulation 기법이다. Reinforcement Learning (RL)에서 environment에 대한 지식을 완전히 알지 못할 때 MC methods는 굉장히 유용하다. DP에서와 같이 어떤 state에서 next state로의 transition에 대한 정보를 사전에 알 필요 없이 model은 단순히 transition을 sampling하기만 하면 된다. agent는 sampling된 state, action, reward와 같은 experience의 sequence를 바탕으로 sample return들에 대해 평균을 냄으로써 RL problem을 해결한다.MC methods는 일반적으로 experience를 episode 단위로 나누어 적용된다. 따라서 모든 episode는 반드시 terminal state가 존재하는 episodic task여야 한다. MC methods 역시 Generalized Policy Iteration (GPI)를 따르며 한 episode가 끝날 때마다 policy evaluation과 policy improvement가 수행되는 step-by-step (online)이 아닌 episode-by-episode 방법이다.Monte Carlo Prediction먼저 value function을 추정하는 policy evaluation 혹은 prediction에 대해 알아보자. state value의 가장 간단한 정의는 state $s$에서 시작하여 policy $\\pi$를 따를 때 얻을 수 있는 expected return이다.\\[v_\\pi(s) \\doteq \\mathbb{E}_\\pi[G_t \\ \\vert \\ S_t = s]\\]Monte Carlo methods는 episode 단위로 수행되기 때문에 먼저 policy $\\pi$를 따라 episode를 생성한다. 그 후 episode 내의 experience로부터 위 정의를 그대로 따라 방문된 state에 대한 value function을 추정한다.같은 episode 내에서 동일한 state가 여러번 방문될 수 있다. 이때 크게 2가지 방법으로 처리한다. first-visit MC method every-visit MC methodfirst-visit MC method는 한 episode 내에서 state $s$를 처음 방문했을때의 return만 그 state의 average return에 반영한다. 반면 every-visit MC method는 한 episode 내에서 state $s$를 방문할때마다 그때의 return을 모두 average return에 반영한다. 이 포스트에서는 first-visit MC method만 고려할 계획이다.MC method를 backup diagram으로 나타내면 아래와 같다. 빨간색 영역이 한 episode이다.Fig 1. MC backup diagram.(Image source: Robotic Sea Bass. An Intuitive Guide to Reinforcement Learning.)Dynamic Programming (DP)에서는 가능한 모든 transition을 고려했다면 MC methods에서는 sampling된 한 episode만 고려하고 있음을 알 수 있다. 또한 DP에서는 one-step transition만을 고려했지만 MC methods에서는 episode가 끝날 때까지의 모든 transition을 고려한다.Monte Carlo Estimation of Action ValuesMonte Carlo Prediction에서 소개한 방법은 value-based 방법이다. 그러나 이 방법은 policy를 개선할 때 큰 문제가 발생하는데 결국 environment에 대한 지식인 state transition과 관련된 probability distribution을 알아야 한다.1 policy improvement를 통해 new greedy policy $\\pi’$을 획득한다고 할 때 수식은 아래와 같다.\\[\\pi'(s) \\doteq \\underset{a}{\\arg\\max} \\ q_\\pi(s, a)\\]new policy를 결정하는 방법은 간단하다. 그 state에서의 action value만 알고 있으면 된다. 문제는 action value를 구할 때 발생한다. action value $q_\\pi(s, a)$를 구하는 수식은 아래와 같다.\\[\\begin{align} q_\\pi(s,a) &amp;\\doteq \\mathbb{E}[R_{t+1} + \\gamma v_\\pi(S_{t+1}) \\ \\vert \\ S_t = s, A_t = a] \\\\ &amp;= \\sum_{s',r}p(s',r \\vert s,a)\\Big[r + \\gamma v_\\pi(s') \\Big]\\end{align}\\]위 수식을 보면 알 수 있듯이 state value function $v_\\pi$를 통해 action value $q_\\pi$를 구할 때 environment dynamics $p(s’,r \\vert s,a)$를 알아야만 한다. policy evaluation에서 random sampling을 통해 environment에 대한 지식 없이 state value $v_\\pi(s)$를 추정할 수 있었지만, policy improvement를 수행하기 위해서는 결국 environment에 대해 알고 있어야만 하는 문제가 발생한다.위 문제는 environment에 대해 알고 있다면 value-based MC methods를 통해서도 해결할 수 있다. 그러나 environment에 대해 알지 못한다면 다른 접근법이 필요한데, state value를 추정하는 것이 아니라 state-action pair에 대한 action value 그 자체를 추정하면 이 문제를 해결할 수 있다. action value $q_\\pi$의 가장 간단한 정의는 state $s$에서 policy $\\pi$에 따라 action $a$를 선택했을 때 얻을 수 있는 expected return이다.\\[q_\\pi(s, a) \\doteq \\mathbb{E}_\\pi[G_t \\ \\vert \\ S_t = s, A_t = a]\\]위 정의에 따라 action value 자체를 추정해 획득한 값으로 policy improvement를 수행하면 environment에 대한 정보가 필요 없어진다.Monte Carlo Control이제 Monte Carlo estimation이 optimal policy를 추정하는 policy improvement 혹은 control에 어떻게 적용될 수 있는지 알아보자. 앞서 언급했지만 MC methods 역시 GPI를 따른다. 다만 이제 policy evaluation에서 state value가 아닌 action value $q_\\pi$를 추정할 것이다.Fig 2. GPI for action value.(Image source: Sec 5.3 Sutton &amp; Barto (2018).)위 그림을 sequence로 나타내면 아래와 같다.\\[\\pi_0 \\overset{E}{\\longrightarrow} q_{\\pi_0} \\overset{I}{\\longrightarrow} \\pi_1 \\overset{E}{\\longrightarrow} q_{\\pi_1} \\overset{I}{\\longrightarrow} \\pi_2 \\overset{E}{\\longrightarrow} \\cdots \\overset{I}{\\longrightarrow} \\pi_\\ast \\overset{E}{\\longrightarrow} q_\\ast\\]위에서 $\\overset{E}{\\longrightarrow}$는 policy evaluation, $\\overset{I}{\\longrightarrow}$는 policy improvement를 나타낸다.policy evaluation은 앞서 Monte Carlo Prediction에서 소개한 방식과 동일하다. 현재 policy $\\pi$에 따라 state-action pair에 대한 experience를 통해 episode가 생성되고, 이 episode에 대해 관찰된 return들을 바탕으로 action value $q_\\pi$를 추정한다. 그 후 계산된 action value $q_\\pi$를 바탕으로 episode 내에 방문된 모든 state에 대해 policy가 개선된다. 이를 episode 단위로 반복하다 보면 optimal policy $\\pi_\\ast$와 optimal action value $q_\\ast$를 획득할 수 있다. 즉, 아래와 같이 3가지 과정으로 요약할 수 있다. policy $\\pi$에 따라 episode 생성 episode 내의 experience를 바탕으로 $q_\\pi$에 대한 policy evaluation policy improvement다만 optimal policy와 action value로 수렴하기 위해서는 반드시 모든 state-action pair가 방문된다는 가정이 필요하다. 즉, agent는 environment에 대해 골고루 탐색해야 한다. 이는 RL에서 굉장히 일반적인 maintaining exploration 문제이다. action value를 통해 policy evaluation을 효과적으로 수행하기 위해서는 반드시 지속적인 exploration을 보장해야한다. 이 포스트에서는 이 문제에 대한 해결책으로 2가지 방법을 소개한다. exploring starts $\\epsilon$-soft policyexploring starts (ES)는 episode 시작 시 state-action pair $(s, a)$를 stochastic하게 선택하며, 이 때 모든 시작 state-action pair는 반드시 0이 아닌 확률을 가진다. 이 경우 episode의 수가 무한할 경우 모든 state-action pair가 방문됨을 보장할 수 있다.그러나 위 방법은 특정 state에서 시작해야하는 조건이 있는 경우 유용하지 않다. 이에 대한 대안으로 각 state에서 선택할 모든 action에 대해 0이 아닌 확률을 보장하는 stochastic policy를 고려한다. 대표적으로 $\\epsilon$-soft policy가 있다.이제 각 방법을 바탕으로 한 MC methods 알고리즘을 알아보자.Monte Carlo ESfirst-visit Monte Carlo ES 알고리즘은 아래와 같다. $\\text{Algorithm: Monte Carlo ES (Exploring Starts), for estimating } \\pi \\approx \\pi_\\ast$ \\(\\begin{align*}&amp; \\textstyle \\text{Initialize:} \\\\&amp; \\textstyle \\qquad \\pi(s) \\in \\mathcal{A}(s) \\text{ (arbitrarily), for all } s \\in \\mathcal{S} \\\\&amp; \\textstyle \\qquad Q(s,a) \\in \\mathbb{R} \\text{ (arbitrarily), for all } s \\in \\mathcal{S}, \\ a \\in \\mathcal{A}(s) \\\\&amp; \\textstyle \\qquad \\textit{Returns}(s,a) \\leftarrow \\text{empty list, for all } s \\in \\mathcal{S}, \\ a \\in \\mathcal{A}(s) \\\\\\\\&amp; \\textstyle \\text{Loop forever (for each episode):} \\\\&amp; \\textstyle \\qquad \\text{Choose } S_0 \\in \\mathcal{S}, \\ A_0 \\in \\mathcal{A}(S_0) \\text{ randomly such that all pairs have probability} &gt; 0 \\\\&amp; \\textstyle \\qquad \\text{Generate an episode from } S_0, A_0, \\text{ following } \\pi \\text{: } S_0, A_0, R_1, \\dots, S_{T-1}, A_{T-1}, R_T \\\\&amp; \\textstyle \\qquad G \\leftarrow 0 \\\\&amp; \\textstyle \\qquad \\text{Loop for each step of episode, } t = T-1, T-2, \\dots, 0 \\text{:} \\\\&amp; \\textstyle \\qquad\\qquad G \\leftarrow \\gamma G + R_{t+1} \\\\&amp; \\textstyle \\qquad\\qquad \\text{Unless the pair } S_t, A_t \\text{ appears in } S_0, A_0, S_1, A_1, \\dots, S_{t-1}, A_{t-1} \\text{:} \\\\&amp; \\textstyle \\qquad\\qquad\\qquad \\text{Append } G \\text{ to } \\textit{Returns}(S_t,A_t) \\\\&amp; \\textstyle \\qquad\\qquad\\qquad Q(S_t,A_t) \\leftarrow \\text{average}(\\textit{Returns}(S_t,A_t)) \\\\&amp; \\textstyle \\qquad\\qquad\\qquad \\pi(S_t) \\leftarrow \\arg\\max_a Q(S_t,a)\\end{align*}\\)exploring starts이기 때문에 각 episode의 시작마다 모든 state-action pair의 확률이 0보다 큰 조건 하에 랜덤하게 state-action pair를 선택한다. 또한 action value $Q(S_t, A_t)$를 정의에 따라 $S_t, A_t$ pair에 대해 지금까지 획득한 return $G_t$들의 expectation으로 update한다.Monte Carlo Control without ESexploring starts는 쉽게 말해 랜덤 스타트로, 문제가 있는 방식이라 소개했었다. 따라서 ES를 사용하지 않고 모든 state-action pair가 방문됨을 보장하려고 한다. 이를 달성할 수 있는 일반적인 방법은 episode의 시작에서만 하는게 아니라 episode 동안 지속적으로 agent가 state-action pair를 무한히 선택할 수 있도록 보장한다. 이것을 달성하기 위한 2가지 접근법이 있다. on-policy methods off-policy methodson-policy methods는 experience 생성에 사용되는 policy를 평가하고 개선한다. 반면 off-policy methods는 experience를 생성하는데 사용되는 policy와 평가 및 개선하는데 사용되는 policy를 분리하는 방법이다.On-policy Monte Carloon-policy methods에서는 policy는 일반적으로 soft하다. soft하다는 것은 모든 $s \\in \\mathcal{S}$와 $a \\in \\mathcal{A}$에 대해 $\\pi(a \\vert s) &gt; 0$는 의미이다. 즉, 모든 state-action pair에게 0이 아닌 확률을 보장한다. 이러한 soft한 policy를 GPI 과정을 거치면서 점점 deterministic한 optimal policy로 수렴한다.위를 달성할 수 있는 가장 간단하 방법 중 하나가 $\\epsilon$-greedy policy이다. $\\epsilon$-greedy policy는 대부분은 action value가 최대인 action을 선택하지만 $\\epsilon$의 확률로 랜덤하게 action을 선택한다. 따라서 모든 action들은 아래와 같이 선택될 확률을 가진다.\\[\\pi(a \\vert s) = \\begin{cases} 1 - \\epsilon + \\dfrac{\\epsilon}{\\vert \\mathcal{A}(s) \\vert} &amp; \\text{if } a = \\text{greedy} \\\\ \\dfrac{\\epsilon}{\\vert \\mathcal{A}(s) \\vert} \\qquad &amp; \\text{if } a = \\text{nongreedy}\\end{cases}\\]$\\epsilon$-greedy policy는 $\\epsilon$-soft policy의 가장 대표적인 예시이다. $\\epsilon$-soft policy는 모든 state와 action에 대해 $\\pi(a \\vert s) \\geq \\dfrac{\\epsilon}{\\vert \\mathcal{A}(s) \\vert}$인 policy이다.On-policy first-visit MC methods 알고리즘을 살펴보자. policy는 $\\epsilon$-greedy이다. $\\text{Algorithm: On-policy first-visit MC control (for } \\epsilon \\text{-soft policies), estimates } \\pi \\approx \\pi_\\ast$ \\(\\begin{align*}&amp; \\textstyle \\text{Algorithm parameter: small } \\epsilon &gt; 0 \\\\&amp; \\textstyle \\text{Initialize: } \\\\&amp; \\textstyle \\qquad \\pi \\leftarrow \\text{an arbitrary } \\epsilon \\text{-soft policy} \\\\&amp; \\textstyle \\qquad Q(s, a) \\in \\mathbb{R} \\text{ (arbitrarily), for all } s \\in \\mathcal{S}, \\ a \\in \\mathcal{A}(s) \\\\&amp; \\textstyle \\qquad \\textit{Returns}(s,a) \\leftarrow \\text{empty list, for all } s \\in \\mathcal{S}, \\ a \\in \\mathcal{A}(s) \\\\\\\\&amp; \\textstyle \\text{Loop forever (for each episode):} \\\\&amp; \\textstyle \\qquad \\text{Generate an episode following } \\pi \\text{: } S_0, A_0, R_1, \\dots, S_{T-1}, A_{T-1}, R_T \\\\&amp; \\textstyle \\qquad G \\leftarrow 0 \\\\&amp; \\textstyle \\qquad \\text{Loop for each step of episode, } t = T-1, T-2, \\dots, 0 \\text{:} \\\\&amp; \\textstyle \\qquad\\qquad G \\leftarrow \\gamma G + R_{t+1} \\\\&amp; \\textstyle \\qquad\\qquad \\text{Unless the pair } S_t,A_t \\text{ appears in } S_0, A_0, S_1, A_1, \\dots, S_{t-1}, A_{t-1} \\text{:} \\\\&amp; \\textstyle \\qquad\\qquad\\qquad \\text{Append } G \\text{ to } \\textit{Returns}(S_t,A_t) \\\\&amp; \\textstyle \\qquad\\qquad\\qquad Q(S_t, A_t) \\leftarrow \\text{average}(\\textit{Returns}(S_t, A_t)) \\\\&amp; \\textstyle \\qquad\\qquad\\qquad A^\\ast \\leftarrow \\arg\\max_a Q(S_t, a) \\\\&amp; \\textstyle \\qquad\\qquad\\qquad \\text{For all } a \\in \\mathcal{A}(S_t)\\text{:} \\\\&amp; \\textstyle \\qquad\\qquad\\qquad\\qquad \\pi(a \\vert S_t) \\leftarrow \\begin{cases} 1 - \\epsilon + \\epsilon / \\vert \\mathcal{A}(S_t) \\vert &amp; \\text{if } a = A_\\ast \\\\ \\epsilon / \\vert \\mathcal{A}(S_t) \\vert \\qquad &amp; \\text{if } a \\neq A_\\ast\\end{cases}\\end{align*}\\)on-policy methods이기 때문에 episode 생성에 사용되는 policy $\\pi$를 평가하고 개선한다. 이 때 $q_\\pi$에 관한 $\\epsilon$-soft policy는 policy improvement theorem에 의해 보장된다.2Off-policy methodsoff-policy methods는 아래와 같은 2개의 policy를 사용하는 방법이다. target policy behavior policytarget policy는 학습에 사용되는 policy로 target policy를 optimal policy로 수렴시키는 것이 목적이다. behavior policy는 behavior 혹은 experience를 생성하는 policy로 target policy에 비해 조금 더 exploratory한 policy이다.on-policy는 일반적으로 비교적 쉬운 편이기 때문에 자주 고려된다. off-policy는 대게 더 높은 분산과 느린 수렴 속도를 가지지만 더 강력하고 general하다. on-policy는 target policy와 behavior policy가 동일한 경우로 취급할 수 있다.조금 더 구체적으로 들어가자면 아래와 같은 과정을 거친다. behavior policy $b$로부터 episode 생성 episode 내의 experience를 바탕으로 $q_\\pi$에 대한 policy evaluation policy improvement그런데 이상한 점이 있다. behavior policy $b$와 target policy $\\pi$의 distribution은 전혀 다를 것이다. 우리가 구할 수 있는 것은 $\\mathbb{E} _ b[X]$인데 어떻게 $b$로부터 생성된 episode를 가지고 $q_\\pi$를 추정할 수 있을까? 이 문제를 해결하기 위해 importance sampling이라는 기법을 사용한다.Importance Samplingimportance sampling은 다른 distribution을 따르는 sample이 주어졌을 때 목표로 하는 distribution의 expected value를 추정하는 기법이다. 대부분의 off-policy methods는 서로 다른 policy를 사용하기 때문에 importance sampling을 통해 expected value를 추정한다.target policy와 behavior policy에 대한 trajectory의 상대적 확률에 따라 return에 가중치를 부여한다. 이를 importance-sampling ratio라고 한다. 먼저 시작 state $S_t$가 주어졌을 때 생성된 state-action trajectory가 아래와 같이 있다고 하자.\\[A_t, S_{t+1}, A_{t+1}, \\dots, S_T\\]이 때 임의의 policy $\\pi$를 따를 때 위 trajectory의 발생 확률은 아래와 같다. 이때 $\\pi$는 trajectory를 생성한 policy가 아니여도 된다.\\[\\begin{align} &amp; \\text{Pr} \\lbrace A_t, S_{t+1}, A_{t+1}, \\dots, S_t \\ \\vert \\ S_t, A_{t:T-1} \\sim \\pi \\rbrace \\\\ &amp; \\qquad = \\pi(A_t \\vert S_t) p(S_{t+1} \\vert S_t, A_t) \\pi(A_{t+1} \\vert S_{t+1}) \\cdots p(S_T \\vert S_{T-1}, A_{T-1}) \\\\ &amp; \\qquad = \\prod_{k=t}^{T-1} \\pi(A_k \\vert S_k) p(S_{k+1} \\vert S_k, A_k)\\end{align}\\]위 수식에서 $p$는 state-transition probability function이다. 임의의 policy가 각각 trajectory를 생성한 behavior policy $b$, expected value를 추정하려는 target policy $\\pi$라고 할 때 위 trajectory의 확률을 바탕으로 importance-sampling ratio $\\rho$를 구할 수 있다.\\[\\rho_{t:T-1} \\doteq \\dfrac{\\prod_{k=t}^{T-1} \\pi(A_k \\vert S_k) p(S_{k+1} \\vert S_k, A_k)}{\\prod_{k=t}^{T-1} b(A_k \\vert S_k) p(S_{k+1} \\vert S_k, A_k)} = \\prod_{k=t}^{T-1}\\dfrac{\\pi(A_k \\vert S_k)}{b(A_k \\vert S_k)}\\]이미 알고 있는 distribution은 분모, expected value를 추정할 distribution은 분자여야 한다. 위 수식의 분모, 분자에 있던 state-transition probability는 소거할 수 있다. 그 이유는 동일한 environment에서의 동일한 trajectory이기 때문에 state-transition probability는 동일하다. 결국 environment에 대한 지식은 필요 없게 된다. 오직 $b$와 $\\pi$, 생성된 trajectory만 있으면 된다.위에서 구한 importance-sampling ratio를 통해 이제 올바른 expected value를 추정할 수 있다. 이제 importance-sampling ratio를 통해 action value를 추정하는 방법을 알아보자.Off-policy Monte Carlo via Importance Samplingimportance-sampling ratio $\\rho$를 사용하면 behavior policy $b$를 따르는 episode를 통해서도 target policy $\\pi$를 따르는 action value $q_\\pi$를 추정할 수 있다. 아래는 $q_\\pi$를 추정하는 수식이다.\\[q_\\pi(s,a) = \\mathbb{E}[\\rho_{t:T-1}G_t \\ \\vert \\ S_t = s, A_t = a]\\]각 state-action pair에 대한 return $G_t$에 importance-sampling ratio $\\rho$를 곱한 후 이 값들에 대한 expected value를 구하면 된다. 그렇다면 어떻게 expected value를 구할 수 있을까? 가장 간단한 방법은 위 value를 다 더한 뒤 개수로 나눈다. 이 방법을 ordinary importance sampling이라고 한다.3 이 방법은 unbiased하지만 높은 variance를 가진다. 따라서 biased하지만 variance가 매우 낮은 weighted importance sampling을 대안으로 활용한다. 아래는 weighted importance sampling을 활용해 추정한 action value $q_\\pi$이다.\\[q_\\pi(s,a) \\doteq \\dfrac{\\sum_{t \\in \\mathcal{J}(s,a)} \\rho_{t:T(t)-1}G_t}{\\sum_{t \\in \\mathcal{J}(s,a)} \\rho_{t:T(t)-1}}\\]$\\mathcal{J}(s,a)$는 state-action pair s, a가 방문된 time step $t$에 대한 집합이다. $T(t)$는 $t \\in \\mathcal{J}(s,a)$일 때의 termination time이다.Fig 3. Ordinary importance sampling vs weighted importance sampling.(Image source: Sec 5.5 Sutton &amp; Barto (2018).)위 그림은 ordinary importance sampling과 weighted importance sampling을 비교하는 그래프이다. 둘다 error가 0으로 수렴하지만 weighted importance sampling이 더 안정적임을 확인할 수 있다.Incremental Implementation of Weighted Average이제 off-policy Monte Carlo를 구현해보자. weighted average를 incremental 방식으로 구현해보려 한다. 먼저 $n-1$개의 return sequence $G_1, G_2, \\dots, G_{n-1}$과 각각에 대응하는 weight $W_i \\ (\\text{e.g., } W_i = \\rho_{t_i:T(t_i)-1})$가 있다고 하자. weighted average $V_n$은 아래와 같다.\\[V_n \\doteq \\dfrac{\\sum_{k=1}^{n-1} W_k G_k}{\\sum_{k=1}^{n-1} W_k}, \\qquad n \\geq 2\\]위 $V_n$과 weight들의 cumulative sum $C_n = \\sum_{k=1}^{n-1} W_k$을 유지하고 있을 때 추가적인 return $G_n$을 획득할 경우 incremental한 방식으로 $V$를 update할 수 있다. $V$에 대한 update rule은 아래와 같다.\\[V_{n+1} \\doteq V_n + \\dfrac{W_n}{C_n} \\Big[G_n - V_n \\Big] \\qquad n \\geq 1,\\]\\[C_{n+1} \\doteq C_n + W_{n+1}\\]위에서 $C_0 \\doteq 0$이고 $V_1$은 임의의 값이다. 위 $V$를 state value라면 $v$로, action value라면 $q$로 변경하기만 하면 된다.Off-policy MC Prediction Algorithm이제 Off-policy MC methods 알고리즘을 보자. 여기서는 prediction 부분만 보이도록 하겠다. 한가지 중요한 사실은 target policy $\\pi$와 behavior policy $b$ 모두 어떤 policy도 가능하지만 coverage를 만족해야한다. coverage란 $\\pi$에 의해 선택될 수 있는 모든 action은 $b$에 의해서도 선택될 수 있어야 함을 의미한다. 즉, $\\pi(a \\vert s) &gt; 0$면 $b(a \\vert s) &gt; 0$이어야 한다. $\\text{Algorithm: Off-policy MC prediction (policy evaluation) for estimating } Q \\approx q_\\pi$ \\(\\begin{align*}&amp; \\textstyle \\text{Input: an arbitrary target policy } \\pi \\\\&amp; \\textstyle \\text{Initialize, for all } s \\in \\mathcal{S}, \\ a \\in \\mathcal{A}(s) \\text{:} \\\\&amp; \\textstyle \\qquad Q(s,a) \\in \\mathbb{R} \\text{ (arbitrarily)} \\\\&amp; \\textstyle \\qquad C(s,a) \\leftarrow 0 \\\\\\\\&amp; \\textstyle \\text{Loop forever (for each episode):} \\\\&amp; \\textstyle \\qquad b \\leftarrow \\text{any policy with coverage of } \\pi \\\\&amp; \\textstyle \\qquad \\text{Generate an episode following } b \\text{: } S_0, A_0, R_1, \\dots, S_{T-1}, A_{T-1}, R_T \\\\&amp; \\textstyle \\qquad G \\leftarrow 0 \\\\&amp; \\textstyle \\qquad W \\leftarrow 1 \\\\&amp; \\textstyle \\qquad \\text{Loop for each step of episode, } t = T-1, T-2, \\dots, 0, \\text{ while } W \\neq 0 \\text{:} \\\\&amp; \\textstyle \\qquad\\qquad G \\leftarrow \\gamma G + R_{t+1} \\\\&amp; \\textstyle \\qquad\\qquad C(S_t,A_t) \\leftarrow C(S_t,A_t) + W \\\\&amp; \\textstyle \\qquad\\qquad Q(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\frac{W}{C(S_t,A_t)}[G - Q(S_t,A_t)] \\\\&amp; \\textstyle \\qquad\\qquad W \\leftarrow W \\frac{\\pi(A_t \\vert S_t)}{b(A_t \\vert S_t)}\\end{align*}\\)그런데 위 알고리즘을 보면 한가지 이상한 점이 있다. 바로 $W = 1$부터 시작하는 것이다. 우리는 앞서 Importance Sampling에서 importance-sampling ratio $\\rho_{t:T-1}$는 time step $t$에서의 ratio부터 고려했었다. 즉, $\\pi(A_t \\vert S_t) / b(A_t \\vert S_t)$를 고려했었다. 따라서 $W = \\pi(A_{T-1} \\vert S_{T-1}) / b(A_t \\vert S_{T-1})$부터 시작해야한다. 그런데 왜 $W = 1$부터 시작하는걸까? 필자 역시 처음 공부했을 때 이러한 의문이 있었으며 어느 곳에서도 답을 찾을 수 없었다. 추후 다른 chapter를 공부하다가 그 이유를 알게 되었다. 한번 알아보자.Importance Sampling에서 정의했던 importance-sampling ratio $\\rho_{t:T-1}$는 시작 state $S_t$가 주어졌을 때 임의의 policy를 따라 생성된 state-action trajectory를 전제로 했었다.\\[A_t, S_{t+1}, A_{t+1}, \\dots, S_T\\]그런데 우리는 action value를 추정하고 있다. action value 추정의 전제는 state-action pair가 주어져있다는 것이다. 즉, $S_t, A_t$는 이미 주어져있기 때문에 우리가 고려해야할 state-action trajectory는 아래와 같다.\\[S_{t+1}, A_{t+1}, \\dots, S_T\\]따라서 action value를 추정할 떄 필요한 importance-sampling ratio는 $\\rho_{t+1:T-1}$이다. Importance Sampling 파트의 trajectory가 임의의 policy 를 따를 때 발생할 확률에 위 trajectory로 대체해보면 간단히 증명할 수 있다. 위와 같은 이유로 $t = T-1$일 때 $\\rho_{T:T-1} = 1$이므로, $W=1$부터 시작한다. 아래는 Reinforcement Learning: An Introduction에서 importance sampling ratio를 $t+1$부터 시작하는 이유를 설명하는 문장이다. We do not have to care how likely we were to select the action; now that we have selected it we want to learn fully from what happens, with importance sampling only for subsequent actions.4근데 위 문장이 Monte Carlo Methods가 아니라 n-step Bootstrapping chapter에 있었어서 다소 아쉬웠다. 이 문장을 보고 나서야 왜 $t+1$부터 시작하는지 위와 같이 이해할 수 있었다.Summary지금까지 Monte Carlo (MC) methods에 대해 알아보았다. MC methods는 sample episode안의 experience로부터 value function을 학습하고 optimal policy를 찾는다. MC methods는 Generalized Policy Iteration (GPI)를 따른다. MC methods는 episode-by-episode 단위로 GPI를 수행한다. action-value function을 추정할 경우 environment에 대한 지식(dynamics) 없이도 policy를 개선하는 것이 가능하다.MC methods는 sampling을 통해 학습하는 방법이기 때문에 충분한 exploration을 보장해주어야 한다. 이에 대한 방법으로 exploring starts와 on-policy methods, off-policy methods가 있다. exploring start는 state-action pair를 랜덤하게 시작하는 방법이지만 현실과는 동떨어진 방법이다. on-policy methods는 하나의 policy로 학습과 탐색을 모두 수행한다. off-policy methods는 학습에 사용되는 target policy와 탐색에 사용되는 behavior policy로 분리하는 방법이다.off-policy methods는 behavior policy를 따라 생성된 data로부터 target policy를 학습하는데 이 두 policy의 distribution이 다르기 때문에 문제가 발생한다. 이를 해결하기 위해 importance sampling이라는 기법을 사용해 behavior policy의 distribution으로부터 target policy의 expected value를 추정한다. 이때 ordinary importance sampling과 weighted importance sampling 2가지 방법이 존재하는데 일반적으로 분산이 낮은 weighted importance sampling이 선호된다.MC methods는 DP와 주요한 2가지 차이점이 있다. 먼저, MC methods는 DP와 달리 environment에 대한 지식(dynamics) 없이 sample experience로부터 학습이 가능하다. 두번째는 MC methods는 bootstrap하지 않다. 즉, value function을 update할 때 DP와 달리 다른 value function의 추정치를 통해 update하지 않고 return을 직접 구해 update한다.References[1] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction; 2nd Edition. 2018.[2] Towards Data Science. Sagi Shaier. Monte Carlo Methods.[3] 생각많은 소심남. [RL] Off-policy Learning for Prediction.Footnotes StackExchange. Why are state-values alone not sufficient in determining a policy (without a model)?. &#8617; Reinforcement Learning: An Introduction; 2nd Edition. 2018. Sec. 5.4, p.123. &#8617; Reinforcement Learning: An Introduction; 2nd Edition. 2018. Sec. 5.5, p.126. &#8617; Reinforcement Learning: An Introduction; 2nd Edition. 2018. Sec. 7.3, p.171. &#8617; " }, { "title": "Dynamic Programming in RL", "url": "/reinforcement-learning/rl-fundamental/dynamic-programming-in-rl/", "categories": "Reinforcement Learning, RL Fundamental", "tags": "RL, AI, DP", "date": "2022-06-25 00:00:00 +0900", "snippet": "이 포스트에서는 RL에서 MDP로 environment의 perfect model이 주어졌을 때 optimal policy를 구하는데 사용되는 기초적인 방식인 Dynamic Programming (DP)를 소개한다.IntroductionReinforcement Learning (RL)에서 environment가 perfect model로 environment의 지식 (일반적으로 MDP)을 완전히 알 수 있다면 Dynamic Programming (DP)를 활용해 optimal policy를 계산할 수 있다. 즉, 모든 가능한 transition의 probability distribution을 완전히 알고 있어야 한다. 이는 굉장히 특수한 상황이다. 우리는 보통 어떤 state에서 action을 선택해 next state로의 transition이 발생했을 때 next state로 transition되었다는 결과만 안다. next state가 얼마나 존재하고 각 next state로 transition될 확률이 어느정도인지 알지 못한다. 이때는 sampling으로 획득한 environment에 대한 experience를 통해 RL 문제를 해결한다.그렇다면 DP란 무엇일까? DP는 복잡한 문제를 간단한 여러 개의 문제로 나누어 푸는 최적화 기법이다. DP는 일반적으로 아래와 같은 2가지 유형의 문제에 적용된다. Overlapping Subproblems Optimal SubstructureOverlapping Subproblems는 동일한 sub problem들이 반복적으로 요구될 때 연산 결과를 저장했다가 사용할 수 있음을 의미한다. Optimal Substructure은 주어진 문제를 sub problem들로 쪼갠 뒤 각각의 sub problem들의 최적해를 사용하여 원래 문제의 최적해를 구할 수 있음을 의미한다.RL에 적용되는 DP의 핵심 아이디어는 위 2가지 특성을 모두 반영해 Bellman equation을 update rule로 전환하는 것이다. 이를 통해 value function을 근사시켜 RL 문제를 해결할 수 있다. 이 과정이 어떻게 진행되는지 알아보자.Generalized Policy IterationGeneralized Policy Iteration (GPI)는 RL 문제를 풀 때 사용되는 일반적인 접근 방법이다. GPI에는 다음과 같은 2가지 과정이 존재한다. Policy Evaluation - policy $\\pi$를 따를 때 value function $v_\\pi$를 계산 Policy Improvement - 계산된 현재 value function을 통해 policy $\\pi$를 개선위 policy evaluation과 policy improvement는 번갈아 가며 수행된다. 아래 다이어그램을 보면 조금 더 직관적으로 이해할 수 있다.Fig 1. GPI diagram.(Image source: Sec. 4.6 Sutton &amp; Barto (2018).)policy evaluation과 policy improvement는 서로 경쟁하면서 협력하는 관계로 볼 수 있다. 이 둘은 서로를 반대 방향으로 잡아 당긴다. policy evaluation에서는 현재 policy $\\pi$에 관해 value function $v_\\pi$가 계산된다. 즉, value function을 policy 쪽으로 끌어당긴 셈이다. 반대로 policy improvement에서는 value function $v_\\pi$에 관해 policy $\\pi$가 개선되기 때문에 policy를 value function 쪽으로 끌어 당겼다. 이렇게 서로 끌어당기다 보면 어느 시점에 한 지점에 도달하게 되고 이 때가 바로 optimal value function과 policy이다. 이러한 관계를 나타내는 그림은 아래와 같다.Fig 2. GPI relationship.(Image source: Sec. 4.6 Sutton &amp; Barto (2018).)몰론 실제로는 엄청나게 복잡한 과정이 내부에서 발생하지만 직관적으로 위와 같이 GPI를 이해할 수 있다. GPI에 대해 알아보았으니 이제 DP에서 GPI가 어떻게 적용되는지 알아보자.Policy Evaluation (Prediction)DP에서의 policy evaluation에 대해 알아보자. policy evaluation은 prediction으로 불리기도 한다. 먼저 Bellman equation을 리마인드하자.\\[\\begin{align} v_\\pi(s) &amp;\\doteq \\mathbb{E}_\\pi[R_{t+1} + \\gamma v_\\pi(S_{t+1}) \\ \\vert \\ S_t = s] \\\\ &amp;= \\sum_a\\pi(a \\vert s) \\sum_{s',r}p(s',r \\vert s,a) \\Big[r + \\gamma v_\\pi(s') \\Big] \\tag{1}\\end{align}\\]DP에서는 위 Bellman equation을 아래와 같은 연속적인 update rule로 적용해 value function $v(s)$를 풀 수 있다.\\[\\begin{align} v_{k+1}(s) &amp;\\doteq \\mathbb{E}_\\pi[R_{t+1} + \\gamma v_k(S_{t+1}) \\ \\vert \\ S_t = s] \\\\ &amp;= \\sum_a\\pi(a \\vert s) \\sum_{s',r}p(s',r \\vert s,a) \\Big[r + \\gamma v_\\pi(s') \\Big] \\tag{2}\\end{align}\\]DP에서는 위 update rule이 모든 state $s \\in \\mathcal{S}$에 대해 수행되며 $v_k$는 일반적으로 $k \\rightarrow \\infty$이면 수렴한다. 이 때 terminal state의 value는 항상 0이어야 한다. 이렇게 iterative하게 value function을 구하는 알고리즘을 iterative policy evaluation이라고 부른다.iterative policy evaluation이 DP인 이유는 update rule을 수행하는 방식 때문이다. 기존 Bellman equation (1)을 Exhaustive search를 통해 푼다고 생각해보자. 현재 state $s$의 value $v_\\pi(s)$는 transition된 next state $s’$의 $v_\\pi(s’)$으로부터 계산된다. $v_\\pi(s’)$은 다시 transition된 s’의 후속 state $s’‘$의 $v_\\pi(s’’)$로부터 계산된다. 즉, 재귀적 관계로 연속된 모든 후속 MDP를 고려해야하는 굉장히 비효율적인 방식이다.DP에서는 기존 Bellman equation을 update rule로 전환해 iterative하게 만들었다. 이때 연속된 모든 후속 state를 관찰하는게 아니라 현재 state $s$에서 가능한 next state들에 대한 one-step transition만을 고려한다. 현재 state의 new value $v_{k+1}(s)$는 후속 state $s’$의 old value $v_k(s’)$로부터 계산되며 다시 재귀적으로 계산하지 않고 지금까지 계산된 $v_k(s’)$을 그대로 사용하겠다는 것이다. 즉, optimal policy를 찾기 위해 각 state별로 쪼개 optimal state value를 구하며 이때 지금까지 구한 state value를 저장해 놓았다가 다음 iteration에서 연산 시 사용하기 때문에 DP의 2가지 특성을 모두 지니고 있다. 아래 그림을 보자.Fig 3. DP backup diagram.(Image source: Robotic Sea Bass. An Intuitive Guide to Reinforcement Learning.)위 그림은 DP의 backup diagram으로 어떤 과정을 거쳐 state value $v_{k+1}(s)$가 계산되는지를 보여준다. 이때 흰색 원은 state, 검은색 원은 action이다. Exhaustive search로 Bellman equation을 푼다면 위 backup diagram의 아래 모든 후속 state들을 고려해야 한다. 그러나 DP에서는 현재 state에서의 one-step transition만을 고려하기 때문에 위 backup diagram에 표시된 빨간색 영역만을 고려한다.iterative policy evaluation의 각 iteration은 모든 state에 대해 한번에 이러한 update rule을 수행한다. 따라서 iterative policy evaluation을 수행하기 위해서는 일반적으로 old value를 저장한 array와 new value를 계산하기 위한 array가 각각 필요하다. 그러나 실제로는 한개의 array로 old value의 보관과 new value의 계산을 동시에 수행한다. 이때는 new value가 계산되면 기존 old value를 실시간으로 덮어쓴다. 따라서 각 state에 대한 value 계산 시 old value가 참조될 수도 있고, 이미 계산이 완료된 new value가 참조될 수도 있다. 2-array든 1-array 방식이든 수렴성은 보장되며 1-array 방식이 수렴속도가 일반적으로 더 빠르다. 다만 1-array 방식은 state value를 update하는 순서에 따라 수렴 속도가 변한다.지금까지 DP를 통해 state value $v_\\pi$를 계산하는 방법을 알아보았다. 이제 계산된 state value를 통해 policy $\\pi$를 개선해보자.Policy Improvement임의의 deterministic policy $\\pi$를 따를 때의 state value $v_\\pi$가 결정되었다고 가정하자. deterministic policy의 의미는 policy $\\pi$의 결과가 action들의 확률 분포가 아닌 action 그 자체인 경우를 말한다. 이 때 어떻게 기존 policy를 개선할 수 있을까? 이는 action value $q_\\pi$를 통해 수행할 수 있다. 먼저 action value $q_\\pi$를 remind 하자.\\[\\begin{align} q_\\pi(s,a) &amp;\\doteq \\mathbb{E}[R_{t+1} + \\gamma v_\\pi(S_{t+1}) \\ \\vert \\ S_t = s, A_t = a] \\\\ &amp;= \\sum_{s',r}p(s',r \\vert s,a)\\Big[r + \\gamma v_\\pi(s') \\Big] \\tag{3}\\end{align}\\]모든 state $s \\in \\mathcal{S}$에 대해 action value가 아래와 같은 관계를 만족한다고 하자.\\[q_\\pi(s, \\pi'(s)) \\geq v_\\pi(s) \\tag{4}\\]이 경우 모든 state로부터 아래와 같이 더 많거나 같은 expected return을 얻을 수 있다.\\[v_\\pi'(s) \\geq v_\\pi(s) \\tag{5}\\]이를 policy improvement theorem이라고 한다. 그렇다면 어떻게 수식 (4)를 만족할 수 있을까? 가장 간단한 방법은 각 state에서 action value $q_\\pi(s,a)$가 최대인 action을 선택하는 것이다. $v_\\pi(s)$는 action value들의 expectation이다. 따라서 최대 action value보다 작거나 같다. 이러한 방식을 greedy policy라고 하며 new policy $\\pi’$은 아래와 같다.\\[\\pi'(s) \\doteq \\underset{a}{\\arg\\max} \\ q_\\pi(s, a) \\tag{6}\\]개선된 정책 $\\pi’(s)$는 기존 $\\pi(s)$에 의한 action과 같을 수도 있고 다를 수도 있지만 무엇이든 간에 분명히 기존 policy만큼 좋거나 더 나을것이다. 기존 policy에 의한 value function에 관해 greedy하게 선택하는 과정을 policy improvement라고 한다.만약 new greedy policy $\\pi’$이 기존 policy $\\pi$와 동일하면 어떨까? 이 경우 기존 policy와 동일한 action을 선택하기 때문에 모든 state에 대해 $v_\\pi = v_{\\pi’}$이 되며 수식 (6)에 의해 아래와 같은 수식을 만족한다.\\[v_{\\pi'}(s) = \\max_a \\mathbb{E}[R_{t+1} + \\gamma v_{\\pi'}(S_{t+1}) \\ \\vert \\ S_t = s, A_t = a] \\tag{7}\\]그런데 위 수식은 Bellman optimality equation과 동일하다. 따라서 $v_{\\pi’}$은 $v_\\ast$이며 $\\pi$와 $\\pi’$ 모두 optimal policy이다.Policy Iterationpolicy iteration은 policy evaluation과 policy improvement를 번갈아 수행하는 방법이다. 어떤 policy $\\pi$를 평가해 $v_\\pi$를 구한 뒤 이를 바탕으로 더 나은 policy $\\pi’$를 얻는다. 다시 $\\pi’$을 평가해 $v_{\\pi’}$을 구한 뒤 이를 바탕으로 더 나은 policy $\\pi’‘$를 얻는다. 이러한 과정을 반복하는 것이 바로 policy iteration이다. 아래는 policy iteration을 sequence 형태이다.\\[\\pi_0 \\overset{E}{\\longrightarrow} v_{\\pi_0} \\overset{I}{\\longrightarrow} \\pi_1 \\overset{E}{\\longrightarrow} v_{\\pi_1} \\overset{I}{\\longrightarrow} \\pi_2 \\overset{E}{\\longrightarrow} \\cdots \\overset{I}{\\longrightarrow} \\pi_\\ast \\overset{E}{\\longrightarrow} v_\\ast\\]위 수식에서 $\\overset{E}{\\longrightarrow}$는 policy evaluation, $\\overset{I}{\\longrightarrow}$는 policy improvement를 나타낸다. finite MDP는 유한하기 때문에 이러한 프로세스는 유한한 iteration 안에 optimal policy와 optimal value function으로 반드시 수렴한다.아래는 policy iteration 알고리즘이다. $\\text{Algorithm: Policy Iteration (using iterative policy evaluation) for estimating } \\pi \\approx \\pi_\\ast$ \\(\\begin{align*}&amp; \\textstyle \\text{1. Initialization} \\\\&amp; \\textstyle \\qquad V(s) \\in \\mathbb{R} \\text{ and } \\pi(s) \\in \\mathcal{A}(s) \\text{ arbitrarily for all } s \\in \\mathcal{S} \\\\\\\\&amp; \\textstyle \\text{2. Policy Evaluation} \\\\&amp; \\textstyle \\qquad \\text{Loop:} \\\\&amp; \\textstyle \\qquad\\qquad \\Delta \\leftarrow 0 \\\\&amp; \\textstyle \\qquad\\qquad \\text{Loop for each } s \\in \\mathcal{S} \\text{:} \\\\&amp; \\textstyle \\qquad\\qquad\\qquad v \\leftarrow V(s) \\\\&amp; \\textstyle \\qquad\\qquad\\qquad V(s) \\leftarrow \\sum_{s',r}p(s',r \\vert s, \\pi(s))[r + \\gamma V(s')] \\\\&amp; \\textstyle \\qquad\\qquad\\qquad \\Delta \\leftarrow \\max(\\Delta, \\vert v - V(s) \\vert) \\\\&amp; \\textstyle \\qquad \\text{until } \\Delta &lt; \\theta \\text{ (a small positive number determining the accuracy of estimation)} \\\\\\\\&amp; \\textstyle \\text{3. Policy Improvement} \\\\&amp; \\textstyle \\qquad \\textit{policy-stable} \\leftarrow \\textit{true} \\\\&amp; \\textstyle \\qquad \\text{For each } s \\in \\mathcal{S} \\text{:} \\\\&amp; \\textstyle \\qquad\\qquad \\textit{old-action} \\leftarrow \\pi(s) \\\\&amp; \\textstyle \\qquad\\qquad \\pi(s) \\leftarrow \\arg\\max_a \\sum_{s',r}p(s',r \\vert s,a)[r + \\gamma V(s')] \\\\&amp; \\textstyle \\qquad\\qquad \\text{If } \\textit{old-action} \\neq \\pi(s),\\text{ then } \\textit{policy-stable} \\leftarrow \\textit{false} \\\\&amp; \\textstyle \\qquad \\text{If } \\textit{policy-stable} \\text{, then stop and return } V \\approx v_\\ast \\text{ and } \\pi \\approx \\pi_\\ast; \\text{ else go to 2}\\end{align*}\\)참고로 2. Policy Evaluation에서 state value $V(s)$를 구할 때와 3. Policy Improvement에서 개선된 policy $\\pi(s)$를 얻기 위해 action value를 구할 때의 수식이 동일한데 그 이유는 policy $\\pi$를 deterministic 하다고 가정했기 때문이다. state value를 구할 때 policy $\\pi$를 따를 때의 action value $q_\\pi(s, a)$에 대한 expectation을 취하는데 policy $\\pi$를 따르는 action $a$의 확률은 1, 나머지 action은 모두 0이기 때문에 $a = \\pi(s)$에 대한 action value $q_\\pi(s, \\pi(s))$가 곧 state value이다.Value Iterationpolicy iteration은 policy evaluation을 통해 현재 policy에 대한 value function을 수렴시키고 나서 policy improvement를 수행할 수 있었다. 그러나 value iteration 기법은 policy evaluation을 수행 시 현재 policy에 대한 value function을 수렴시키지 않는다. 대신 현재 policy에 대한 value function을 딱 한 번만 계산 후 바로 policy improvement를 수행한다. 즉, policy evaluation의 1 iteration과 policy improvement를 결합한 것을 반복해서 수행한다. 이 과정을 하나의 단순한 update rule로 나타낼 수 있다.\\[\\begin{align} v_{k+1}(s) &amp;\\doteq \\max_a\\mathbb{E}[R_{t+1} + \\gamma v_k(S_{t+1}) \\ \\vert \\ S_t = s, A_t = a] \\\\ &amp;= \\max_a \\sum_{s',r}p(s',r \\vert s,a) \\Big[r + \\gamma v_k(s') \\Big] \\tag{8}\\end{align}\\]위 수식처럼 나타낼 수 있는 이유는 policy improvement시 action value가 최대인 action으로 policy가 개선되기 때문이다. 즉, greedy policy이기 때문에 다음 policy evaluation에서 action value가 최대인 action을 제외한 나머지 action이 policy에 의해 선택될 확률은 0다. 따라서 한 번의 update에서 action value의 max값을 선택하는 것과 동일해진다.또한 위 수식 (8)은 Bellman optimality equation과 동일하다. 즉, Bellman optimality equation을 iterative한 update rule로 변경한 것이 value iteration이다. value iteration 역시 $k \\rightarrow \\infty$이면 $v_\\ast$로 수렴하며 이때의 greedy policy가 곧 $\\pi_\\ast$이다.아래는 value iteration 알고리즘이다. $\\text{Algorithm: Value Iteration, for estimating } \\pi \\approx \\pi_\\ast$ \\(\\begin{align*}&amp; \\textstyle \\text{Algorithm parameter: a small threshold } \\theta &gt; 0 \\text{ determining accuracy of estimation} \\\\&amp; \\textstyle \\text{Initialize } V(s) \\text{, for all } s \\in \\mathcal{S}^+ \\text{, arbitrarily except that } V(\\textit{terminal}) = 0 \\\\\\\\&amp; \\textstyle \\text{Loop:} \\\\&amp; \\textstyle \\qquad \\Delta \\leftarrow 0 \\\\&amp; \\textstyle \\qquad \\text{Loop for each } s \\in \\mathcal{S} \\text{:} \\\\&amp; \\textstyle \\qquad\\qquad v \\leftarrow V(s) \\\\&amp; \\textstyle \\qquad\\qquad V(s) \\leftarrow \\max_a \\sum_{s',r}p(s',r \\vert s,a)[r + \\gamma V(s')] \\\\&amp; \\textstyle \\qquad\\qquad \\Delta \\leftarrow \\max(\\Delta, \\vert v - V(s) \\vert) \\\\&amp; \\textstyle \\text{until } \\Delta &lt; \\theta \\\\\\\\&amp; \\textstyle \\text{Output a deterministic policy, } \\pi \\approx \\pi_\\ast \\text{, such that} \\\\&amp; \\textstyle \\qquad \\pi(s) = \\arg\\max_a \\sum_{s',r}p(s',r \\vert s,a)[r + \\gamma V(s')]\\end{align*}\\)지금까지 value iteration에 대해 알아보았다.Summary이번 포스트에서는 finite MDPs를 풀기 위해 dynamic programming 기법을 활용한 방법을 알아보았다. policy evaluation은 주어진 policy에 대한 value function을 계산한다. policy improvement는 계산된 value function을 바탕으로 policy를 개선한다. DP에서의 policy iteration은 policy evaluation과 policy improvement를 번갈아 수행한다. 반면 value iteration은 policy evaluation의 1 iteration과 policy improvement를 결합한 방식을 수행한다. 이때 policy iteration은 Bellman expected equation, value iteration은 Bellman optimality equation이 사용된다는 차이가 있다. DP에서의 이러한 과정은 generalized policy iteration (GPI)로 나타낼 수 있으며 이는 대부분의 reinforcement learning (RL)에도 적용된다. 따라서 RL에서의 DP를 이해하는 것은 중요하다고 볼 수 있다.References[1] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction; 2nd Edition. 2018.[2] Towards Data Science. Rohan Jagtap. Dynamic Programming in RL." }, { "title": "Finite Markov Decision Processes", "url": "/reinforcement-learning/rl-fundamental/finite-markov-decision-processes/", "categories": "Reinforcement Learning, RL Fundamental", "tags": "RL, AI, MDP", "date": "2022-05-30 00:00:00 +0900", "snippet": "이 포스트에서는 Reinforcement Learning에서 기반이 되는 finite Markov Decision Processes (MDPs)와 finite MDPs 문제를 해결하기 위한 Bellman equations에 대해 소개한다.What is MDPsMarkov Decision Processes (MDPs)는 연속적인 의사 결정을 형식화한 프레임이다. MDPs와 Multi-armed bandits 환경의 가장 큰 차이점은 MDPs에서는 선택한 action들이 environment의 states를 변경시켜 future rewards에 영향을 미친다는 점이다. 즉, actions와 states가 연관성이 있는 associative setting이다. MDPs는 $(\\mathcal{S}, \\mathcal{A}, P, R)$로 구성되며 각 요소는 아래와 같다. $\\mathcal{S}$ - a set of states $\\mathcal{A}$ - a set of actions $P$ - state-transition probability function $R$ - reward functionMDPs에서는 앞서 말했듯이 actions가 immediate rewards 뿐만 아니라 이후의 states들과 future rewards에 영향을 미친다. 그렇기 때문에 MDPs에서는 immediate rewards와 future rewards 사이에 tradeoff를 할 필요가 있다.MDPs에서 learner이자 decision maker를 agent라고 하며, agent가 상호작용하는 agent 외의 모든 요소를 environment라고 한다. decision making은 agent가 action을 선택하는 행위이다. discrete time steps $t = 0, 1, 2, 3, \\dots$이 있을 때 각 time step $t$에서 agent는 environment의 state $S_t \\in \\mathcal{S}$에서 action $A_t \\in \\mathcal{A}(s)$를 선택한다. 그러면 새로운 state $S_{t+1}$로 전이되고 environment로부터 numerical reward $R_{t+1} \\in \\mathcal{R} \\subset \\mathbb{R}$을 획득한다. 아래는 MDP에 대한 그림이다.Fig 1. The agent–environment interaction in a Markov decision process.(Image source: Sec. 3.1 Sutton &amp; Barto (2018).)위 과정은 매 time step 마다 끊임없이 반복되며 MDP와 agent는 아래와 같은 sequence 혹은 trajectory를 생성한다.\\[S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, \\dots\\]이러한 states, actions, rewards ($\\mathcal{S}$, $\\mathcal{A}$, $\\mathcal{R}$) 모두 유한할 때 finite MDP라고 한다. 이 때 $R_t$와 $S_t$는 잘 정의된 discrete probability distribution으로 이전의 모든 state나 action들이 아닌 오직 직전 state $S_{t-1}$과 action $A_{t-1}$에만 의존하며 $S_{t-1}$과 $A_{t-1}$이 주어졌을 때 $S_t$와 $R_t$가 발생할 확률 $p$를 정의할 수 있다.\\[p(s', r \\vert s, a) \\doteq \\text{Pr}\\lbrace S_t = s', R_t = r \\ \\vert \\ S_{t-1} = s, A_{t-1} = a \\rbrace\\]위 function $p$는 MDP의 dynamics를 정의한다. state는 미래에 대한 차이를 만들어내는 과거의 agent-environment interaction에 대한 모든 측면의 정보를 포함해야하며, 이 때 state는 Markov property를 가진다고 말한다.위에서 정의한 dynamics $p$로부터 state-transition probability를 유도할 수 있다.\\[p(s' \\vert s, a) \\doteq \\text{Pr}\\lbrace S_t = s' \\ \\vert \\ S_{t-1} = s, A_{t-1} = a \\rbrace = \\sum_{r \\in \\mathcal{R}}p(s', r \\vert s, a)\\]또한 state-action pair에 대한 expected reward를 계산할 수 있다.\\[r(s, a) \\doteq \\mathbb{E}[R_t \\ \\vert \\ S_{t-1} = s, A_{t-1} = a] = \\sum_{r \\in \\mathcal{R}}r\\sum_{s' \\in \\mathcal{S}}p(s', r \\vert s, a)\\]state-action-next-state에 대한 expected reward 역시 계산할 수 있다.\\[r(s, a, s') \\doteq \\mathbb{E}[R_t \\ \\vert \\ S_{t-1} = s, A_{t-1} = a, S_t = s'] = \\sum_{r \\in \\mathcal{R}}r\\dfrac{p(s', r \\vert s, a)}{p(s' \\vert s, a)}\\]이 포스트에서는 위 수식 중 dynamics $p(s’, r \\vert s, a)$를 주로 사용하였다.Goals in RLReinforcement Learning (RL)에서 agent는 immediate reward가 아닌 오랜 기간에 걸친 cumulative reward를 maximize하는 것을 목표로 한다. reward는 우리가 달성하고자 하는 것을 나타내는 중요한 지표이다. 주의할 점은 이 reward signal을 설정할 때 how가 아닌 what의 관점으로 설정해야한다. 달성하고자 하는 목표가 무엇인지에 초점을 맞추되 이것을 달성하기 위한 지식을 제시해서는 안된다.Episode배틀그라운드라는 게임을 생각해보자. 이 게임은 배틀로얄 장르로 매치 시작 시 비행기에서 낙하 후 총기를 비롯한 아이템을 파밍해 전투를 펼치는 게임이다. 각 매치는 싱글플레이 기준 매치 도중 사망하거나 적이 전부 사망해 홀로 생존 시 종료되며 다시 매치 시작 시 이전 매치에서 획득한 총기, 아이템 등은 전부 초기화된다. 각 매치는 사망 혹은 홀로 생존과 같이 terminal state가 존재하는데 게임 내 모든 상호작용을 하나의 sequence라고 볼 때 매치 단위의 subsequence로 쪼갤 수 있다. 이러한 subsequence를 episode라고 하는데 episode는 앞서 언급한 terminal state에 종료된다. terminal state는 주로 게임에서의 승리나 패배와 같다. episode가 terminal state에 도달해 종료되면 다시 처음 state로 초기화되고 새로운 episode가 시작된다. 새로운 episode는 이전 episode와 독립적인 관계이다. 이러한 종류의 episodes를 가진 tasks를 episodic tasks라고 부른다.Returncumulative reward를 수학적으로 정의한 것이 expected return $G_t$이며 이는 time step $t$ 이후에 획득한 rewards sequence $R_{t+1}, R_{t+2}, R_{t+3}, \\dots ,$에 대한 함수이다. $G_t$를 구할 때 단순히 rewards sequence의 합으로 구할 수 있지만 이는 episodic tasks에서만 유효하다. terminal state가 존재하지 않는 continuing tasks에서는 무한한 time steps에서 rewards를 획득하기 때문에 $G_t \\rightarrow \\infty$가 될 것이다. 따라서 episodic tasks 뿐만 아니라 continuing tasks에서도 expected return $G_t$를 구하기 위해 일반적으로 미래가 고려된 discounted rewards sequence의 합으로 구한다. 이를 discounted return이라고 하며 수식은 아래와 같다.\\[G_t \\doteq R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots = \\sum_{k=t+1}^T \\gamma^{k-t-1}R_k\\]$T$는 termination time으로 continuing task면 $T = \\infty$이다. $0 \\leq \\gamma \\leq 1$는 discount rate로 더 먼 미래에 획득한 reward일수록 더 많이 discount한다.위 수식을 연속적인 time steps에서의 return 형태로 변경할 수 있다. 즉, 재귀적으로 변경할 수 있는데 이는 강화학습 전반에서 굉장히 중요한 수식이다.\\[\\begin{aligned} G_t &amp;\\doteq R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\gamma^3 R_{t+4} \\cdots \\\\ &amp;= R_{t+1} + \\gamma(R_{t+2} + \\gamma R_{t+3} + \\gamma^2 R_{t+4} \\cdots) \\\\ &amp;= R_{t+1} + \\gamma G_{t+1}\\end{aligned}\\]Value Functionvalue function은 states 혹은 state-action pairs가 얼마나 좋은지를 추정하는 함수이다. “얼마나 좋은가”는 보통 expected return의 관점에서 정의된다. 미래에 획득할 rewards는 agent가 행동하는 방식에 의존하기 때문에 이에 대한 value functions는 agent의 행동 방식인 policy에 영향을 받는다. policy는 각 state에서 가능한 각 행동들의 선택 확률로 agent가 time step $t$에서 policy $\\pi$를 따를 때, $\\pi(a \\vert s)$는 $S_t = s$일 때 $A_t = a$일 확률이다.agent가 state $s$에 있을 때 agent에게 얼마나 좋은지를 추정하는 state-value function $v_\\pi(s)$는 state $s$에서 시작하고 policy $\\pi$를 따를 때 얻을 수 있는 expected return이다. MDPs에서는 $v_\\pi$를 아래와 같이 정의할 수 있다.\\[v_\\pi(s) \\doteq \\mathbb{E}_\\pi[G_t \\ \\vert \\ S_t = s]\\]주의할 점은 terminal state의 value는 항상 0이다.위와 비슷하게 policy $\\pi$에 대한 action-value function $q_\\pi$는 state $s$에서 policy $\\pi$에 따라 action $a$를 선택했을 때 얻을 수 있는 expected return이다.\\[q_\\pi(s, a) \\doteq \\mathbb{E}_\\pi[G_t \\ \\vert \\ S_t = s, A_t = a]\\]value function $v_\\pi$와 $q_\\pi$는 경험으로부터 추정된다. 경험이란 agent가 직접 states에 방문해보고 actions를 선택함으로써 얻게 되는 return과 같은 정보들을 말한다.Bellman Expectation Equationstate-value function은 expected return $G_t \\doteq R_{t+1} + \\gamma G_{t+1}$와 같이 재귀적 관계를 만족한다. 즉, $v_\\pi$를 현재 state value와 후속 state value 사이의 관계로 나타낼 수 있으며 이를 $v_\\pi$에 대한 Bellman expectation equation이라고 한다. 수식은 아래와 같으며 복잡한 증명은 생략한다.\\[\\begin{aligned} v_\\pi(s) &amp;\\doteq \\mathbb{E}_\\pi[G_t \\ \\vert \\ S_t = s] \\\\ &amp; = \\mathbb{E}_\\pi[R_{t+1} + \\gamma v_\\pi(S_{t+1}) \\ \\vert \\ S_t = s]\\end{aligned}\\]action-value function 역시 위와 마찬가지로 재귀적 관계를 나타내는 $q_\\pi$에 대한 Bellman expectation equation으로 나타낼 수 있다.\\[q_\\pi(s, a) \\doteq \\mathbb{E}_\\pi[R_{t+1} + \\gamma q_\\pi(S_{t+1}, A_{t+1}) \\ \\vert \\ S_t = s, A_t = a]\\]$v_\\pi$에 대한 Bellman expectation equation을 아래와 같은 backup diagram으로 나타낼 수 있다. 참고로 backup diagram인 이유는 후속 states에서의 value로부터 역으로 현재 state에서의 value를 구하는 backup operation 관계를 표현하고 있기 때문이다.Fig 2. Backup diagram for state-value.(Image source: Sec. 3.5 Sutton &amp; Barto (2018).)비어있는 circle은 $v_\\pi$, 검은색 circle은 $q_\\pi$를 나타낸다. 가장 위의 node는 $v_\\pi(s)$로 바로 아래의 $q_\\pi(s, a)$를 가리킨다. 즉, $v_\\pi$에 대한 Bellman expectation equation은 어떤 state에서 선택 가능한 각 action들의 action-value $q_\\pi$들에 대한 expectation이다. 따라서 $v_\\pi$를 아래와 같은 수식으로 나타낼 수 있다.\\[v_\\pi(s) \\doteq \\sum_a\\pi(a \\vert s)q_\\pi(s, a)\\]$q_\\pi$에 대한 Bellman expectation equation의 backup diagram은 아래와 같다.Fig 3. Backup diagram for action-value.(Image source: Sec. 3.5 Sutton &amp; Barto (2018).)$p$는 MDP의 dynamics로 state $s$에서 action $a$를 선택했을 때 reward $r$과 next state $s’$이 발생할 확률이다. 가장 위의 node는 $q_\\pi(s, a)$로 바로 아래의 $v_\\pi(s’)$을 가리킨다. 즉, $q_\\pi(s, a)$는 어떤 state $s$에서 action $a$를 선택했을 때 획득한 return들의 expectation으로 나타낼 수 있다.\\[q_\\pi(s, a) \\doteq \\sum_{s', r} p(s', r \\vert s, a) \\Big[r + \\gamma v_\\pi(s') \\Big]\\]위 두 Bellman expectation equation을 바탕으로 $v_\\pi$를 아래와 같이 정의할 수 있다.\\[v_\\pi(s) \\doteq \\sum_a \\pi(a \\vert s) \\sum_{s', r}p(s', r \\vert s, a)\\Big[r + \\gamma v_\\pi(s') \\Big]\\]지금까지 알아본 내용은 특정 policy $\\pi$를 따를 때 value를 추정하는 방법이다. 그러나 이것은 MDP에서 할 수 있는 최적의 방식이 아니다. 어디까지나 특정 policy $\\pi$에 대한 value 추정일 뿐이다. 이제 MDP의 문제를 해결하는 방법을 알아보자.Optimal Value FunctionRL은 cumulative reward를 maximize하는 것이 목표로 한다. 위 Bellman equation의 수식을 보면 policy $\\pi$에 따라 expected return을 나타내는 state value가 달라짐을 알 수 있다. 즉, cumulative reward는 policy $\\pi$에 의존한다. 따라서 cumulative reward를 maximize하는 optimal policy $\\pi_\\ast$를 찾는 것이 목적이며 state-value function이 optimal policy를 따를 때 optimal state-value function $v_\\ast$라 한다. 이때 $v_\\ast$는 모든 policy에 대해 가장 큰 state-value function이다.\\[v_\\ast(s) \\doteq \\max_\\pi v_\\pi(s)\\]state-value는 expected return을 나타내기 때문에 optimal policy $\\pi_\\ast$를 따르는 optimal state-value 역시 maximize된다.위와 마찬가지로 optimal action-value function $q_\\ast$는 모든 policy에 대해 가장 큰 action-value function이다.\\[q_\\ast(s, a) \\doteq \\max_\\pi q_\\pi(s, a)\\]Optimal policy그렇다면 optimal policy $\\pi_\\ast$를 어떻게 찾을 수 있을까? 어떤 한 policy $\\pi$와 다른 policy $\\pi’$이 있다고 할 때 모든 states에 대한 $\\pi$를 따르는 value function이 모든 states에 대한 $\\pi’$을 따르는 value function보다 크거나 같을 때 더 좋은 policy라고 판단할 수 있다. 이를 수식으로 나타내면 아래와 같다.\\[\\pi \\geq \\pi' \\ \\text{if} \\ v_\\pi(s) \\geq v_{\\pi'}(s) \\ \\text{for all} \\ s \\in \\mathcal{S}\\]Bellman Optimality EquationBellman optimality equation은 Bellman expectation equation과 비슷하나 value들에 대한 expectation이 아닌 maximum value만을 고려한다는 차이가 있다. 아래는 optimal state-value function $v_\\ast$에 대한 backup diagram이다.Fig 4. Backup diagram for optimal state-value.(Image source: Sec. 3.6 Sutton &amp; Barto (2018).)가장 위의 node $v_\\ast(s)$는 action-value에 대해 maximum value를 선택한다. 즉, $v_\\ast$에 대한 Bellman optimality equation은 optimal policy를 따르는 state value가 그 state에서의 best action에 대한 expected return 혹은 action value와 동일하다.\\[\\begin{aligned} v_\\ast(s) &amp;= \\max_{a \\in \\mathcal{A}(s)} q_{\\pi_\\ast}(s, a) \\\\\\end{aligned}\\]아래는 optimal action-value function function $q_\\ast$에 대한 backup diagram이다.Fig 5. Backup diagram for optimal action-value.(Image source: Sec. 3.6 Sutton &amp; Barto (2018).)가장 위의 node는 $q_\\ast(s, a)$로 바로 아래의 $v_\\ast(s’)$을 가리킨다. $q_\\ast$에 대한 Bellman optimality equation은 Bellman expectation equation과 같이 여전히 기댓값을 취하지만 유일한 차이점은 이미 각 states에 대한 optimal values를 알고 있다는 점이다.\\[q_\\ast(s, a) \\doteq \\mathbb{E}[R_{t+1} + \\gamma v_\\ast(S_{t+1}) \\ \\vert \\ S_t = s, A_t = a]\\]위 내용을 바탕으로 $v_\\ast$와 $q_\\ast$에 대한 Bellman optimality equation을 다시 정의할 수 있다. 아래는 $v_\\ast$에 대한 Bellman optimality equation을 현재 state와 후속 state 사이의 재귀적 관계로 표현한 수식이다.\\[\\begin{aligned} v_\\ast(s) &amp;= \\max_a \\mathbb{E}[R_{t+1} + \\gamma v_\\ast(S_{t+1}) \\ \\vert \\ S_t = s, A_t = a] \\\\ &amp;= \\max_a \\sum_{s', r}p(s', r \\vert s, a)\\Big[r + \\gamma v_\\ast(s') \\Big]\\end{aligned}\\]마찬가지로 $q_\\ast$에 대한 Bellman optimality equation 역시 현재 state-action pair와 후속 state-action pair 사이의 재귀적 관계로 표현할 수 있다.\\[\\begin{aligned} q_\\ast(s, a) &amp;= \\mathbb{E} \\Big[R_{t+1} + \\gamma \\max_{a'} q_\\ast(S_{t+1}, a') \\ \\Big\\vert \\ S_t = s, A_t = a \\Big] \\\\ &amp;= \\sum_{s', r}p(s', r \\vert s, a) \\Big[r + \\gamma \\max_{a'}q_\\ast(s', a') \\Big]\\end{aligned}\\]Bellman optimality equation을 풀면 RL의 목적인 optimal policy를 찾을 수 있다. 그러나 이 방법은 실제로 유용하지 않다. Bellman optimality equation을 푸는 행위는 exhaustive search와 유사한 행위이다. RL에서는 environment의 가능한 states가 계산이 불가능한 영역 수준으로 많다. 가장 대표적인 예시가 그 유명한 AlphaGo의 바둑이다. 바둑의 경우의 수는 계산 불가능의 영역이다. 그럼에도 AlphaGo가 성공했던 이유는 RL의 기반인 Bellman optimality equation을 근사적으로 잘 풀어냈기 때문이다.References[1] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction; 2nd Edition. 2018.[2] Towards Data Science. blackburn. Reinforcement Learning: Bellman Equation and Optimality (Part 2).[3] Wikipedia. Markov decision process." }, { "title": "게임 물리 요약 (Game Physics Summary)", "url": "/game-development/game-physics-summary/", "categories": "Game Development", "tags": "", "date": "2022-05-22 00:00:00 +0900", "snippet": "이 포스트에서는 게임에 적용되는 기본적인 물리 법칙에 대해 소개한다.등가속도 운동속도는 시간에 따른 위치의 변화율, 가속도는 시간에 따른 속도의 변화율이다. 시간 $t$에 대한 위치를 $\\textbf{x}(t)$, 속도를 $\\textbf{v}(t)$, 가속도를 $\\textbf{a}(t)$라고 할 때 아래와 같은 관계가 성립한다.\\[\\textbf{a}(t) = \\dot{\\textbf{v}}(t) = \\ddot{\\textbf{x}}(t)\\]$\\dot{f}$는 함수 $f$에 대한 미분이다. 등가속도 운동의 속도는 아래와 같이 정의된다.\\[\\textbf{v}(t) = \\textbf{v}_0 + \\int_0^t \\textbf{a}(t)dt = \\textbf{v}_0 + \\textbf{a}_0t\\]$\\textbf{v}_0$는 $t=0$에서의 속도이다. 위 속도를 바탕으로 위치를 결정할 수 있다.\\[\\textbf{x}(t) = \\textbf{x}_0 + \\int_0^t \\textbf{v}(t)dt = \\textbf{x}_0 + \\int_0^t(\\textbf{v}_0 + \\textbf{a}_0t) = \\textbf{v}_0t + \\dfrac{1}{2}\\textbf{a}_0t^2\\]$\\textbf{x}_0$는 $t=0$에서의 위치이다.포물선 운동포물선 운동은 중력에 의해 영향을 받는 물체의 운동으로 3차원 공간에서 중력이 작용하는 axis를 $z$라고 가정할 때 중력 가속도 $\\textbf{g} = [0,0,-g]$로 정의된다. 여기서 $g$는 중력 상수로 지구의 지표면을 기준으로 약 $9.8 \\ m/s^2$이며 아래쪽 방향으로 작용한다. 포물선 운동에서 위치 $\\textbf{x}(t)$는 아래와 같이 정의된다.\\[\\textbf{x}(t) = \\textbf{x}_0 + \\textbf{v}_0t + \\dfrac{1}{2}\\textbf{g}t^2\\]위치 $\\textbf{x} = [x, y, z]$라고 할 때 위 수식의 각 성분은 아래와 같이 구성된다.\\[\\textbf{x}(t) =\\begin{bmatrix} x(t) \\\\ y(t) \\\\ z(t)\\end{bmatrix} =\\begin{bmatrix} x_0 + v_xt \\\\ y_0 + v_yt \\\\ z_0 + v_zt - \\dfrac{1}{2}gt^2\\end{bmatrix}\\]중력에 의한 운동에서 발사체가 최대 높이에 도달할 때의 순간 속도 $v = 0$이다. 최대 높이에 도달하는 시간을 $t$라고 할 때 아래와 같은 방식으로 구할 수 있다.\\[\\dfrac{d}{dt}z(t) = v_z - gt = 0 \\quad \\therefore t = \\dfrac{v_z}{g}\\]위에서 구한 $t$값을 바탕으로 최대 높이 $h$를 구할 수 있다.\\[h = z_0 + v_z\\dfrac{v_z}{g} - \\dfrac{1}{2}g\\Big(\\dfrac{v_z}{g}\\Big)^2 = z_0 + \\dfrac{v_z^2}{2g}\\]발사체가 발사된 이후 원래 높이 $z_0$로 내려왔을 때의 시간 $t \\neq 0$는 아래와 같이 구할 수 있다.\\[z(t) = z_0 + v_zt - \\dfrac{1}{2}gt^2 = z_0 \\quad \\therefore t = \\dfrac{2v_z}{g}\\]이를 바탕으로 발사체가 원래 높이 $z_0$로 내려올 때까지의 이동한 수평 거리 $r$은 아래와 같이 구할 수 있다.\\[r_x = v_xt = \\dfrac{2v_xv_z}{g} \\\\r_y = v_yt = \\dfrac{2v_yv_z}{g}\\]초기 속력 $s$, 최대 높이 $h$가 주어졌을 때 발사체의 발사각 $\\theta$는 아래와 같다.\\[h = z_9 + \\dfrac{v_z^2}{2g} = z_0 + \\dfrac{(s \\sin \\theta)^2}{2g} \\quad \\therefore \\theta = \\sin^{-1} \\Big(\\dfrac{1}{s}\\sqrt{2g(h - z_0)} \\Big)\\]초기 속력 $s$, 속도 $\\textbf{v}$의 $xy$ 평면에 정사영된 벡터의 크기를 $v_{xy}$, 원래 높이로 내려올 때까지의 $xy$평면에서의 수평 이동 거리 $r_{xy} = \\lVert r_x + r_y \\rVert_2$이 주어져 있을 때 발사체의 발사각 $\\theta$는 아래와 같다.\\[r_{xy} = \\dfrac{2v_{xy}v_z}{g} = \\dfrac{2(s \\cos \\theta)(s \\sin \\theta)}{g} = \\dfrac{s^2 \\sin 2\\theta}{g} \\quad \\therefore \\theta = \\dfrac{1}{2}\\sin^{-1}\\dfrac{r_{xy}g}{s^2}\\]Force물체에 힘이 가해지면 운동량과 가속도가 변한다. 여러 힘이 가해질 경우 하나의 전체 힘으로 합할 수 있다.\\[F_{\\text{total}} = \\sum F_i\\]뉴턴 법칙은 아래와 같다. 제 1법칙: 관성의 법칙 제 2법칙: 가속도 법칙 - $F = ma$ 제 3법칙: 반작용 법칙 - $F_{ij} = -F_{ji}$중력중력은 지구 표면에서 작용하는 힘이다. 이때 중력가속도 $g = -9.8 \\ m/s^2$이다.\\[F = mg\\]만유인력은 두 물체 사이에 작용하는 힘으로 두 물체 사이의 거리를 $r$, 중력상수 $G = 6.673 \\times 10^{-11}$, 두 물체의 질량을 각각 $m$, $M$이라고 할 때 그 수식은 아래와 같다.\\[F = G\\dfrac{mM}{r^2}\\]수직항력수직항력은 표면이 물체에 가하는 반작용 힘이다. 항상 표면에 수직인 방향으로 작용한다. 중력 가속도가 $g$, 경사각이 $\\theta$일 때 수직항력 $F_n$은 일반적으로 아래와 같다.\\[F_n = -mg\\cos\\theta\\]Fig 1. Normal force(Image source: Wikipedia Normal force)마찰력마찰력은 표면 상의 물체의 운동을 방해하는 힘으로 물체에 가해지는 접선 방향 힘의 반대 방향으로 작용한다. 마찰력의 크기는 표면과 특성에 의존한다. 물체의 질량에 비례하나 접촉 면적과는 무관하다. 마찰력 $F_f$는 수직항력 $F_n$으로부터 계산 가능하다.\\[F_f = \\mu F_n\\]$\\mu$는 마찰계수로 표면의 특성에 따라서 정해진다. 마찰력은 운동마찰려과 정지마찰력이 있는데 둘 중 하나만 적용된다. 운동마찰은 서로 상대적으로 움직이는 두 표면 사이에서 발생하는 힘으로 각자의 운동에 대한 저항으로 작용한다. 운동마찰력을 $F_k$, 운동마찰계수를 $\\mu_k$라고 할 때 수식은 아래와 같다.\\[F_k = \\mu_k F_n\\]정지마찰은 표면이 정지된 물체를 움직이지 못하도록 붙들고 있는 힘이다. 정지마찰력을 $F_s$, 정지마찰계수를 $\\mu_s$라고 할 때 수식은 아래와 같다.\\[F_s = \\mu_s F_n\\]정지마찰력 $F_s$일 때 물체에 가해지는 힘 $F$의 크기가 $F_s$보다 큰 순간 물체가 움직이기 시작한다. 그때부터 정지마찰력 $F_s$는 사라지고 운동마찰력 $F_k$가 작용한다. 대체로 $F_k &lt; F_s$인데 정지된 물체를 움직이게 하는 것이 이미 움직이는 물체를 계속 움직이게 하는 것보다 대체로 더 힘들다.스프링 운동스프링 운동은 비강체 물체를 모델링하기 위해 주로 쓰인다. rigid body의 움직임을 제약하는 joint를 생성해 처리한다. 스프링 힘(Hooke’s law) $F_s$은 아래와 같다.\\[F_s = -k_sx\\]$k_s$는 강성계수이며 두 점의 위치를 각각 $p_0$, $p_1$이라고 할 때 두 물체의 변위 $x = p_1 - p_0$이다. 두 물체의 원래 거리를 $d$라고 할 때 스프링 힘은 아래와 같다.\\[F_s = -k_s(\\lVert x \\rVert_2 - d)\\dfrac{x}{\\lVert x \\rVert_2}\\]두 지점의 거리 $\\lVert x \\rVert_2$가 $d$보다 가까우면 밀어내고, $d$보다 멀면 당기도록 작용한다.감쇠력은 에너지 손실을 도입하여 스프링이 무한히 oscillate하지 않도록 하게 한다. 감쇠력 $F_d$는 아래와 같다.\\[F_d = -k_dv\\]$k_d$는 점성 감쇠계수이다. 각 위치에서의 속도 벡터를 $v_0$, $v_1$이라고 할 때 속도 변화 $v = v_1 - v_0$이다. 최종적으로 감쇠된 스프링 시스템은 아래와 같이 표현할 수 있다.\\[F = F_s + F_d = -k_sx -k_dv\\]운동량운동량 $p$는 질량과 속도를 곱한 벡터이다.\\[p = mv\\]두 물체가 충돌 시 운동량은 보존 되며 이를 운동량 보존 법칙이라고 한다.\\[m_1v_1 + m_2v_2 = m_1v_1' + m_2v_2'\\]$v_1$, $v_2$는 충돌 전 각 물체의 속도이며 $v_1’$, $v_2’$은 충돌 후 각 물체의 속도이다.충돌충돌을 검사하는 일반적인 방법은 다음과 같이 크게 3가지가 있다. Particle vs Plane Plane vs Plane Edge vs Plane이 중 Particle vs Plane 충돌이 구현하기 가장 쉬운편이다. 따라서 이 포스트에서는 Particle vs Plane 충돌에 대해 알아본다.Particle vs Plane Collision Detection3차원 공간에서 다음과 같은 요소들이 존재한다. $\\text{x}$ - particle의 위치 $\\text{p}$ - plane 위의 임의의 점 $\\text{n}$ - 평면의 정방향 normal vector이때 $(\\text{x} - \\text{p}) \\cdot \\text{n}$가 0보다 크면 particle은 plane과 $\\text{x}$는 충돌 전 상태, 0이면 plane과 접촉, 0보다 작으면 평면을 통과한 상태이다. 이는 내적 자체가 각도 정보를 포함하고 있기 때문이다.Particle vs Plane Collision Response실제 collision detection 시 오차로 인해 내적값이 0이 되는 경우를 찾기 어렵다. 따라서 내적 값이 양수 -&gt; 음수로 변할 떄 충돌했다고 가정한다. 즉, 평면을 관통했을 때 충돌했다고 판정한다. 그 후 관통한 particle을 평면 위로 이동시킨 후 충돌에 대한 후처리로 particle이 튕겨져나가는 물리적 처리를 한다.$\\text{v}$ - current velocity of the particle$\\text{n}$ - normal vector on the illegal side of the plane (direction vector whose magnitude is 1)이때 속도 $\\text{v}$의 normal 성분인 $\\text{v}_n$은 아래와 같다.\\[\\text{v}_n = (\\text{v} \\cdot \\text{n})\\text{n}\\]속도 $\\text{v}$의 tangential 성분도 유도 가능하다.\\[\\text{v}_t = \\text{v} - \\text{v}_n\\]위 두 벡터를 바탕으로 particle이 튕겨져나가는 bounced response $\\text{v}_b$를 구할 수 있다.\\[\\begin{aligned} \\text{v}_b &amp;= \\text{v}_t - \\text{v}_n \\\\ &amp;= (1 - k_f)\\text{v}_t - k_r\\text{v}_n\\end{aligned}\\]$k_f$는 마찰계수, $k_r$은 복원계수이다.물리상태 계산물체의 이전 물리 상태(위치, 속도 등)와 가해진 힘을 알면 시간에 대한 적분을 통해 물체의 이후 상태를 결정할 수 있다. 아래는 움직임에 대한 수식이다.\\[F = ma \\rightarrow a = \\dfrac{F}{m} \\\\\\dfrac{dv}{dt} = a \\rightarrow dv = a \\ dt \\\\\\dfrac{dx}{dt} = v \\rightarrow dx = v \\ dt\\]Runge-Kutta 적분법4차 Runge-Kutta 적분법 (RK4)을 활용하면 상당히 정확한 수치해석적 적분을 구현할 수 있다. 아래와 같은 function $f$와 초기값 $t_0$, $y_0$가 있다고 하자.\\[\\dfrac{dy}{dt} = f(t, y), \\quad y(t_0) = y_0\\]$y$는 time $t$에 대한 알려지지 않은 함수로 근사화할 대상이다. step-size $h &gt; 0$일 떄 아래와 같은 방식으로 근사화할 수 있다.\\[\\begin{aligned} y_{n+1} &amp;= y_n + \\dfrac{1}{6}h(k_1 + 2k_2 + 2k_3 + k_4), \\\\ t_{n+1} &amp;= t_n + h\\end{aligned}\\]$y_{n+1}$은 $y(t_{n+1})$에 대한 RK4 근사값으로 next value $y_{n+1}$은 current value $y_n$과 4개의 기울기 요소의 weighted average와의 합으로 구성된다. 기울기 요소는 아래와 같다. $k_1 = f(t_n, y_n)$ $k_2 = f \\Big(t_n + \\dfrac{h}{2}, y_n + h\\dfrac{k_1}{2} \\Big)$ $k_3 = f \\Big(t_n + \\dfrac{h}{2}, y_n + h\\dfrac{k_2}{2} \\Big)$ $k_4 = f(t_n + h, y_n + hk_3)$아래는 RK4 적분법에서의 기울기 요소를 나타낸다.Fig 2. Runge-Kutta slopes(Image source: Wikipedia Runge-Kutta methods)References[1] Incheon National University - Game Programming Lecture[2] Wikipedia Normal force[3] Wikipedia Runge-Kutta methods" }, { "title": "Multi-armed Bandits", "url": "/reinforcement-learning/rl-fundamental/multi-armed-bandits/", "categories": "Reinforcement Learning, RL Fundamental", "tags": "RL, AI", "date": "2022-05-21 00:00:00 +0900", "snippet": "이 포스트에서는 Reinforcement learning (RL)의 기본 내용인 Multi-armed Bandits 환경과 기본적인 아이디어들에 대해 알아본다.Reinforcement learning vs othersReinforcement learning (RL)과 다른 learning의 가장 큰 구별점은 사용하는 정보의 차이에 있다. 다른 learning은 주로 올바른 action을 나타내는 일종의 정답 label이 존재하는 instructive feedback을 사용하며, 이러한 feedback을 사용하는 learning을 supervised learning이라고 한다. 그러나 RL에서는 evaluative feedback을 사용한다. evaluative feedback은 이것이 얼마나 좋은 action인지를 나타내지만 best action인지 아닌지를 나타내지는 않는다. 이를 unsupervised learning이라고 한다.What is Multi-armed BanditsMulti-armed Bandits 환경은 슬롯 머신에서 여러 개의 레버를 당겨 보상을 획득하는 환경이다. 이 때 레버의 개수를 $k$개라고 할 때 $k$-armed bandit problem이라고 하며 아래와 같은 환경으로 정의된다. $k$개의 다른 action들을 반복적으로 선택함. 각 선택에 대해 stationary probability distribution을 따르는 수치적인 reward를 획득함. 일정 기간(time steps) 동안의 expected total reward를 maximized하는게 목적임.stationary probability distribution은 시간이 흐름에도 변하지 않는 정적인 확률 분포를 의미한다.Fig 1. Multi-armed bandits.(Image source: Lil’Log.)$k$-armed bandit problem과 일반적인 reinforcement learning problem의 가장 큰 차이점은 $k$-armed bandit problem은 어떤 상태에서 선택한 행동으로 즉각적인 보상만 획득할 뿐, 레버를 당기는 action들이 environment의 states와 future rewards를 변경시키지 않는다. 즉, actions와 states가 연관성이 없으며 이를 nonassociative setting이라고 한다. 반대로 associative setting에서는 선택한 action들이 states를 변경시켜 future rewards에 영향을 미치는 파급효과를 가진다.$k$-armed bandit problem에서 각 time step $t$에서 선택한 action을 $A_t$, 획득한 reward을 $R_t$라고 할 때 임의의 action $a$에 대한 value $q_\\ast(a)$는 $a$에 대한 expected reward이다.\\[q_\\ast(a) \\doteq \\mathbb{E}[R_t \\vert A_t = a]\\]그러나 실제 $q_\\ast(a)$를 모르기 때문에 우리는 이 값을 추정해야한다. time step $t$에서 추정된 action $a$의 value를 $Q_t(a)$라고 할 때 우리의 목적은 이 값을 $q_\\ast(a)$에 근접시키는 것이다.Exploitation vs Explorationaction value를 추정하는 과정에서 가장 value가 높은 action을 greedy action이라고 하며 이들에 대한 선택을 exploiting이라고 한다. 그 외의 action을 선택할 때는 exploration이라고 부른다. exploitation은 현재 가진 정보를 기준으로 즉각적인 최고의 보상을 획득할 수 있는 수단이다. 그러나 exploration은 단기간 적은 보상을 획득하지만 내가 모르는 정보를 탐색해 현재 greedy action보다 더 나은 action을 발견하여 더 높은 total reward를 획득할 수 있는 수단이다. 결국 exploitation과 exploration 사이에 적절한 선택이 필요하며 이는 그 유명한 exploitation vs exploration dilemma로 강화학습의 숙명과도 같은 문제이다.Action-value Methodsaction value를 추정하는 가장 간단한 방법은 지금까지 획득한 reward의 평균을 구하는 것이다.\\[Q_t(a) \\doteq \\dfrac{\\sum_{i=1}^{t-1}R_i \\cdot 𝟙_{A_i=a}}{\\sum_{i=1}^{t-1}𝟙_{A_i=a}}\\]$𝟙_{predicate}$은 $predicate$이 true이면 1, false이면 0을 반환하는 함수이다.action value에 따라 action을 선택하는 가장 간단한 방법은 가장 높게 추정된 action value를 가진 action을 선택하는 것이다. 즉, greedy action을 선택한다.\\[A_t \\doteq \\underset{a}{\\arg\\max}\\ Q_t(a)\\]위 방법은 항상 exploitation을 수행하기 때문에 지금보다 더 나은 행동을 발견할 수 없다. 이에 대안 대안으로 대부분은 exploitation을 수행하되 $\\epsilon$의 확률로 랜덤하게 action을 선택한다. 이를 $\\epsilon$-greedy 방법이라 한다.Incremental Implementation어떤 단일 action의 $i$번째 선택 시 획득 한 reward를 $R_i$라고 할 때, 이 action을 $n - 1$번 선택했을 때의 action value의 추정치 $Q_n$을 $n - 1$번 획득한 reward들의 평균으로 추정한다면 아래와 같은 수식으로 표현할 수 있다.\\[Q_n \\doteq \\dfrac{R_1 + R_2 + \\cdots + R_{n-1}}{n - 1}\\]그러나 위와 같은 수식에서는 그동안 획득한 모든 reward들을 모두 기록해야하며, 새로운 reward를 획득할 때 마다 처음부터 다시 reward들을 모두 더하는 계산을 해야한다는 문제가 있다. 이에 대한 대안으로 평균을 구하는 수식을 incremental한 형태로 변경할 수 있는데 이 경우 위 수식처럼 reward들을 기록할 필요가 없으며 계산 모든 reward들을 합할 필요가 없어진다. 기존 평균 값에 새롭게 획득한 reward의 일정 비중만을 누적하면 되는 원리이다. 기존 action value $Q_n$과 새롭게 획득한 $n$번째 reward $R_n$이 있을 때 action value에 대한 incremental formula는 아래와 같다.\\[Q_{n+1} = Q_n + \\dfrac{1}{n}[R_n - Q_n]\\]위 수식에 대한 일반적인 형태는 아래와 같다.\\[\\textit{NewEstimate} \\leftarrow \\textit{OldEstimate} + \\textit{StepSize} \\Big[\\textit{Target} - \\textit{OldEstimate} \\Big]\\]위 수식에서 $\\Big[\\textit{Target} - \\textit{OldEstimate} \\Big]$는 추정치에 대한 error이며 이를 바탕으로 점점 Target에 다가간다.Nonstationary Problemreward에 대한 확률들이 시간이 지나도 변하지 않는 stationary problem에서는 평균을 구하는 위 방법이 유용할 지 모르지만 nonstationary 환경에서는 그렇지 않다. 이 경우엔 지난 과거의 보상보다 최근 보상에 더 큰 비중을 주는게 합당하다. 이에 대한 하나의 방법으로 step-size를 상수로 사용한다. 아래는 이에 대한 incremental update 수식이다.\\[Q_{n+1} \\doteq Q_n + \\alpha[R_n - Q_n]\\]step-size parameter인 $\\alpha \\in (0,1]$는 상수이다. 다만 $\\alpha$ 값을 step에 따라 변경하는게 더 효과적일 때도 있다.Initial Value위 수식은 past rewards와 initial estimate $Q_1$의 weighted average로 표현될 수 있다.\\[\\begin{aligned} Q_{n+1} &amp;= Q_n + \\alpha[R_n - Q_n] \\\\ &amp;= (1-\\alpha)^nQ_1 + \\sum_{i=1}^n\\alpha(1-\\alpha)^{n-i}R_i\\end{aligned}\\]위 수식을 보면 알겠지만 현재 action value는 initial value인 $Q_1(a)$에 영향을 받는다. 즉 bias가 발생하였다. 몰론 표본평균방법일 경우 모든 action들이 적어도 한번 선택된다면 이러한 bias는 사라지지만 위 수식처럼 step-size $\\alpha$가 상수일 경우 bias는 영구적이다. 그러나 $\\alpha \\in (0, 1]$이기 때문에 시간이 지날 수록 결국 이러한 bias는 작아지게 된다. 그렇기 때문에 이러한 bias는 실제로 그다지 문제가 되지 않는다.Upper-Confidence-Bound Action Selection$\\epsilon$-greedy 방법은 exploration을 무차별적으로 수행하게 만든다는 문제가 있다. action value의 추정치가 최대값에 얼마나 가까운지와 불확실성은 얼마나 되는지를 모두 고려해, 실제로 최적이 될 가능성에 따라 non-greedy action들 사이에서 선택하는 것이 조금 더 효과적일 것이다. 이에 대한 대안으로 upper confidence bound (UCB) 방법이 있으며 그 수식은 아래와 같다.\\[A_t \\doteq \\underset{a}{\\arg\\max}\\ \\Bigg[Q_t(a) + c \\sqrt{\\dfrac{\\ln t}{N_t(a)}} \\ \\Bigg]\\]$N_t(a)$는 time step $t$ 이전에 action $a$가 선택된 횟수이며, $c &gt; 0$는 exploration을 컨트롤 하는 정도로 신뢰도를 결정한다. square-root 부분은 $a$의 값에 대한 추정에서 불확실성을 나타낸다. 이를 통해 action $a$의 true value에 대한 일종의 upper bound를 설정할 수 있다. action $a$가 선택될 때에는 분자 $\\ln t$가 증가하긴 하지만 분모 $N_t(a)$가 증가하기 때문에 불확실성은 대게 감소한다. 그 이유는 분자는 log-scale이지만 분모는 linear-scale이기 때문이다. $a$ 외의 다른 action이 선택될 때는 분자 $\\ln t$는 증가하지만 분모 $N_t(a)$는 변하지 않기 때문에 불확실성은 증가한다. 위 수식에 따라 action value의 추정치 $Q_t(a)$가 너무 낮거나, action $a$가 너무 자주 선택됬을 경우 점점 선택되는 빈도가 줄게 된다. 어떤 action $a$의 action value $Q_t(a)$가 높아 이 action이 한동안 계속 자주 선택될 경우 이 action에 대한 불확실성은 줄어든다. 반대로 다른 action들은 그동안 선택되지 않았기 때문에 불확실성이 늘어나며 어느 순간 cross가 발생해 다른 action의 upper bound가 더 커져 다른 action을 수행하게 된다. 그러나 $t \\rightarrow \\infty$일 경우 분자는 log-scale이지만 분모는 linear-scale이기 때문에 결국 0으로 수렴한다. 즉, time step $t$가 작을 때는 exploration이 활발히 일어나지만 time step $t$가 증가할 수록 전체 action에 대한 불확실성은 낮아지고 결국 action value $Q_t(a)$에 대해서만 action을 선택하는 exploitation을 수행할 것이다.UCB 방법은 $k$-armed bandits에서 $\\epsilon$-greedy 보다 좋은 성능을 낸다. 그러나 좀 더 일반적인 RL setting으로 확장하는 것은 상당히 어려우며 실용적이지 못하다. UCB는 nonstationary 문제를 다루는데 어려움이 있으며 large state space에서 function approximation을 사용할 때 어려움이 있다.Gradient Bandit Algorithms각 action $a$에 대한 numerical preference를 $H_t(a)$를 학습하는 것을 고려해보자. preference가 클 수록 더 자주 action이 선택된다. 여기서 preference는 action value $Q_t(a)$와는 다르게 reward 측면에서 해석되지 않는다. 또한 action을 선택할 때 한 action의 preference와 다른 action들의 preference 사이의 상대적 비교로 결정한다. $k$개의 action이 있다고 할 때 각 action을 선택하는 확률은 soft-max distribution을 따르며 그 수식은 아래와 같다.\\[\\text{Pr}\\lbrace A_t = a \\rbrace \\doteq \\dfrac{e^{H_t(a)}}{\\sum_{b=1}^k e^{H_t(b)}} \\doteq \\pi_t(a)\\]$\\pi_t(a)$는 time step $t$에서 action을 선택하는 확률이다. preference $H_t(a)$를 학습하기 위한 방법 중 하나로 stochastic gradient ascent가 있다. action $A_t$를 선택한 뒤 reward $R_t$를 획득했을 때 action preference들은 아래와 같은 수식으로 업데이트된다.\\[H_{t+1}(a) \\doteq H_t(a) + \\alpha \\dfrac{\\partial \\mathbb{E}[R_t]}{\\partial H_t(a)}\\]위 수식은 gradient ascent에 대한 기본적인 아이디어이며 이를 바탕으로 아래와 같은 수식을 얻을 수 있다.\\[\\begin{aligned} &amp;H_{t+1}(A_t) \\doteq H_t(A_t) + \\alpha(R_t - \\bar{R_t})(1 - \\pi_t(A_t)) &amp; \\text{and} &amp; \\\\ &amp;H_{t+1}(a) \\doteq H_t(a) - \\alpha(R_t - \\bar{R_t})\\pi_t(a) &amp; \\text{for all} \\; a \\neq A_t\\end{aligned}\\]$\\bar{R_t}$는 time step $t$까지의 모든 reward의 평균이다. $\\bar{R_t}$는 reward에 대한 baseline으로 획득한 reward $R_t$가 baseline보다 클 경우 $A_t$를 미래에 수행할 확률은 증가하고, baseline보다 작을 경우엔 감소한다. 선택되지 않은 나머지 action들은 $A_t$와 반대로 업데이트된다.References[1] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction; 2nd Edition. 2018.[2] Lil’Log. The Multi-Armed Bandit Problem and Its Solutions." }, { "title": "Curiosity-driven Exploration by Self-supervised Prediction", "url": "/reinforcement-learning/exploration-methods/curiosity-driven-exploration/", "categories": "Reinforcement Learning, Exploration Methods", "tags": "RL, AI", "date": "2022-05-07 00:00:00 +0900", "snippet": "이 포스트에서는 Reinforcement Learning을 효과적으로 수행할 수 있도록 intrinsic curiosity reward를 사용하는 아이디어를 제시한 Curiosity-driven Exploration by Self-supervised Prediction에 대해 소개한다.IntroductionCuriosity-driven Exploration by Self-supervised Prediction 논문은 강화학습에서 효과적인 학습을 수행하기 위해 intrinsic reward인 curiosity 개념을 도입해 사용하는 방법을 보인다. 이 논문이 획기적인 이유는 강화학습의 영원한 숙제인 exploration vs exploitation dilemma 뿐만 아니라 비슷하지만 다른 새로운 환경에도 적용 가능하도록 하는 generalization 문제를 모두 해결했기 때문이다. 심지어 environment에 의한 reward 없이도 적절한 exploration을 유도할 수 있는 굉장히 강력한 모델이다. 이 포스트에서는 이 논문에 대해 자세히 소개하려 한다. 아래는 실제 환경에서 environment에 의한 reward 없이 exploration을 수행하는 모습이다.&nbsp;Fig 1. Discovering how to play without extrinsic reward.(Image source: Github pathak22/noreward-rl.)What is intrinsic reward강화학습 알고리즘은 total rewards(보상)를 최대화하는 policy(정책)를 학습하는 것을 목표로 한다. agent가 environment의 특정 state $s$에서 policy $\\pi$에 의해 action $a$를 수행하면 reward $r$을 획득할 수 있다. 이때 획득한 reward $r$을 바탕으로 agent의 action $a$를 결정하는 policy $\\pi$를 update할 수 있다.Fig 2. The agent–environment interaction in a Markov decision process.(Image source: Sec. 3.1 Sutton &amp; Barto (2017).)policy $\\pi$를 효과적으로 update하기 위해서는 reward가 끊임없이 주어져야한다. 주어진 데이터가 많을 수록 더 좋은 결과를 얻을 수 있기 때문이다. 그러나 실제 환경은 다르다. agent가 action을 수행했다고 해서 반드시 reward가 제공되지는 않는다. 아니, 오히려 reward가 제공되는 경우는 극히 드물다. 어떤 특정 task를 성공적으로 수행해야 reward가 제공된다고 할 때 그 task를 수행하기가 너무 어렵다면 당연히 reward를 획득하는 것도 어렵다. 즉, 그 task를 성공적으로 수행하기 전까지는 reward의 변화가 없는 셈이다. 강화학습에서는 이러한 환경을 sparse environment라고 부른다.Intrinsic Motivation - Curiosity원래 강화학습에서는 policy $\\pi$가 제대로 학습되지 않았을 때 environment의 특정 goal state에 도달하기 위해서는 랜덤성에 의존할 수 밖에 없었다. 이러한 방식은 simple environment를 제외하고는 효과적이지 못하다. agent의 exploration을 랜덤성에만 의존하지 않고 보다 효과적으로 이끌기 위해 제안된 아이디어 중 하나가 바로 intrinsic reward의 활용이다. 아래는 reward에 대한 2가지 구분이다. extrinsic reward - environment에 의해 직접 획득한 reward intrinsic reward - motivation과 같이 agent에게 내제된 reward로 environment로부터 직접적으로 얻지 않음실제 인간을 생각해보자. 인간은 매우 sparse한 environment에서 살고 있다. 강화학습처럼 인간은 대부분 수천, 수만번 동일한 행동을 수행해 그에 해당하는 보상을 획득하지 못한다. 그런 상황 속에서도 인간은 최적의 reward를 획득하기 위해 시도한다. 이것이 가능한 근본적 이유는 무엇일까? 3살짜리 어린아이를 생각해보자. 3살짜리 어린아이를 혼자 두었다고 할 때 따로 학습을 시키지 않았음에도 스스로 잘 놀며 이로 인해 행복이나 즐거움 같은 보상을 획득한다. 이것이 가능한 이유는 3살짜리 아이가 내재되어있는 동기부여인 호기심(curiosity)을 이용하기 때문이다. 인간의 근본적인 감정인 curiosity를 이용해 environment를 탐구하고 새로운 state를 발견한다.위 아이디어를 바탕으로 강화학습에서도 비슷하게 intrinsic motivation인 curiosity를 이용하였다. 이 논문에서 curiosity는 2가지 용도로 사용된다. agent가 새로운 지식을 추구하기 위해 environment를 탐색하도록 돕는다. 미래의 scenario에 도움이 될 수 있는 스킬을 학습하게 한다.intrinsic reward를 활용할 시 sparse environment에서 굉장히 강력한 퍼포먼스를 보인다.Problemintrinsic reward를 활용하는 방법은 크게 아래와 같이 구분된다. agent가 새로운 state를 explore 하도록 한다. 행동 결과를 예측하는 능력을 바탕으로 행동을 수행하여 error와 불확실성을 줄인다.위 방법을 적용할 때 아래와 같은 문제들이 발생한다. 이미지와 같은 high-dimensional continuous state space에서 “novelty”나 prediction error/uncertainty를 측정하는 모델을 구축하는 것은 어렵다. environment에 noise가 작용할 경우 agent-environment system의 확률을 다루는 것은 어렵다. noise가 environment에 별다른 영향을 미치지 않지만 agent는 완전히 새로운 state로 인식할 수 있다. 물리적, 시각적으로는 다르지만 기능적으로 비슷한 환경에 generalization를 적용하는 것이 어렵다. 이는 특히 매우 큰 환경에서는 굉장히 중요한 문제이다.위 문제들에 대한 해결책으로 agent가 특정 state에서의 행동 결과를 예측하긴 어렵지만 배울만한 가치가 있을 때만 reward를 제공하는 것이다. 하지만 이러한 learnability를 추정하는 것 역시 상당히 어려운 문제이다.Key insightlearnability를 추정하기 위한 environment 변화에 대한 예측 시, 위에서 제시한 문제들을 피하기 위해 agent의 action 때문에 environment가 변할 가능성이 있거나 agent에게 영향을 미칠 수 있는 environment의 변화만 예측한다. 그 외 나머지 요소는 모두 버린다. 이를 위해 굉장히 민감한 raw space(e.g. pixels)가 아닌 agent에 의해 수행된 action과 관련된 정보만을 표현하는 feature space를 학습한다.Curiosity-Driven Explorationagent는 curiosity-driven intrinsic reward signal을 생성하는 reward generator와 intrinsic reward를 최대화하는 action들의 sequence를 구하는 policy로 구성된다. 또한 아주 가끔씩(sparse environment이기 때문에) extrinsic reward를 획득한다. time step $t$에서의 intrinsic curiosity reward를 $r_t^i$, extrinsic reward를 $r_t^e$라고 할 때 policy는 $r_t = r_t^i + r_t^e$를 최대화하는 방향으로 학습된다.신경망 매개변수가 $\\theta_P$일 때 policy $\\pi(s_t; \\theta_P)$에 대해 $\\theta_P$는 아래와 같이 reward의 expected sum을 최대화하는 방향으로 학습된다.\\[\\max_{\\theta_P}\\mathbb{E}_{\\pi(s_t; \\theta_P)}[\\textstyle\\sum_tr_t]\\]Prediction error as curiosity rewardcuriosity reward $r^i$는 agent의 environment에 대한 지식의 prediction error를 기반으로 디자인된다. 문제는 환경을 어떻게 예측할 것인가이다. Problem에서 언급했듯이 이미지와 같은 raw pixel space를 예측하는 것은 굉장히 부적절하다. 이는 굉장히 민감한 환경이기 때문이다. agent에게 아무런 영향도 미치지 못하지만 pixel이 조금만 변경되더라도 prediction error는 여전히 큰 상태일거고 agent는 이로인해 쓸모없는 target에 계속 curiosity를 가지는 함정에 빠지게 될 것이다. 따라서 Key insight에서 언급했듯이 agent에게 유의미한 정보만을 표현하는 feature space를 학습해야한다. 이에 따라 먼저 agent의 관찰을 다음과 같은 3가지 케이스로 나눈다. agent에 의해 컨트롤 될 수 있는 것들 agent가 컨트롤 할 수는 없지만 agent에게 영향을 미칠 수 있는 것들 agent에 의해 컨트롤 되지 않고 영향도 못미치는 것들curiosity에 대해 좋은 feature space는 1과 2를 모델링해야하며 3에 의해 영향을 받지 않아야 한다.Intrinsic Curiosity Module이 논문에서 feature space로 인코딩하고 intrinsic curiosity reward를 제공하기 위한 메커니즘인 Intrinsic Curiosity Module(ICM)을 소개한다. ICM은 크게 inverse dynamics model과 forward dynamics model로 구분되며 각각은 서로 다른 신경망이다.Fig 3. Intrinsic Curiosity Module (ICM).(Image source: Curiosity-driven Exploration by Self-supervised Prediction.)inverse dynamics model은 2개의 서브 모듈로 구성된다. 첫번째 서브 모듈은 raw state $s_t$와 $s_{t+1}$를 feature vector $\\phi(s_t)$와 $\\phi(s_{t+1})$로 인코딩한다. 이 논문에서는 raw pixel을 변환하기 위해 CNN을 사용하였다. 두번째 서브 모듈은 $\\phi(s_t)$와 $\\phi(s_{t+1})$을 입력 받아 action $a_t$의 추정치인 $\\hat{a}_t$를 예측한다. 이 논문에서는 fully connected layer에 위 feature vector가 concatenated된 single feature vector를 입력으로 사용하였다. Markov Decision Process(MDP)에 의하면 $s$에서 $a$에 의해 $s’$로 전이된다. 그런데 이 model은 현재 상태 $s$와 그 결과인 $s’$이 주어져 있을 때 역으로 원인인 $a$를 추정하기 때문에 inverse model이다. 두 서브 모듈에 대한 신경망의 learning function $g$는 아래와 같다.\\[\\hat{a}_t = g\\Big(s_t, s_{t+1}; \\theta_I\\Big)\\]신경망 매개변수 $\\theta_I$는 $\\hat{a}_t$와 $a_t$에 대한 손실 함수 $L_I$를 최소화하는 방향으로 최적화된다. 만약 $a_t$가 discrete하다면 $g$의 output은 soft-max distribution이다.\\[\\min_{\\theta_I}L_I(\\hat{a}_t, a_t)\\]이를 통해 inverse model은 state에서 agent의 action과 관련있는 feature를 추출할 수 있다.다음은 forward dynamics model이다. 이 model에서는 $a_t$와 $\\phi(s_t)$를 입력으로 받아 $\\phi(s_{t+1})$의 추정치인 $\\hat{\\phi}(s_{t+1})$을 예측한다. 이 논문에서는 2개의 fully connected layer로 구성된 신경망에 $\\phi(s_t)$와 $a_t$가 concatenated된 vector를 입력한다. 이 model은 현재 상태 $s$와 원인인 $a$가 주어져있고 그 결과인 $s’$을 예측하고 있기 때문에 forward model이다.\\[\\hat{\\phi}(s_{t+1}) = f\\Big(\\phi(s_t), a_t; \\theta_F \\Big)\\]신경망 매개변수 $\\theta_F$는 손실 함수 $L_F$를 최소화하는 방향으로 최적화된다.\\[L_F\\Big(\\phi(s_t), \\hat{\\phi}(s_{t+1})\\Big) = \\dfrac{1}{2}\\lVert \\hat{\\phi}(s_{t+1}) - \\phi(s_{t+1}) \\rVert_2^2\\] 손실함수 $L_F$의 입력이 왜 $\\phi(s_{t+1})$이 아닌 $\\phi(s_t)$인지는 잘 모르겠음. 이유 알려주시면 ㄳ.Prediction error as curiosity reward에서 언급했듯이 intrinsic curiosity reward $r_t^i$는 agent의 environment에 대한 지식의 prediction error이다. environment에 대한 지식은 raw space $s$가 아닌 agent에게 유의미한 정보만을 표현하는 인코딩된 feature vector $\\phi(s)$를 사용한다. 그 수식은 아래와 같다.\\[r_t^i = \\dfrac{\\eta}{2}\\lVert \\hat{\\phi}(s_{t+1}) - \\phi(s_{t+1}) \\rVert_2^2\\]이때 $\\eta &gt; 0$는 scaling factor이다. 위 수식을 보면 forward model이 예측한 feature $\\hat{\\phi}(s_{t+1})$와 실제 feature $\\phi(s_{t+1})$의 prediction error가 curiosity로 사용된다. 호기심이라는것 자체가 내가 예상한 것과 전혀 다른 결과가 나올 때 보통 강하게 느껴지기 때문에 이러한 수식이 적용된 것이라고 판단된다. 또한 forward model의 손실 함수 역시 prediction error인데 이 이유는 그 상태에 대해 어느정도 알게 되었기 때문에 그 상태에 대한 관심이 떨어져서이다. 관심이 떨어지면 호기심 역시 줄어든다. 어쩌면 인간과 굉장히 유사한 메커니즘으로 볼 수 있다.inverse model은 environment의 feature를 인식하고 추출하며 forward model은 feature에 대한 prediction을 진행한다. 이 prediction error로 curiosity intrinsic reward가 생성된다. 이때 각 model의 손실함수는 함께 최적화된다.위에서 본 내용을 전체적으로 종합하면 최적화 문제는 아래와 같다.\\[\\min_{\\theta_P, \\theta_I, \\theta_F} \\bigg[ -\\lambda \\mathbb{E}_{\\pi(s_t; \\theta_P)}[\\textstyle\\sum_tr_t] + (1-\\beta)L_I + \\beta L_F \\bigg]\\]$\\lambda &gt; 0$인 scalar로 intrinsic reward signal 학습의 중요도 대비 policy gradient loss의 중요도를 의미한다. policy gradient는 원래 reward를 maximize하는 방향으로 policy $\\pi$가 업데이트 되지만 $-$ 부호를 붙여 minimize 시키게 만들었다. $0 \\leq \\beta \\leq 1$는 inverse model loss와 forward model loss 사이의 비중이다. 이 논문에서는 $\\beta = 0.2$, $\\lambda = 0.1$의 값을 사용하였다.Summary지금까지 ICM에 대해 알아보았다. ICM은 intrinsic curiosity reward를 통해 agent가 유의미한 exploration을 할 수 있도록 함으로써 exploration vs exploitation dilemma를 해결하였다. 또한 sensory한 high-dimensional observation space를 agent에게 유의미한 feature space로 인코딩함으로써 generalization 문제를 해결하였다.이 논문에서는 VizDoom과 Super mario Bros 게임에서 각각 실험을 진행하였다. 실험에서는 A3C + ICM과 vanilla A3C를 비교하였는데 상당히 유의미한 성능 차이를 보였다. 또한 A3C + ICM과 A3C + ICM(pixels - no feature encoding)의 비교 결과 역시 유의미만 차이를 보였다. 이는 특히 generalization 부분에서 상당한 차이를 발생시켰다. 이 논문이 더 대단한 점은 extrinsic reward 없이 intrinsic reward 만으로도 agent가 environment를 잘 탐색하도록 만들었다는 점이다. 자세한 실험 결과는 논문을 직접 참조하길 바란다.References[1] Pathak et al. Curiosity-driven Exploration by Self-supervised Prediction.[2] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction; 2nd Edition. 2017.[3] Github. pathak22/noreward-rl." }, { "title": "[Unity] Save System Pattern", "url": "/unity/save-system-pattern/", "categories": "Unity", "tags": "Unity, Save System, Design Pattern", "date": "2022-04-06 00:00:00 +0900", "snippet": "IntroductionUnity에서 Save System을 효과적으로 구축하기 위한 패턴을 직접 생각하고 구현해보았다. 팀원들과 밑바닥부터 스스로 생각하고 논의해서 얻어낸 결과이기 때문에 지금 소개할 패턴은 오히려 비효율적일 수도 있다. 다만 우리의 목적에는 충분히 들어맞고, 직접 생각하고 구현해보았다는 의의가 있다고 생각한다.Goal우리의 목적은 다음과 같다. 여러 개의 게임 데이터를 하나의 파일로 저장한다. 예를 들면 플레이어 데이터, 상점 데이터, 게임 진행 현황 등 분할된 게임 데이터들을 하나의 파일에 저장한다. 저장해야할 데이터는 각각의 인스턴스에서 관리한다. 저장해야할 데이터가 늘어나거나 줄어도 메인 시스템은 변경되지 않는다. Save나 Load를 요청하는 컨트롤러는 실제 어떤 데이터가 저장되고 Load되는지 모른다. 오직, Save, Load에 대한 시점만 결정한다. Load 명령이 요청되면 각각의 인스턴스는 자기가 저장했던 데이터를 획득해 동기화한다.위 목적을 달성하기 위해 우리는 크게 역할을 3가지로 구분했다. Save 매니저(파일 입출력을 담당) Save 컨트롤러(Save, Load 시점을 결정함) Save 리스너(Save, Load가 발동 될 시 각자 원하는 데이터를 Save 및 Load함)Save 매니저Save 매니저는 파일 입출력을 담당하고 Save, Load에 대한 명령을 받으면 리스너에게 알린다.Save 컨트롤러Save 컨트롤러는 Save 매니저의 Save, Load 메서드를 호출함으로써 Save 매니저에게 명령한다. Save 시점은 수동일 수도 있고 자동일 수도 있으며 이는 각 컨트롤러 오브젝트에 따라 다르다.Save 리스너실제 저장되는 데이터를 관리한다. Save 매니저를 구독하며 Save 알림을 받으면 저장할 데이터를 반환한다. Load 알림을 받으면 저장했던 데이터를 입력 받아 동기화 작업을 처리한다. 예를 들면 Save 알림을 받으면 Player의 체력, 스태미너, 공격력 등 Player의 Stat 데이터를 모아놓은 데이터 컨테이너를 반환하고, Load 알림을 받으면 저장되어있던 체력, 스태미너, 공격력 등을 자신의 Stat 데이터와 동기화한다. Save나 Load하라는 알림을 받더라도, 어떤 데이터를 언제 Save하고 Load 할 지에 대해서는 전적으로 리스너가 결정한다.Class Diagram위 내용을 바탕으로 아래와 같이 클래스 다이어그램을 만들어보았다. Save 컨트롤러에 대해서는 따로 표시하진 않았다.먼저, 저장해야할 새로운 오브젝트가 추가된 경우 단순히 SaveManager에 ISaveable interface의 상속을 받은 Listener를 등록하고 데이터 컨테이너만 만들면 된다. 즉, 굉장히 유연해진다. 또한 기존에 등록된 오브젝트의 저장될 데이터 항목을 변경하는데도 굉장히 유연하다. 데이터 컨테이너는 전적으로 Listener가 관리하기 때문이다. 이는 오브젝트 제거에도 동일하게 적용된다.다만, 모든 데이터를 한번에 하나의 파일에 저장하고 싶진 않을 수 있다. 예를 들면 플레이어의 세팅(e.g. 입력키, 음량, 그래픽 등)과 플레이어의 게임 데이터 파일(실제 플레이어의 게임 진행 현황)은 완전히 다르다. 이것들을 한번에 저장하는 것은 말이 안되며 플레이어의 게임 데이터 파일을 여러개 만들고 싶을 수 있다. 이를 위해 우리는 2가지 요소를 도입했다. 첫번째는 SaveKey이다. 저장할 데이터를 큰 틀에서 분류하는 역할을 한다. 위 클래스 다이어그램을 보면 알겠지만 SaveKey.GameData와 SaveKey.Setting이 enum 타입의 값으로써 존재한다. 이는 데이터가 저장되는 단위를 큰 틀에서 분류한다. 두 번째는 같은 분류라도 여러 개의 파일을 만들고 싶을 수 있다. 이 때는 단순히 파일 이름인 fileName을 구분해주면 된다. 참고로 Key 값에 대한 타입은 굳이 enum이 아니여도 된다. 확실한건 큰틀에서 분류할 수 있는 역할만 수행해주면 된다.Source Code위 Save System에 적용 된 내용을 전부 이 포스트에서 보이는건 무리가 있어 Listener에 대해서만 보이겠다.ISaveable interface/// &lt;summary&gt;/// Interface for listener to save and load./// &lt;/summary&gt;public interface ISaveable{ /// &lt;summary&gt; /// If multiple instances of same type subscribe save manager, you need to identify them by ID. /// &lt;/summary&gt; string ID { get; } /// &lt;summary&gt; /// The listener can save a data by returning it. You need to define \"Serializable\" attribute for the data type. /// &lt;/summary&gt; /// &lt;returns&gt;a data to save&lt;/returns&gt; object Save(); /// &lt;summary&gt; /// The listener can load a data by getting from data parameter. /// &lt;/summary&gt; /// &lt;param name=\"data\"&gt;a data loaded&lt;/param&gt; void Load(object loaded);}Player Data Save Example등록과 해제 시점은 각 클래스의 역할에 맞게 적절한 시점으로 설정해주면 된다.여기서는 PlayerEntity가 존재하는 동안 항시 Save/Load 로직이 동작해야하기 때문에 Awake()에서 등록하고 OnDestory()에서 등록을 해제한다.public class PlayerEntity : MonoBehaviour, ISaveable{ private void Awake() { SaveManager.Add(this, SaveKey.GameData); } private void OnDestroy() { SaveManager.Remove(this, SaveKey.GameData); } string ISaveable.ID =&gt; null; object ISaveable.Save() =&gt; new PlayerData(this, this.IsDead ? this.MaxHealth : this.Health); void ISaveable.Load(object loaded) { if (loaded is PlayerData data) { this.MaxHealth = data.maxHelath; this.Health = data.health; this.MaxStamina = data.maxStamina; this.ResurrectionChance = data.resurrectionChance; this.RemainedLife = data.remainedLife; } }}[System.Serializable]public class PlayerData{ public readonly float health; public readonly float maxHelath; public readonly float maxStamina; public readonly bool resurrectionChance; public readonly int remainedLife; public PlayerData(PlayerEntity player) : this(player, player.Health) { } public PlayerData(PlayerEntity player, float health) { this.health = health; this.maxHelath = player.MaxHealth; this.maxStamina = player.MaxStamina; this.resurrectionChance = player.ResurrectionChance; this.remainedLife = player.RemainedLife; }}Feedback현재는 SaveManager에서 Binary 형식의 파일만 입출력하고 있다. 현재 프로젝트에서는 Binary 형식의 파일 입출력이면 충분했지만 CSV, Json 등의 다양한 파일 형식으로 저장하고 싶을 수 있다. 따라서 이에 대한 개선이 필요하다." }, { "title": "Deep Learning Basics (딥러닝 기초)", "url": "/deep-learning/deep-learning-basic/", "categories": "Deep Learning", "tags": "DL, AI", "date": "2022-03-10 00:00:00 +0900", "snippet": "Introduction딥러닝의 목적은 함수 근사이다. 예를 들어 입력이 2배가 되는 함수를 구한다면 $y = 2x$로 쉽게 구할 수 있을 것이다. 어떤 문제든지 입력과 출력이 있다. 딥러닝은 굉장히 복잡한 문제에 대해 출력을 보이는 하나의 함수를 구하는 방법이다.조금 더 구체적으로 보자면 딥러닝은 크게 추론과 학습으로 나눌 수 있다. 주어진 입력에 대해 추론을 한 후 이를 바탕으로 다시 학습을 진행한다. 이를 수없이 반복하면 반복적 패턴을 발견하고 점점 합리적 추론을 이끌어낸다. 이는 사람과 유사하다. 사람도 처음에는 어떤 일에 대해 종종 잘못된 결과를 내지만 일을 하면서 점점 발전한다. 이것이 딥러닝에도 그대로 반영되어있다.딥러닝에서 추론은 순전파(forward propagation), 학습은 역전파(backward propagation)를 통해 이루어진다. 추론이라는 과정은 입력 -&gt; 출력 방향이기 때문이고, 학습은 출력 -&gt; 입력 방향으로 피드백이 이루어지기 때문이다. 이러한 과정을 거쳐 어떤 문제를 해결하는 하나의 함수를 구하는 것이다. 이 포스트에서 출처를 명시하지 않은 자료는 모두 Deep Learning from Scratch의 자료이다.Neural Network먼저 딥러닝에서 신경망을 구축해야한다. 신경망이란 다음과 같다. 신경망은 인간 두뇌의 작동 방식을 반영함으로써 컴퓨터 프로그램이 패턴을 인식하고 AI, 머신 러닝 및 딥 러닝 분야의 일반적인 문제점을 해결할 수 있도록 지원합니다. (출처: IBM)신경망은 다음과 같이 구성된다. (출처: IBM)위 그림을 보면 알 수 있지만 신경망은 입력층(Input layer), 은닉층(Hidden layer), 출력층(Output player)로 구성된다.각 층은 여러개의 노드(뉴런)로 구성되며 각 노드당 하나의 값을 가지고 있다고 생각하면 된다. 인접한 층끼리는 서로 상호작용을 하는데 신호(입력)를 받는 층의 각 노드는 신호를 주는 층의 모든 노드로부터 신호를 받는다. 이때 신호가 바로 입력($x$)에 그 유명한 신경망 매개변수($w$)가 곱해진 $wx$ 값이다. Note: 몰론 엄밀하게는 편향(bias, $b$)이라는 값 역시 들어온다.Forward Propagation구체적으로 신경망의 처리 과정 중 추론 과정인 순전파에 대해 알아보자. 신경망의 장점은 국소적 범위에서의 원리만 알면 전체가 동일하게 동작한다는 것이다.신경망은 여러 순차적인 층(layer)으로 구성되며 어떤 데이터를 입력할 시 각 층을 지나 올바른 최종 결과를 추론하는 것이 목표이다. 이 때 순전파란 입력 데이터에 대한 출격값을 신경망의 연속적인 층의 앞 방향으로 전달하는 연속적인 연산 과정을 의미한다.신경망은 아래와 같은 그림처럼 처리된다. 아래 그림의 신경망은 입력층 -&gt; 1층 -&gt; 2층 -&gt; 출력층으로 입력층, 은닉층(2개), 출력층으로 구성된다.위 그림에서 1층의 첫번째 노드 $a_1^{(1)}$의 값은 $a_1^{(1)} = x_1w_{11}^{(1)} + x_2w_{12}^{(1)} + b_1^{(1)}$을 통해 구해진다. 이때 입력은 $x_1, x_2$, 신경망 매개변수는 $w$, 편향은 $b$이다. 특히 위 그림에서 편향 $b$를 위해 입력층에 값이 $1$인 노드를 추가했다. 이는 위 식에서 $1 \\times b$라고 생각하기 위해서이다. 다소 변수 구성이 복잡한데 다음 그림에서 변수 구성에 대해 설명한다.위 그림에서 매개변수 $w_{12}$의 $1$이 다음 층이고, $2$가 앞층이라 다소 헷갈릴 수 있다. 이렇게 설정된 이유는 이후에 연산을 처리할 때 신경망 매개변수가 다음층에 귀속된 것처럼 취급하기 때문이다.다시 원래로 돌아와서 순전파 연산을 생각해보면 $a_2^{(1)}, a_3^{(1)}$ 연산은 기존 $a_1^{(1)}$을 구한 방식과 동일하다는 것을 알 수 있으며 $a_1^{(1)} = x_1w_{11}^{(1)} + x_2w_{12}^{(1)} + b_1^{(1)}$ 연산을 확장해 간단한 벡터 표기로 나타낼 수 있다.\\[\\textbf{A}^{(1)} = \\textbf{X} \\cdot \\textbf{W}^{(1)} + \\textbf{B}^{(1)}\\]각 식의 구성 요소 $\\textbf{A}^{(1)} = \\begin{bmatrix} a_1^{(1)} &amp; a_2^{(1)} &amp; a_3^{(1)}\\end{bmatrix}$ $\\textbf{X} = \\begin{bmatrix} x_1 &amp; x_2\\end{bmatrix}$ $\\textbf{W}^{(1)} = \\begin{bmatrix} w_{11}^{(1)} &amp; w_{21}^{(1)} &amp; w_{31}^{(1)} \\\\ w_{12}^{(1)} &amp; w_{22}^{(1)} &amp; w_{32}^{(1)}\\end{bmatrix}$ $\\textbf{B}^{(1)} = \\begin{bmatrix} b_1^{(1)} &amp; b_2^{(1)} &amp; b_3^{(1)}\\end{bmatrix}$위와 같이 입력 신호에 대한 은닉층에서의 가중치 합 벡터 $\\textbf{A}$의 각 원소는 활성화 은닉층의 활성화 함수(activation function) $h(x)$에 의해 변환된다. 활성화 함수는 신경망에서 가중치 합을 변환해주는 함수로 노드(뉴런)이 활성화될지 말지 결정한다. 단순히 가중치 합 계산에서 끝나는 것이 아니라 활성화 함수를 통해 신경망의 표현을 좀 더 풍부하게 해준다.활성화 함수에는 여러 함수가 사용되는데 구체적인 함수는 아래에서 설명하도록 하겠다. 지금은 값을 변형시킨다는 정도만 알고있어도 충분하다.아래 그림은 연산된 가중치 합 벡터 $\\textbf{A}$를 활성화 함수를 통해 $\\textbf{Z} = h(\\textbf{A})$로 변환하는 과정을 보여준다.위에서 변환된 결과인 벡터 $\\textbf{Z}$는 다시 다음 층에 대한 입력이 되어, 위에서 진행했던 연산 과정을 반복해 최종 출력층까지 진행된다. 1층 $\\rightarrow$ 2층 2층 $\\rightarrow$ 출력층 참고로 위 마지막 출력층의 활성화 함수가 $\\sigma$로 나오는데, 이는 출력층의 활성화 함수로 은닉층에서 사용되는 활성화 함수와 보통 다른 함수를 쓰기 때문에 다르게 표시했다.지금까지 순전파 과정을 알아보았다. 그런데 위의 추론 과정을 보면 우리는 각 층에서 단순히 입력 데이터에 대한 가중치 곱을 더한 후 활성화 함수를 통해 변형한 것밖에 하지 않았다.\\[\\textbf{A} = \\textbf{X} \\cdot \\textbf{W} + \\textbf{B} \\\\\\textbf{Z} = h(\\textbf{Z})\\]위 과정이 왜 추론 과정인걸까? 신경망은 단순한 위 과정을 통해 어떻게 합리적 추론을 이끄는 걸까?예를 들어 $y = w_1x_1 + w_2x_2 + b$라는 함수가 있다고 해보자. 이 함수는 알겠지만 평면 방정식으로 3차원 공간에서 임의의 평면을 표현한다. 즉, 우리는 어떤 입력 데이터 $x_1, x_2$를 $w_1, w_2, b$를 통해 평면 영역 위의 점으로 나타낼 수 있다. 각 층에서의 신호 계산은 차원만 다를 뿐 위와 유사한 선형 방정식에 신호를 변형시키는 활성화 함수만 존재한다. 그런데 층이 여러개가 있다는 것은 위의 연산이 연속적으로 발생한다는 것이고, 이는 신경망 전체를 바라보았을 때 하나의 합성함수를 구성한다.아주 간단한 예시를 들어보자. 1층에 가중치 곱 함수 $t = a_1x$와 활성화 함수 $s = t^2$, 2층에 $u = a_2s$와 $y = \\sin(u)$가 있다고 해보자. 이들의 합성함수는 $y = \\sin(a_1^2a_2x^2)$으로 가중치 곱을 구하는 선형 함수와 간단한 비선형 활성화 함수 만으로 완전히 새로운 비선형 함수를 만들었다. 이와 같이 여러번 합성된다면 이 합성함수는 더욱 정교해질 것이다. 즉, 층이 깊어질수록 표현력은 높아지는 것이다. 이 합성함수의 매개변수에 올바른 값이 존재한다면 우리는 위와 같은 신호 변환과 전달만으로도 입력에 대한 추론을 할 수 있다. 따라서 우리의 목표는 신경망을 구축하고 데이터 패턴에 따른 올바른 신견망 매개변수를 찾는 것이다.Activation Function신경망 학습 이전에 중요한 한가지를 알고 넘어가야한다. 바로 활성화 함수이다.활성화 함수는 합성함수를 만들 때 가중치 곱에 대한 합을 구하는 선형 함수를 비선형 함수로 변환시켜주는 굉장히 중요한 함수이다. 단순 선형 함수들의 합성함수는 선형함수이기 때문에 합성의 의미가 없다. 즉, 층을 깊게 하는 의미가 없다. 층 하나로 하나의 선형 함수가 표현되며 이것만 학습시키면 선형 함수로만 구성된 여러 층을 학습 시키는 것과 동일하다. 따라서 활성화 함수는 반드시 비선형 함수여야한다.여기서는 책에서 소개된 아주 기초적인 몇개의 활성화 함수만 소개한다.Sigmoid주로 은닉층에서 사용되며 수식은 아래와 같다.\\[h(x) = \\displaystyle\\frac{1}{1 + e^{-x}}\\]그래프ReLU주로 은닉층에서 사용되며 수식은 아래와 같다.\\[\\begin{aligned} h(x) = \\begin{cases} x &amp; (x &gt; 0) \\\\ 0 &amp; (x \\leq 0) \\end{cases}\\end{aligned}\\]그래프Softmax주로 분류 문제를 풀 때 출력층에서 사용된다. Softmax 함수의 특징은 출력 노드의 값이 0 ~ 1 범위로, 출력 총합은 1이다. 즉, Softmax 함수의 출력은 일종의 확률이다. 수식은 아래와 같다.\\[y_k = \\displaystyle\\frac{e^{a_k}}{\\sum_{i=1}^n{e^{a_i}}}\\]위 식에서 $n$은 출력층의 뉴런 수, $a$는 입력, $y_k$는 출력층의 뉴런 중 $k$번째 출력이다.Backward Propagation추론 과정을 살펴봤으니 이제 학습 과정을 살펴보자. 신경망 학습이란 결국 최적의 신경망 매개변수 $\\textbf{W}$와 편향 $\\textbf{b}$를 찾는 과정이다. 신경망이 입력에 대해 어떤 결과를 추론했을 때, 그 추론은 맞을 수도 있고 틀릴 수도 있다. 특히, 학습 초기에는 거의 대부분 틀린다. 신경망이 어떤 추론을 이끌어냈을 때 그것이 올바른 답인지 아닌지 피드백을 해야할 필요가 있다. 피드백을 반영해야 학습이라고 할 수 있는 것이다.피드백을 하기 위해선 먼저 올바르게 추론했는지 평가를 해야한다. 이러한 평가를 나타내기 위해 흔히 손실 함수를 사용한다. 손실 함수란 말그대로 최적값으로부터 얼마만큼 손실인지 여부를 나타내는 함수이다. 따라서 손실이 작으면 작을 수록 올바른 정답을 유추했다고 평가할 수 있다.우리는 손실 함수가 미분 가능하다고 가정할 때 최소값에 해당되는 위치는 극소일 것이다. 어떤 함수의 극소를 찾는 방법은 여러가지가 있지만 가장 일반적으로 쓰이는 방법은 경사하강법이다. 경사하강법이란 현재 위치에서의 함수의 기울기(경사)를 활용해 일정 부분만큼 하강하는 방법이다. 함수의 기울기는 그 함수가 증가 함수인지, 감소 함수인지를 나타내는 중요한 지표이다. 즉, 기울기를 통해 우리는 함수값이 작아지는 방향을 알 수 있으며 그 방향으로 이동하다보면 원하는 최적 위치에 도달할 수 있다.손실 함수손실 함수는 수없이 많겠지만 이 책에서 소개하는 손실 함수는 2개이다.Mean Squared Error평균 제곱 오차(mean squared error, MSE)는 가장 자주 쓰이는 손실 함수 중 하나이다. 예측값과 실제값의 차이인 오차의 제곱을 평균을 낸다.\\[E = \\displaystyle\\frac{1}{2}\\displaystyle\\sum_k{(y_k - t_k)^2}\\]$y_k$는 예측값, $t_k$는 실제값이며 $\\displaystyle\\frac{1}{2}$은 미분 시 상수항을 1로 만들기 위해서이다.평균 제곱 오차는 제곱의 합이기 때문에 반드시 0보다 크거나 같다. 따라서 우리의 목표는 평균 제곱 오차를 0으로 도달시키는 것이다.Cross Entropy Error교차 엔트로피 오차(cross entropy error, CEE)는 분류 문제에서 자주 사용되는 손실 함수 중 하나이다. 우리는 분류 문제에서 Softmax 활성화 함수를 출력층에서 사용한다. 그렇기 때문에 최종 Output은 확률이다. 예를 들어 0 ~ 9까지의 숫자 분류 문제가 있다면 최종 output은 입력 데이터가 0 ~ 9 중 각각에 해당될 확률 분포이다. 우리는 Softmax를 통해 확률 분포를 얻었고, 확률 분포에 대한 오차를 구하려고 한다.먼저, Entropy 개념에 대해 간략하게만 알아보자. Entropy는 간단히 말하면 불확실성의 수준이다. 우주는 엔트로피가 증가하는 방향으로 진행된다는 그 유명한 말은 누구나 알것이다.아래 그림을 보자.(출처: Towards Data Science)우리가 삼각형과 원 둘 중에 하나를 랜덤하게 선택한다고 할 때 어느 컨테이너가 가장 불확실성이 클까? 당연히, 2번째 컨테이너일 것이다. 1번째 컨테이너는 삼각형을, 3번째 컨테이너는 원을 뽑을 확률이 압도적으로 높지만 2번째 컨테이너는 삼각형이 뽑힐지, 원이 뽑힐지 모른다. 이것이 불확실성이다.이러한 Entropy 개념을 활용한 손실 함수가 교차 교차 엔트로피 오차이다. 수식은 아래와 같다.\\[E = -\\displaystyle\\sum_k{t_k\\log{y_k}}\\]$y_k$는 예측값, $t_k$는 실제값이다.교차 엔트로피 오차의 입력은 확률이기 때문에 항상 0 ~ 1사이의 값이다. 따라서 $\\log$ 함수의 특성 상 교차 엔트로피 오차는 반드시 0보다 크거나 같다. 따라서 우리의 목표는 교차 엔트로피 오차를 0에 도달시키는 것이다. 아래는 $\\log$함수의 그래프이다.$\\log$함수 그래프교차 엔트로피와 관련된 자세한 내용은 아래 링크를 참조하길 바란다. Towards Data Science오차역전파법 개요최적의 신경망을 만들기 위해서는 신경망 매개변수의 최적값을 찾아야한다. 신경망 매개변수에 따른 추론 능력이 어느정도인지 평가하기 위해 손실함수를 정의했다. 결국 손실함수는 신경망 매개변수에 의한 합성함수이며 손실 함수 값을 최소로 만들기 위해 신경망 매개변수를 조절해야한다. 경사하강법을 사용하면 손실 함수의 기울기 정보만 알면 최적값을 찾을 수 있다. 즉, 우리는 신경망 매개변수 $\\theta$에대한 손실함수의 변화량인 $\\displaystyle\\frac{\\partial L}{\\partial \\theta}$를 구해야 한다.기울기를 구하기 위한 방법은 여러가지가 있다. 가장 간단한 방법은 수치미분이다. 수치미분 기법은 함수의 원형을 몰라도 기울기를 구할 수 있다는 장점이 있지만, 계산 속도가 느리다. 딥러닝에서는 계산의 효율성을 위해 오차역전파법이란 개념을 사용한다.합성함수 미분오차역전파법의 핵심은 합성함수 미분이다. 위에서 언급했듯이 손실함수는 신경망 매개변수에 의한 합성함수이다. 다음과 같은 합성함수가 있다고 할 때 합성함수 미분은 간단히 구할 수 있다.\\[y = f(t), \\; t = g(x) \\\\\\dfrac{dy}{dx} = \\dfrac{dy}{dt}\\dfrac{dt}{dx}\\]위 합성함수의 도함수는 $f(t)$의 도함수인 $\\dfrac{dy}{dt}$와 $g(x)$의 도함수인 $\\dfrac{dt}{dx}$의 곱으로 구성된다. 즉, 합성함수 미분은 합성함수를 구성하는 각 함수의 미분의 곱일 뿐이다.신경망 연산 과정 분리앞서 언급했듯이 합성함수 미분은 각 함수의 미분의 곱이다. 따라서 우리는 신경망의 연산 과정을 쪼갤 필요가 있다. 말 그대로 합성함수의 형태로 만들어주기 위해서이다. 각각의 연산 과정을 계층이라고 한다.신경망 연산 계층은 모듈화의 가치가 있는 최소 단위로 나눈다. 또한 각 계층은 순전파와 역전파 함수가 반드시 구현되어야한다.(이에 대해서는 좀있다 소개한다.) 예를 들면 Sigmoid나 ReLU 등의 활성화 함수는 하나의 계층이다. 입력에 대한 가중치 총합을 구하기 위한 신경망 매개변수와의 행렬 내적 연산 역시 하나의 계층으로 나타낼 수 있다. 특히 이 행렬 내적 계층을 Affine 계층이라고 부른다.이 방식의 장점은 각 계층이 국소적 계산만하면 된다는 점에 있다. 이로 인해 모듈화가 쉽고, 여러 계층을 조합해 새로운 신경망을 쉽게 만들 수 있으며, 연산 속도에도 큰 이점이 있다.구조오차역전파법의 구조는 앞서 언급했듯이 각 계층은 순전파와 역전파 함수가 구현되어있으며 국소적 계산만을 담당한다. 순전파 함수와 역전파 함수는 오차역전파법의 핵심이다. 순전파는 앞서 언급했듯이 입력에 대한 연산 결과를 앞(forward) 층으로 전달하는 연속적인 과정이다. 즉, 입력층 -&gt; 출력층 방향으로의 연산 과정이다. 반대로 역전파는 출력층 -&gt; 입력층으로 연산 결과를 전달하는데 이 때 연산 결과가 바로 원함수의 도함수의 함수값인 기울기이다. 아래 그림을 보자.신경망 계층 구조 그림위 그림은 신경망 연산을 위한 계층 구조이다. $2$와 유사한 그림 데이터를 입력했을 때 최종 출력층에서 각 정답 Layer에 대한 확률을 출력하고 있다. 이 과정이 순전파이다. 그와 동시에 피드백을 하기 위해 기울기를 구해야하는데 그 기울기를 구하기 위해 출력층 -&gt; 입력층으로 다시 전달하는 과정이 역전파이다. 이 때 각 계층은 순전파와 역전파 함수가 구현되어있으며 국소적 계산만을 담당한다. 즉, 각 계층은 하나의 단순한 원함수와 원함수의 도함수로만 구성된다.각 계층의 순전파, 역전파 함수순전파와 역전파 함수를 구현한 각 계층을 조합해 신경망을 만들 수 있다고 했다. 몇가지 예시를 살펴보자.ReLU 계층ReLU는 활성화 함수로 하나의 계층을 구성할 수 있다. 아래는 순전파에 사용되는 ReLU 함수이다.\\[\\begin{aligned} y = \\begin{cases} x &amp; (x &gt; 0) \\\\ 0 &amp; (x \\leq 0) \\end{cases}\\end{aligned}\\]그렇다면 역전파 함수는 어떨까? 그냥 단순히 위 함수를 미분하기만 하면 된다.\\[\\begin{aligned} \\dfrac{\\partial y}{\\partial x} = \\begin{cases} 1 &amp; (x &gt; 0) \\\\ 0 &amp; (x \\leq 0) \\end{cases}\\end{aligned}\\]Sigmoid 계층순전파 함수는 아래와 같다.\\[y = \\dfrac{1}{1 + e^{-x}}\\]Sigmoid 함수를 미분하자. 고등학교를 나왔다면 위 함수의 미분 자체는 어렵지 않을 것이다.\\[\\dfrac{\\partial y}{\\partial x} = \\dfrac{e^{-x}}{(1 + e^{-x})^2}\\]다만 위 식을 그대로 사용하지는 않는다. 위 식을 변형하면 지수계산을 하지 않고도 값을 얻을 수 있어 계산 속도를 증가시킬 수 있다. 위 식을 $x$에 대한 식이 아닌 $y$에 대한 식으로 변형하면 다음과 같다.\\[\\dfrac{\\partial y}{\\partial x} = \\dfrac{1}{1 + e^{-x}}\\dfrac{1 +e^{-x} - 1}{1 + e^{-x}} = y(1-y)\\]즉, 위와 같이 $y$에 대한 식으로 변형하면 순전파때 구한 $y$값을 추가적인 지수 연산 없이 단순한 곱셈과 뺄셈만으로 미분의 결과를 얻을 수 있다.Affine 계층Affine 계층은 입력과 신경망 매개변수 $\\theta$의 행렬 내적 연산을 수행하는 계층이다. 따라서 수식 자체는 단순한 선형방정식으로 간단하다. 다만 행렬 내적의 성격상 좌측의 열과, 우측의 행 차원의 크기를 동일하게 해야만 수행 가능하므로 주의해야한다. 순전파 함수의 수식은 아래와 같다.\\[y = \\textbf{X} \\cdot \\textbf{W} + \\textbf{b}\\] $\\textbf{X}$: 이전 layer의 value $\\textbf{W}$: 가중치 $\\textbf{b}$: 편향역전파 함수를 구할 때는 입력과 신경망 매개변수가 행렬임을 고려해 미분을 해야한다.\\[\\dfrac{\\partial L}{\\partial \\textbf{X}} = \\dfrac{\\partial L}{\\partial \\textbf{Y}} \\cdot \\textbf{W}^\\text{T} \\\\\\dfrac{\\partial L}{\\partial \\textbf{W}} = \\textbf{X}^\\text{T} \\cdot \\dfrac{\\partial L}{\\partial \\textbf{Y}}\\]증명 과정은 생략한다.오차역전파법 구성지금까지 오차역전파법을 만들기 위해 합성함수 미분과 신경망 계층 개념을 알아보았다. 위 계층들을 조합하면 신경망이 완성된다. 입력층에 데이터를 입력만 하면 값이 계층을 따라 연산 후 전달을 반복해 데이터의 흐름을 만들 수 있다.References[1] Github WegraLee/deep-learning-from-scratch[2] IBM 신경망" }, { "title": "[Unity] Scene View에서 Vector의 Position Handle 조작하기", "url": "/unity/unity-movetool-release1/", "categories": "Unity", "tags": "Unity, Unity Editor, Attribute, Reflection, Vector", "date": "2022-01-17 00:00:00 +0900", "snippet": "1. 개요유니티로 게임을 만들다 보면 Vector의 Position을 조작하는 일이 수도 없이 많다. 게임 오브젝트의 Position은 Transform 컴포넌트가 지원하는 자체적인 Move-Tool 도구로 Scene View에서 쉽게 위치를 조작할 수 있지만, 일반적인 Vector2 혹은 Vector3 타입의 필드의 값을 변경할 때는 수동으로 값을 직접 입력해야 한다. 이는 굉장한 불편함이다.몰론 Vector 구조체가 아닌, Empty GameObject를 생성 후 Transform 타입의 필드에 할당하여 조작하면 되지만 Vector 구조체 단독으로 사용하는 것보다는 메모리, 성능 등 효율성이 다소 떨어진다. 구조체의 성능적인 장점을 최대한 활용하는 것이 아무래도 좋지 않겠는가? 아래 장면을 보자.위 장면을 보면 알 수 있지만 Transfrom 컴포넌트를 조작하는 것이 아닌 Vector3 타입의 직렬화된 필드 자체를 Scene View에서 조작하고 있다. 위와 같이 필드로 선언되어있는 Vector 구조체를 Scene View의 Move-Tool 도구를 활용해 조작하는 방법을 알아보자.Releases이 블로그는 Release 1을 기준으로 작성되었기 때문에 deprecated 된 내용이 포함되어있습니다. 업데이트된 버전을 이용하시기 바랍니다.아래는 제가 배포하는 공식 릴리스입니다. Version Release Date Source C# main(unstable) – main 7.0 or higher Release 2 2022-02-16 release-2 7.0 or higher 2. 구현에 앞서 필요한 지식우리가 흔히 유니티에서 인스펙터를 간편히 조작하기 위해 아래와 같이 필드에 Attribute 를 정의한다.[SerializeField]private Vector3 privateVector;위 코드에서 정의한 Attribute 는 유니티에서 가장 자주 쓰이는 Attribute 중 하나인 SerializeField이다. 이 Attribute 는 private 필드를 인스펙터 등에서 조작 가능하게 직렬화 시켜준다. 이처럼 Vector 구조체 필드 역시 복잡한 과정 없이, 간단히 Attribute 의 선언만으로 쉽게 Scene View에서 조작하고 싶다. 이를 위해 위와 같이 Attribute 를 만들기로 결정했다.Attribute(특성)를 어떻게 구현할까? MS 공식 문서 특성(C#)에서는 아래와 같이 기술하고 있다. 특성은 메타데이터 또는 선언적 정보를 코드(어셈블리, 형식, 메서드, 속성 등)에 연결하는 강력한 방법을 제공합니다. 특성이 프로그램 엔터티와 연결되면 리플렉션 이라는 기법을 사용하여 런타임에 특성이 쿼리될 수 있습니다. 자세한 내용은 리플렉션(C#)을 참조하세요.위 내용만 봐서는 프로그래밍에 깊은 지식과 이해가 있지 않은 한 이해하기 어렵다고 생각한다. 또한 Reflection(리플렉션)이라는 개념도 등장한다. 이들은 C#의 고급기술에 속한다(뇌피셜). 나도 처음에는 다소 생소하고 어려웠었다. 그러나 Move-Tool 구현을 위해 Reflection 에 대해 알아갈 수록, 정말 대단한 기능이라고 느껴질 정도로 강력한 기능이었다. 이에 대해 간단히 알아보자.Atribute(특성)먼저, Attribute에 대해 내가 이해한 핵심적인 요약은 아래와 같다. Attribute 는 프로그램이 이해할 수 있는 주석이다.우리가 주석을 작성하는 이유는 개발하는 나, 코드를 읽는 다른 개발자가 코드의 내용을 이해할 수 있게 하기 위해 작성한다. Attribute 역시 그러하다. 프로그램에게 이 클래스, 인터페이스, 메서드, 프로퍼티 혹은 필드가 이러한 특성을 지니고 있다라고 알려주는 역할을 한다. 예를 들면 C#에서 가장 유명한 Attribute 중 하나인 Obsolete는 프로그램 혹은 컴파일러에게 더 이상 사용되지 않는다라고 알려준다.마찬가지로 우리가 Vector 구조체를 Scene View에서 조작할 수 있게 하기 위해 Attribute 를 만들려고 한 이유도 이와 같다. 즉, 이 필드는 Scene View에서 Position을 조작할 수 있는 Move-Tool 도구를 지원해야한다고 프로그램에게 알리기 위해서이다.Reflection(리플렉션)그렇다면 누가 어떤 Attribute 를 가지고 있는지 알 수 있을까? C#에서는 Reflection(리플렉션) 이라는 기법을 통해 가능하다. Reflection 은 런타임에 어떤 타입에 대한 정보를 뜯어볼 수 있도록 해준다. 이는 굉장한 기능이다. 우리가 선언한 클래스 혹은 인터페이스를 비롯해 각종 메서드, 필드에 대해 런타임에 확인하고 조작할 수 있다는 것을 의미한다. 어떤 클래스 내에 선언한 필드에 어떤 Attribute 가 할당되어있는지 역시 런타임에 조사가 가능하다.이것이 가능한 이유는 바로 아래와 같다. C#의 모든 타입의 Base Class인 Object Class에 Type GetType() 메서드가 선언되어 있다.즉, 모든 형식의 타입에 대해 정보를 열람할 수 있다.앞서 말했듯이 Reflection 은 어떤 타입에 대한 정보를 뜯어볼 수 있도록 해준다. 즉, 어떤 타입에 대한 정보를 담고 있는 보관함이 필요한데, 그게 바로 System.Type 클래스이다. Type 인스턴스를 통해 어떤 타입의 정보를 열람하고, 타입 멤버의 데이터를 얻거나 수정이 가능하다. 자세한 내용은 MS 공식 문서 리플렉션(C#)을 참조하기 바란다.3. 전체 알고리즘위에서 구현을 위한 지식을 간단히 소개했다. 필요한 문법은 아래에서 간단히 소개할 것이나, 자세한 내용이 궁금하다면 직접 문서를 참조하길 권장한다.아래는 소스코드가 있는 Github 링크이다. 소스코드가 상당히 길기 때문에, Github Repository에서도 제공하였다. Code from Github4. MoveToolAttribute먼저, 필요한 Attribute 를 선언하자. 나는 Scene View에서 Move-Tool 도구를 지원해준다는 의미로 MoveToolAttribute라는 이름으로 선언했다.C#에서 Attribute(특성) 를 생성하기 위해서는 System.Attribute 클래스의 상속을 받아야 한다. 위 코드에서는 UnityEngine.PropertyAttribute를 상속받았는데 이 클래스는 Attribute클래스의 파생 클래스이다. 따라서 Attribute 를 만들 수 있다.[AttributeUsage(AttributeTargets.Field, AllowMultiple = false, Inherited = true)]위 코드는 우리가 만들려는 Attribute 의 특성을 기술하는 AttributeUsage 특성이다.System.AttributeTargets은 C#에 정의되어 있는 enum 타입이다. 특성을 적용하는 데 유효한 애플리케이션 요소(클래스, 구조체, 메서드, 필드 등)를 지정할 수 있다. 참고로 AttributeTargets의 열거형 값은 AttributeTargets.Class | AttributeTargets.Method와 같이 비트 OR 연산으로 결합 하여 사용할 수 있다.우리는 Vector 타입의 필드에만 적용할 거기 때문에 AttributeUsage의 생성자에 AttributeTargets.Field 값을 인자로 주었다. 이에 대한 자세한 내용은 MS 공식 문서 AttributeTargets 열거형에서 확인하기 바란다.5. MoveToolAvailableAttributeVector 타입이 아닌 Vector 타입의 필드 및 컬렉션을 포함하는 커스텀 타입에서 Move-Tool 도구를 사용하고 싶을 때 정의해야하는 Attribute 이다.Attribute 적용 대상은 class와 struct이다.6. MoveToolEditorMoveToolAttribute를 정의한 필드를 위해 Move-Tool(Position Handle)을 유니티 에디터의 Scene View에 생성 및 배치하는 기능을 구현한다. 이를 위해 커스텀 에디터를 만들어야 한다.전체 알고리즘C#의 Reflection과 Attribute 기능을 최대한 활용하여 구현했다.참고로 흔히 에디터를 작성할 때 사용하는 Editor 베이스 클래스의 멤버인 serializedObject 프로퍼티나 기타 방법을 통해 얻을 수 있는 SerializedProperty 타입의 직렬화된 프로퍼티들을 전혀 활용하지 않는다.이전에 이것들을 활용해 코드를 작성했었는데 아래와 같은 치명적인 문제가 있었다.Unity 공식 문서 Editor.serializedObject의 설명 Do not use the serializedObject inside OnSceneGUI or OnPreviewGUI. Use the target property directly in those callback functions instead.근데 이해가 안갔던 점은 MoveToolEditor를 다른 Editor에서 생성 후 OnSceneGUI를 호출할때에만 위의 Exception 발생했었다. 즉, 다른 Editor를 사용하지 않고 MoveToolEditor가 단독으로 사용되면 serializedObject를 OnSceneGUI에서 사용하더라도 Exception이 발생하지 않았다.또한 Exception 메세지는 발생하지만 기능 자체는 정상적으로 동작했다.그럼에도 계속 Exception 메세지가 콘솔창에 쌓이기 떄문에 결국 버그 수정을 위해 완전히 리뉴얼했다.이 포스트에서 소개하는 코드는 리뉴얼된 버전으로 serializedObject를 사용하지 않고, C#의 순수 Reflection 기능만을 활용해 알고리즘을 구현하였다.아래는 MoveToolEditor의 전체 알고리즘이다.GetSerializedFields어떤 타입의 모든 직렬화된 필드를 열거자로 반환한다.// Return SerializedFields.private IEnumerable&lt;FieldInfo&gt; GetSerializedFields(Type type){ var fields = type.GetFields(BindingFlags.Instance | BindingFlags.Public | BindingFlags.NonPublic); // If it's the public field and doesn't have NonSerializedAttribute, then add it. // If it's the non-public field and has UnityEngine.SerializeField, then add it. var serializedFields = from f in fields where (f.IsPublic &amp;&amp; f.GetCustomAttribute&lt;NonSerializedAttribute&gt;(false) == null) || (!f.IsPublic &amp;&amp; f.GetCustomAttribute&lt;SerializeField&gt;(false) != null) select f; return serializedFields;}LINQ 쿼리문을 통해 필드들을 필터링했다. public 필드는 NonSerializedAttribute가 존재하지 않을 때, non-public 필드는 UnityEngine.SerializeField가 존재할 때 직렬화가 가능하기 때문에, 이에 대해 필터링 후 열거자 형태로 반환한다.SetMoveToolSetMoveTool() 메서드는 에디터의 Move-Tool 기능을 실행시켜주는 메서드이다./// &lt;summary&gt;/// Run MoveToolEditor. You can use position handles of fields that have MoveToolAttribute in the unity editor scene view. /// &lt;/summary&gt;public void SetMoveTool(){ var targetType = target.GetType(); var fields = GetSerializedFields(targetType); foreach (var field in fields) { // Check if MoveToolAttribute is defined. var attr = field.GetCustomAttribute&lt;MoveToolAttribute&gt;(false); if (attr == null) continue; SetMoveToolAvailableField((field, -1), (field, -1), attr, this.target); }}위 코드에서 target은 inpected되고 있는 오브젝트로, 보통 유니티 에디터의 Hierarchy 창에서 선택한 오브젝트이다.target 오브젝트의 모든 직렬화된 필드를 GetSerializedFields()를 통해 가져온다. 그 중 MoveToolAttribute가 존재하는 필드만 Move-Tool 기능을 구현하며, 이를 위해 SetMoveToolAvailableField() 메서드를 호출한다.위 메서드에서 사용한 GetCustomAttribute&lt;T&gt;()는 System.Reflection의 확장 메서드로, 어떤 멤버에 정의되어 있는 Attribute 를 반환한다. MS 공식 문서 GetCustomAttribute&lt;T&gt;(MemberInfo, Boolean)HasAvailableAttribute해당 타입에 MoveToolAvailableAttribute와 SerializableAttribute가 정의되어 있는지 확인한다. 이는 커스텀 타입이 Move-Tool 기능을 사용할 수 있는지 확인하기 위한 메서드이다.// Check if both MoveToolAvailableAttribute and SerializableAttribute are defined.private bool HasAvailableAttribute(Type type){ var available = type.GetCustomAttribute&lt;MoveToolAvailableAttribute&gt;(false); var seralizable = type.GetCustomAttribute&lt;SerializableAttribute&gt;(false); return available != null &amp;&amp; seralizable != null;}IsVector해당 타입이 Vector이거나 Vector 컬렉션인지를 확인하기 위한 메서드이다.// Check if it's vector type or vector collection type.private bool IsVector(Type type) =&gt; type == typeof(Vector2) || type == typeof(Vector3) || typeof(IEnumerable&lt;Vector3&gt;).IsAssignableFrom(type) || typeof(IEnumerable&lt;Vector2&gt;).IsAssignableFrom(type);AddIndexLabel단순히 레이블에 인덱스 기호와 번호를 추가해주는 메서드이다.// Add index label to this label parameter.// e.g. Label [index]private string AddIndexLabel(string label, int index, bool isFront = false){ if (index &gt;= 0) { if (isFront) { label = $\"[{index}] {label}\"; } else { label += $\" [{index}]\"; } } return label;}SetMoveToolAvailableField드디어 메인 코드이다.유니티 에디터의 Scene View에 Move-Tool 기능을 구현하기 위해 관련 요소를 파악한다.최상위 필드에서 최하위 목록까지 내려가 최종적으로 구현 가능한지 조건을 따져 Move-Tool 기능을 구현하는 Top-down 방식의 메서드이다.코드가 재귀호출 구조이며 상당히 복잡한 편이다. 일단은 대략적인 구조만 살펴보고 아래 설명을 본 후 다시 돌아와서 전체 구조를 살펴보는걸 권장한다./// &lt;summary&gt;/// Set Position Handles in the unity editor scene view./// &lt;/summary&gt;/// &lt;param name=\"top\"&gt;top level field declared in the MonoBehaviour component&lt;/param&gt;/// &lt;param name=\"current\"&gt;current field checked now, current.obj is the instance where current.field is declared&lt;/param&gt;/// &lt;param name=\"attr\"&gt;defined for the top level field&lt;/param&gt;/// &lt;param name=\"n\"&gt;Don't set any value. It's the count of recursive calls.&lt;/param&gt;private void SetMoveToolAvailableField((FieldInfo field, int index) top, (object obj, FieldInfo field, int index) current, MoveToolAttribute attr, int n = 0){ // If it's vector, call immediately SetPositionHandle() method and then terminate. if (IsVector(current.field.FieldType)) { string label = string.Empty; if (attr.LabelOn) { label = string.IsNullOrEmpty(attr.Label) ? AddIndexLabel(top.field.Name.InspectorLabel(), top.index) : AddIndexLabel(attr.Label, top.index); if (top.field != current.field) label += $\" - {(n &gt; 1 ? AddIndexLabel(current.field.Name.InspectorLabel(), current.index, true) : current.field.Name.InspectorLabel())}\"; } SetVectorField(current.obj, current.field, label, attr.LocalMode); return; } var type = current.field.FieldType; //current field type // Array if (type.IsArray) { type = type.GetElementType(); if (!HasAvailableAttribute(type)) return; var serializedFields = GetSerializedFields(type); var array = current.field.GetValue(current.obj) as Array; for (int i = 0; i &lt; array.Length; i++) { if (top.field == current.field) top.index = i; // Recursive call for each field declared in the element type of current array object obj = array.GetValue(i); foreach (var nextField in serializedFields) SetMoveToolAvailableField(top, (obj, nextField, i), attr, n + 1); if (type.IsValueType) array.SetValue(obj, i); } } // List else if (type.IsGenericType &amp;&amp; typeof(IList).IsAssignableFrom(type)) { type = type.GetGenericArguments()[0]; if (!HasAvailableAttribute(type)) return; var serializedFields = GetSerializedFields(type); var list = current.field.GetValue(current.obj) as IList; for (int i = 0; i &lt; list.Count; i++) { if (top.field == current.field) top.index = i; // Recursive call for each field declared in the element type of current list object obj = list[i]; foreach (var nextField in serializedFields) SetMoveToolAvailableField(top, (obj, nextField, i), attr, n + 1); if (type.IsValueType) list[i] = obj; } } // Just single field else { if (!HasAvailableAttribute(type)) return; var serializedFields = GetSerializedFields(type); // Recursive call for each field declared in the current field type object obj = current.field.GetValue(current.obj); foreach (var nextField in serializedFields) SetMoveToolAvailableField(top, (obj, nextField, -1), attr, n + 1); // If current field is a value type, you must copy boxed obj to this field. It's because obj isn't the field instance itself, but new boxed instance. if (type.IsValueType) current.field.SetValue(current.obj, obj); }}현재 필드가 Vector일 경우현재 필드가 Vector일 경우 Move-Tool 기능을 위한 Position Handle을 유니티 Scene View에 생성 및 배치하는 SetPositionHandle() 메서드를 호출하고 재귀호출을 종료한다. SetPositionHandle() 메서드는 뒤에서 자세히 설명할 예정이다.// If it's vector, call immediately SetPositionHandle() method and then terminate.if (IsVector(current.field.FieldType)){ string label = string.Empty; if (attr.LabelOn) { label = string.IsNullOrEmpty(attr.Label) ? AddIndexLabel(top.field.Name.InspectorLabel(), top.index) : AddIndexLabel(attr.Label, top.index); if (top.field != current.field) label += $\" - {(n &gt; 1 ? AddIndexLabel(current.field.Name.InspectorLabel(), current.index, true) : current.field.Name.InspectorLabel())}\"; } SetVectorField(current.obj, current.field, label, attr.LocalMode); return;}Vector가 아닌 단일 필드일 경우먼저 위 SetMoveToolAvailableField() 메서드의 조건문 분기 중 else문 부터 보자.Vector 타입이 아닌 커스텀 타입의 컬렉션이 아닌 단일 필드일 경우의 처리이다.// Just single fieldelse{ if (!HasAvailableAttribute(type)) return; var serializedFields = GetSerializedFields(type); // Recursive call for each field declared in the current field type object obj = current.field.GetValue(current.obj); foreach (var nextField in serializedFields) SetMoveToolAvailableField(top, (obj, nextField, -1), attr, n + 1); // If current field is a value type, you must copy boxed obj to this field. It's because obj isn't the field instance itself, but new boxed instance. if (type.IsValueType) current.field.SetValue(current.obj, obj);}현재 필드의 타입이 Move-Tool 기능을 이용 가능한지 확인하기 위해 앞서 정의했던 HasAvailableAttribute() 메서드를 호출한다.이용 가능하다면, 현재 타입에 선언되어 있는 모든 직렬화된 필드를 GetSerializedFields() 메서드를 통해 가져온다.현재 필드 인스턴스를 FieldInfo.GetValue() 메서드를 통해 얻은 후 object obj 지역변수에 할당한다.현재 필드 타입에 정의된 각각의 직렬화된 필드에 대해 SetMoveToolAvailableField() 메서드를 재귀 호출한다.이때, current 튜플 매개변수의 obj에 직전에 얻은 지역변수 obj, field에 현재 필드 타입에 정의된 각각의 직렬화된 필드, index에 인덱스를 인자로 넘겨준다. 현재 필드는 배열이 아닌 단일 필드이기 때문에 인덱스는 -1의 값을 준다.FieldInfo.GetValue()FieldInfo.GetValue() 메서드는 리플렉션의 핵심 기능 중 하나이다. 우리가 어떤 인스턴스와 그 인스턴스에 선언된 필드 정보를 알고 있다면, 그 인스턴스의 필드가 보유하고 있는 실제 값을 알아낼 수 있다. 반대로 FieldInfo.SetValue() 메서드도 존재한다.이것은 정말 강력한 기능이다. 그 이유는 아래와 같다. 어떤 클래스 혹은 구조체 필드가 private이나 protected 등의 키워드를 통해 외부로부터 보호되고 있는 필드일 지라도, 그 필드에 대한 정보만 안다면, 외부에서 그 필드가 보유하고 있는 값을 알아내고 수정할 수 있다.자세한 내용은 MS 공식 문서 FieldInfo.GetValue(Object) 메서드를 참조하기 바란다.구조체에 대한 리플렉션현재 필드 타입에 정의된 각각의 직렬화된 필드에 대한 재귀호출이 모두 끝났다면, 지역변수 obj 인스턴스에 대한 각각의 직렬화된 필드들은 모두 값이 변경되었다. 이때, 현재 필드가 구조체인 value type 이라면 문제가 발생한다.현재 필드가 클래스라면 obj 지역변수는 곧 current.obj를 통해 얻은 실제 인스턴스 그 자체이다. 몰론 엄밀하게는 인스턴스에 대한 참조이다.그러나 구조체라면 current.obj를 통해 얻은 인스턴스 참조가 아닌, 값만 동일한 박싱되어 새롭게 생성된 인스턴스이다. 이는 value type 의 박싱 언박싱 과정때문이다. 따라서 지역변수 obj를 변경하더라도 새롭게 생성된 인스턴스를 변경한 것이기 때문에 기존에 current.obj가 보관하고 있는 현재 필드는 변경되지 않는다. 따라서 변경된 obj 지역변수를 현재 필드에 복사해줘야한다.위 이유로 구조체를 고려하지 않았을 때는 구조체에 대해서는 Position Handle 자체는 표시했지만 핸들을 움직이려고 시도해도 고정된채로 남아있었다.Vector가 아닌 배열 혹은 컬렉션 필드일 경우Vector가 아닌 배열 혹은 컬렉션 필드는 위 Vector가 아닌 단일 필드일 경우에서 소개한 내용의 연장선일 뿐이다. 단지, 현재 단일 필드에 대해서가 아닌, 배열 혹은 컬렉션의 각각의 원소에 대해서 재귀호출을 실행한다는 차이가 있다. 배열과 컬렉션은 원리가 완전히 동일하므로 여기서는 배열에 대해서만 코드를 제시한다.참고로 이 코드에서의 GetValue(), SetValue()는 FieldInfo.GetValue(), FieldInfo.SetValue()가 아닌 Array.GetValue(), Array.SetValue()이다.// Arrayif (type.IsArray){ type = type.GetElementType(); if (!HasAvailableAttribute(type)) return; var serializedFields = GetSerializedFields(type); var array = current.field.GetValue(current.obj) as Array; for (int i = 0; i &lt; array.Length; i++) { if (top.field == current.field) top.index = i; // Recursive call for each field declared in the element type of current array object obj = array.GetValue(i); foreach (var nextField in serializedFields) SetMoveToolAvailableField(top, (obj, nextField, i), attr, n + 1); if (type.IsValueType) array.SetValue(obj, i); }}SetPositionHandle벡터 타입의 필드나 컬렉션에 대해 Move-Tool 기능을 위한 Position Handle을 유니티 에디터 Scene View에 실제로 생성 및 배치하는 메서드이다.// Add position handles of this field to unity editor scene view. This field is okay whether vector field or vector collection field.private void SetVectorField(object obj, FieldInfo field, string label, bool localMode){ // If it's local mode, then origin point is set to target(MonoBehaviour) position. Vector3 origin = localMode ? (this.target as MonoBehaviour).transform.position : Vector3.zero; var fieldType = field.FieldType; // Field if (fieldType == typeof(Vector3)) { Vector3 oldValue = (Vector3)field.GetValue(obj); SetHandleVector3(label, origin, oldValue, obj, field, v =&gt; field.SetValue(obj, v)); } else if (fieldType == typeof(Vector2)) { Vector2 oldValue = (Vector2)field.GetValue(obj); SetHandleVector2(label, origin, oldValue, obj, field, v =&gt; field.SetValue(obj, v)); } // Array else if (fieldType.GetElementType() == typeof(Vector3)) { var array = field.GetValue(obj) as Array; for (int i = 0; i &lt; array.Length; i++) { string temp = label; if (!string.IsNullOrEmpty(label)) temp += $\" [{i}]\"; Vector3 oldValue = (Vector3)array.GetValue(i); SetHandleVector3(temp, origin, oldValue, obj, field, v =&gt; array.SetValue(v, i)); } } else if (fieldType.GetElementType() == typeof(Vector2)) { var array = field.GetValue(obj) as Array; for (int i = 0; i &lt; array.Length; i++) { string temp = label; if (!string.IsNullOrEmpty(label)) temp += $\" [{i}]\"; Vector2 oldValue = (Vector2)array.GetValue(i); SetHandleVector2(temp, origin, oldValue, obj, field, v =&gt; array.SetValue(v, i)); } } // List else if (fieldType == typeof(List&lt;Vector3&gt;)) { var list = field.GetValue(obj) as List&lt;Vector3&gt;; for (int i = 0; i &lt; list.Count; i++) { string temp = label; if (!string.IsNullOrEmpty(label)) temp += $\" [{i}]\"; Vector3 oldValue = list[i]; SetHandleVector3(temp, origin, oldValue, obj, field, v =&gt; list[i] = v); } } else if (fieldType == typeof(List&lt;Vector2&gt;)) { var list = field.GetValue(obj) as List&lt;Vector2&gt;; for (int i = 0; i &lt; list.Count; i++) { string temp = label; if (!string.IsNullOrEmpty(label)) temp += $\" [{i}]\"; Vector2 oldValue = list[i]; SetHandleVector2(temp, origin, oldValue, obj, field, v =&gt; list[i] = v); } } // If you want to use position handles of other serializable collection, then add here or modify list part.}위 메서드에서 중요한 포인트는 Position Handle을 배치하고 필드의 값을 변경하기 위해 FieldInfo.GetValue()와 FieldInfo.SetValue() 메서드를 활용하고 있다는 점이다. 필드에 대한 정보만 알고 있다면 그 필드의 값을 얻거나 수정하는게 가능하다.SetHandleVector3 &amp; SetHandleVector2이 메서드들은 각각 Vector3와 Vector2 타입의 값에 대해 실제로 Position Handle을 배치하고 값을 변경하는 메서드이다. 리플렉션을 통해 값을 수정하고 있기 때문에 Undo.RecordObject()와 PrefabUtility.RecordPrefabInstancePropertyModifications()를 호출해줘야 한다. 그런데 PrefabUtility.RecordPrefabInstancePropertyModifications()의 경우 유니티 공식문서에서는 호출하라고 적혀있지만 사용하지 않아도 기존 기능과 차이가 없다. 이유를 모르겠다. Undo.RecordObject - Scripting API - Unity// Create Position Handle for Vector3. If it's changed, set and record new value.// You need to implement a mechanism to set the new Vector3 value in setValue delegate.private void SetHandleVector3(string label, Vector3 origin, Vector3 oldValue, object obj, FieldInfo field, Action&lt;Vector3&gt; setValue){ Handles.Label(origin + oldValue, label, style); EditorGUI.BeginChangeCheck(); Vector3 newValue = Handles.PositionHandle(origin + oldValue, Quaternion.identity) - origin; if (EditorGUI.EndChangeCheck()) { // enable ctrl + z &amp; set dirty Undo.RecordObject(target, $\"{target.name}_{target.GetInstanceID()}_{obj.GetHashCode()}_{field.Name}\"); setValue(newValue); // In the unity document, if the object may be part of a Prefab instance, we have to call this method. // But, even if i don't call this method, it works well. I don't know the reason. PrefabUtility.RecordPrefabInstancePropertyModifications(target); }}// Create Position Handle for Vector2. If it's changed, set and record new value.// You need to implement a mechanism to set the new Vector2 value in setValue delegate.private void SetHandleVector2(string label, Vector2 origin, Vector2 oldValue, object obj, FieldInfo field, Action&lt;Vector2&gt; setValue){ Handles.Label(origin + oldValue, label, style); EditorGUI.BeginChangeCheck(); Vector2 newValue = (Vector2)Handles.PositionHandle(origin + oldValue, Quaternion.identity) - origin; if (EditorGUI.EndChangeCheck()) { // enable ctrl + z &amp; set dirty Undo.RecordObject(target, $\"{target.name}_{target.GetInstanceID()}_{obj.GetHashCode()}_{field.Name}\"); setValue(newValue); // In the unity document, if the object may be part of a Prefab instance, we have to call this method. // But, even if i don't call this method, it works well. I don't know the reason. PrefabUtility.RecordPrefabInstancePropertyModifications(target); }}7. 정리우리는 MonoBehaviour에 대한 단 하나의 에디터만을 작성했으며 이를 모든 MonoBehaviour 컴포넌트 객체에 적용할 수 있다. 만약, 리플렉션을 활용하지 않으면 어떤 컴포넌트의 필드에 Move-Tool 기능을 구현하고 싶을 때마다 그 컴포넌트에 대한 에디터를 새롭게 작성하고, 그 필드에 값을 넣는 코드를 매번 작성해야 한다. 이는 굉장히 비효율적이다.그러나 Reflection 을 활용하면 위와 같이 동적으로 제어할 수 있다. 개발자가 만든 어떤 타입의 어떤 필드에 Move-Tool 기능을 구현할 지는 모르겠지만, 개발자가 Move-Tool 기능을 구현해달라고 MoveToolAttribute를 선언만 하면, 자동으로 에디터는 이 Attribute를 인식해 관련된 필드 정보를 가져와 Move-Tool 기능을 구현하여, 값을 변경시켜준다." }, { "title": "[Unity] 유니티 첫 게임 - 첫번째 보스 리퍼", "url": "/unity/unity-first-game-boss/", "categories": "Unity", "tags": "Unity, Game Development, Project, Enemy", "date": "2022-01-09 00:00:00 +0900", "snippet": "1. 개요유니티로 개발한 첫 게임에서 구현한 보스 몬스터이다.아직 개발 중이기 때문에 천천히 업데이트할 예정이다.에셋 정보사용한 에셋은 동료가 직접 그린 그림이며 아래와 같다.보스 기본 상태 - 무기 모드보스 무기 - 낫2. 낫 무기낫 무기 구조기본 구조는 아래와 같다. Reaping Hook - 낫 무기 본체 Effect - 낫 무기 이동 흔적 이펙트 Bar Start - 막대기 부분의 시작점 Bar End - 막대기 부분의 끝점 위 Bar Start와 Bar End 오브젝트는 막대기 부분의 영역 표시를 위한 오브젝트로 칼날 부분과 막대기 부분의 데미지 경감을 주기 위해 만들었다.낫 - Hierarchy콜라이더 설정낫 무기 이미지 자체가 Capsule Collider 2D 등의 일반적인 Collider로는 만들기 어렵다. Collider를 겹쳐서 만들어도 되지만 이는 성능적인 이슈가 있다고 한다. 따라서 이미지 형상 그대로 Collider를 형성할 수 있는 Polygon Collider 2D를 활용하였다.낫 - Scene View낫 - Inspector이펙트 설정Trail Renderer 컴포넌트를 활용해 낫의 이동 궤적을 따라 이펙트를 렌더링하는 기능을 추가했다.3. 낫 무기를 활용한 스킬낫 무기의 형태를 완성했으니 이제 이를 스킬을 구현해 활용하겠다.Single Reaping Hook Swing낫 하나를 타겟(플레이어) 근처에 생성해 휘두른다.Double Reaping Hook Swing낫 두개를 타겟(플레이어) 근처에 생성해 휘두른다.Throw Rotated Reaping Hook회전하는 낫을 리퍼 근처에 생성해 일정 시간 대기 후 플레이어를 향해 날린다.Sequential Reaping Hook낫 4개를 순차적으로 생성 후 순차적으로 일직선으로 날린다. 플레이어에 위치에 따라 회전과 스케일의 $x$성분을 조정한다.4. 손뼈 무기를 활용한 스킬손뼈 무기의 경우 특별히 따로 설명할 것은 없다. 따라서 바로 손뼈 무기를 활요한 스킬 설명으로 넘어가겠다.Grab플레이어의 상당한 뒤쪽에서 손뼈를 생성 후, 손뼈가 바라보는 $x$축과 평행한 방향으로 빠르게 이동한다. 손뼈가 타겟(플레이어)과 충돌 시 플레이어를 잡는다. 잡힌 상태에서 플레이어는 이동과 총기 발사가 제한되며, 지속데미지를 입는다. 잡힌 상태에서 빠져나오기 위해서는 스페이스 키를 총 30번 눌러야 한다.참고로 스페이스 키를 눌러야 하는 횟수 표시를 위한 UI를 적용하기 위해 TextMeshPro - Text 컴포넌트를 활용하였다.Smash추가될 예정5. 그 외 스킬Spread이 스킬은 다른 개발자 동료가 개발한 스킬이다.투사체를 4방향으로 지속적으로 발사한다. 매 발사 시점마다 발사각을 회전시킨다." }, { "title": "[Unity] 포물선 운동(점프) 구현", "url": "/unity/unity-projectile-motion/", "categories": "Unity", "tags": "Unity, Algorithm, Physics, Jump, Parabola, Projectile Motion", "date": "2021-12-24 00:00:00 +0900", "snippet": "1. 개요어떤 힘을 줘서 포물선 궤적으로 이동시키는 것 자체는 단순하다. 힘을 가하기만 하면 중력과의 상호작용에 의해 자연스럽게 포물선 운동을 한다. 하지만 내가 구현하고 싶은 것은 의도한 궤적으로 이동하는 포물선 운동이다. 단순히 힘을 주기만 하면 어느정도 높이로 올라가고 얼마나 멀리 가는지 알기 어렵다. 의도한 움직임을 구현할 수 있다면 더 정밀한 컨트롤이 가능해진다. 이를 한번 구현해보자.2. 고려 사항앞서 3차원이 아닌 2차원 상에서의 포물선 운동이라는 점을 미리 말하겠다. 2차원 포물선 운동은 고등학교를 나왔다면 충분히 이해할 수 있다.먼저 고려해야할 사항을 알아보자. 어떤 방식으로 힘을 줄 것인가? 무엇을 기준으로 포물선 운동의 궤적을 정할 것인가?위 사항에 대해 나는 아래와 같이 결정했다. Rigidbody2D.AddForce()의 mode 매개변수에 ForceMode2D.Impulse 값을 준다. 이는 힘을 1번만 줘서 속도 변화의 즉각적 반영을 위한 것이다. 포물선 운동을 가장 쉽게 특정할 수 있는 방법은 바로 최대 높이까지의 변위이다. 최대 높이의 위치가 결정된다면 그 포물선 운동은 유일하다. 그 이유는 2차함수를 생각하면 알 수 있다. 2차함수의 특징은 임의의 한 점과 꼭짓점이 있으면, 이 두 점을 지나는 유일한 2차함수를 결정할 수 있다.3. 포물선 운동 알고리즘포물선 운동의 특징포물선 운동은 다음과 같은 특징을 가진다. 포물선 운동의 $x$성분은 등속도 운동을 한다. 포물선 운동의 $y$성분은 등가속도 운동을 한다. 최대 높이에서 $y$성분의 속도는 0이다.위 특징은 굉장히 중요하다. 포물선 운동에서 각 성분은 독립적이기 때문에 $x$성분의 속도와 $y$성분의 속도를 따로 구할 것이다.충격량Rigidbody2D.AddForce()에 ForceMode2D.Impulse 모드로 힘을 가할 시 force 매개변수에는 충격량이 입력된다. 따라서 몇가지 물리 법칙을 활용하여 충격량을 구해야 한다.충격량과 운동량의 변화량은 같다충격량과 관련된 법칙이다. 충격량 $\\vec{I} = \\vec{F}t$이고 운동량 $\\vec{p} = m\\vec{v}$이며 이 때 아래와 같은 법칙을 얻을 수 있다.\\[\\vec{I} = \\Delta{\\vec{p}}\\]\\[\\vec{F}t = m\\Delta{\\vec{v}}\\]즉, 충격량을 구하기 위해서는 운동량의 변화량을 구하면 되며, 질량은 이미 알고 있는 정보이기 때문에 결국 속도 변화만 알면 된다. 참고로 충격량과 운동량은 이름에 양이 들어가있다고 해서 스칼라가 아니다. 엄연한 벡터라는 점을 유의해야 한다.알고리즘 구현하기메서드 선언위 2. 고려 사항에서 나는 최대 높이까지의 변위를 기준으로 포물선 운동을 구현하겠다고 했다. 즉, 우리는 매개변수로 최대 높이까지의 변위를 받을 것이다. 아래는 최대 높이까지의 변위이다.\\[\\vec{s} = [d, h]\\]아래는 위 정보를 바탕으로 선언한 메서드이다.private void JumpForce(Vector2 maxHeightDisplacement) { }최대 높이까지의 변위 정보를 Vector2 구조체 타입의 maxHeightDisplacement 매개변수에서 받는다.역학적 에너지 보존 법칙최대 높이까지의 변위는 이미 알고 있는 정보이다. 이 정보를 바탕으로 포물선 운동을 위한 각 성분의 처음 속도를 구할 수 있다. 중력계에서의 운동은 아래와 같은 법칙이 성립한다.\\[mgh = \\displaystyle\\frac{1}{2}mv^2\\]맞다. 그 유명한 역학적 에너지 보존 법칙이다. 위 식을 보면 알겠지만 최대 높이 $h$만 알면 지면에서의 속도 $v$를 알 수 있다.$y$성분의 처음 속도 $v_y$ 구하기위 역학적 에너지 보존 법칙 식을 활용하자. 최대 높이까지의 변위는 이미 알고 있는 정보이기 때문에, 위 식을 $v$에 대해 정리하면 $y$성분의 속도 $v_y$를 구할 수 있다.\\[v_y = \\pm\\sqrt{2gh}\\]우리가 구하려는 처음 속도 $v_y$는 $+\\sqrt{2gh}$이다. $-\\sqrt{2gh}$는 최대 높이에서 떨어졌을 때 지면에서의 속도이다.아래는 위 수식을 적용한 코드이다.Rigidbody2D rigid = this.rigid;// m*k*g*h = m*v^2/2 (단, k == gravityScale) &lt;= 역학적 에너지 보존 법칙 적용float v_y = Mathf.Sqrt(2 * rigid.gravityScale * -Physics2D.gravity.y * maxHeightDisplacement.y);위 식에서 중력가속도 $g$는 실제로 작용하는 중력가속도의 크기이기 때문에 Rigidbody2D.gravityScale 정보를 반영했다. 각 오브젝트마다 gravityScale이 다를 수 있기 때문이다. 따라서 중력가속도 $g$는 rigid.gravityScale * -Physics2D.gravity.y이다.$x$성분의 속도 $v_x$ 구하기$v_x$를 구하기 위해서는 속도에 대한 이해가 필요하다. 아래는 속도의 정의이다.\\[\\vec{v}_{avg} = \\displaystyle\\frac{\\vec{s}}{t}\\]속도란 단위 시간당 이동거리이다. 중요한 것은 그냥 속도가 아닌 평균 속도이다. $x$성분은 등속도 운동이기 때문에 처음 속도가 곧 평균 속도이지만, $y$성분은 등가속도 운동이기 때문에 평균 속도를 구하는 과정이 필요하다.그렇다면 왜 $y$성분의 평균 속도를 구하는가? 그 이유는 시간 $t$에 있다. 앞서 말했지만 우리는 이미 변위 정보를 알고 있다. 최대 높이까지 도달하는데 걸린 시간 $t$만 알 수 있다면 위 속도의 정의를 활용해 $v_x$를 구할 수 있다. 시간 $t$를 구하기 위해 우리는 동일한 시간 $t$동안 $x$성분은 등속도 운동으로, $y$성분은 등가속도 운동으로 최대 높이의 위치에 도달한다는 성질을 활용할 것이다. 아래 그림을 보자.Science Ready위 그림에서 각 포물선 위의 벡터의 $x$성분 크기는 유지되고, $y$성분은 최대 높이에서 크기가 0이 된다.등가속도 운동의 특징 중 하나는 평균 속도가 처음 위치에서의 속도와 나중 위치에서의 속도의 평균이다. 이는 등가속도 운동에서 속도 그래프가 직선이기 때문이다. 이 사실을 활용해 $y$성분의 평균 속도를 구해보자.처음 속도는 $v_y$이고 최대 높이에서의 속도는 $0$이다. 즉, 처음 위치에서 최대 높이까지 도달하는데 필요한 평균 속도는 $\\displaystyle\\frac{v_y}{2}$이다.\\[v_{avg} = \\displaystyle\\frac{v_y + 0}{2} = \\displaystyle\\frac{v_y}{2}\\]평균 속도를 구했으니 이제 속도의 정의를 활용해 최대 높이까지 도달하는데 걸린 시간 $t$를 구할 수 있다.\\[t = \\displaystyle\\frac{s}{v_{avg}} = \\displaystyle\\frac{2h}{v_y}\\]$y$성분에 대한 1차원 운동이기 때문에 내적을 쓸 필요 없이 나누기만 하면 된다.이제 드디어 $x$성분의 속도를 구할 수 있다. $x$성분은 등속도 운동이기 때문에 처음속도가 곧 평균속도이다. 최대 높이까지의 $x$성분의 변위는 $d$이다.\\[s = d\\]\\[v_x = \\displaystyle\\frac{s}{t} = \\displaystyle\\frac{d}{2h}v_y\\]아래는 위 식을 적용한 코드이다.// 포물선 운동 법칙 적용float v_x = maxHeightDisplacement.x * v_y / (2 * maxHeightDisplacement.y);충격량을 구해 힘을 가하기앞의 충격량과 운동량의 변화량은 같다에서 알아봤듯이 아래 법칙을 활용하여 충격량을 구할 것이다.\\[\\vec{F}t = m\\Delta{\\vec{v}}\\]$\\Delta{\\vec{v}}$는 변화시키고자 하는 속도를 위해 필요한 속도의 변화량으로 나중속도 - 처음속도이다. 처음 속도 $\\vec{v_0}$는 원래 오브젝트가 가지고 있던 속도, 나중 속도는 우리가 직전에 구한 $\\vec{v_1} = [v_x, v_y]$이다. 원래 속도를 가지고 있던 오브젝트에 힘을 줘 속도 $v$를 $\\vec{v_1} = [v_x, v_y]$로 변화시켜야 우리가 원하는 포물선 운동이 구현된다. 속도의 변화량을 구하는 것은 간단하다.\\[\\Delta{\\vec{v}} = \\vec{v_1} - \\vec{v_0}\\]위 정보를 종합하여 충격량을 구했다. 아래는 충격량을 구해 Rigidbody2D에 힘을 가하는 코드이다.Vector2 force = rigid.mass * (new Vector2(v_x, v_y) - rigid.velocity);rigid.AddForce(force, ForceMode2D.Impulse);완성된 알고리즘private void JumpForce(Vector2 maxHeightDisplacement){ Rigidbody2D rigid = this.rigid; // m*k*g*h = m*v^2/2 (단, k == gravityScale) &lt;= 역학적 에너지 보존 법칙 적용 float v_y = Mathf.Sqrt(2 * rigid.gravityScale * -Physics2D.gravity.y * maxHeightDisplacement.y); // 포물선 운동 법칙 적용 float v_x = maxHeightDisplacement.x * v_y / (2 * maxHeightDisplacement.y); Vector2 force = rigid.mass * (new Vector2(v_x, v_y) - rigid.velocity); rigid.AddForce(force, ForceMode2D.Impulse);}4. 활용코드위 알고리즘을 활용해보겠다. 먼저 아래는 위 알고리즘을 활용한 간단한 코드이다.위 코드에서 최대 높이인 maxHeightDisplacement를 필드로 선언했다. InterpolatedFunction은 대리자 타입으로 보간된 함수를 등록할 수 있다.f = PhysicsUtility.NewtonPolynomial(Vector2.zero, maxHeightDisplacement, new Vector2(2 * maxHeightDisplacement.x, 0f)); // 2차함수 보간Awake()에서 사용한 위 PhysicsUtility.NewtonPolynomial() 메서드는 매개변수로 입력한 점들을 지나는 유일한 다항함수를 반환하는 메서드이다. 이와 관련된 내용은 뉴턴 다항식 보간법에서 확인할 수 있다.private void DrawProjectileMotionLine(){ float interval = 2 * maxHeightDisplacement.x / pointCount; Vector2 current = Vector2.zero; for (int i = 0; i &lt; pointCount; i++) { float next_x = current.x + interval; Vector2 next = new Vector2(next_x, f(next_x)); Debug.DrawLine(current + rigid.position, next + rigid.position, Color.green, 3f); current = next; }}위 메서드는 오브젝트가 움직이는 포물선 궤적을 Scene창에 그리기 위해 구현했다.동작 장면인스펙터인스펙터 설정 화면이다.Max Height Displacement는 $(3, 3)$으로 지정했다.아래는 각각 Gravity Scale 값에따른 동작 장면이다.Gravity Scale: 1정지 상태에서의 포물선 궤적으로의 점프이다.Gravity Scale: 2Gravity Scale: 4앞서 중력가속도 $g$를 구할 때 Rigidbody2D.gravityScale 정보를 반영했기 때문에 Gravity Scale 값에 관계 없이 의도한 포물선 궤적으로 이동하는 모습을 확인할 수 있다.운동상태에서 다시 점프Gravity Scale 값은 2.5이며 포물선 궤적으로 운동 중에 다시 한번 포물선 궤적의 점프를 시도했다. 원하는 궤적으로 정확히 운동하는 모습을 확인할 수 있다.위 충격량을 구해 힘을 가하기에서 속도 변화인 $\\Delta{\\vec{v}}$를 구한 이유가 바로 위와 같이 정지된 상태가 아닌 운동 상태에서 원하는 궤적으로 움직이기 위해서이다." }, { "title": "[Unity] 유니티 첫 게임 - 닌자", "url": "/unity/unity-first-game-ninja/", "categories": "Unity", "tags": "Unity, Game Development, Project, Enemy", "date": "2021-12-16 00:00:00 +0900", "snippet": "1. 개요Unity로 개발한 첫 게임에서 구현한 닌자 몬스터를 소개한다.사용한 에셋은 Ninja Sprite Sheet이며 유니티 에셋 스토어에서 무료로 구할 수 있다.참고로 스킬, 이동, 체력 관리는 모두 직접 코딩해야 한다. 에셋에는 스프라이트 관련 파일만 존재한다.구현된 닌자 몬스터의 특징은 다음과 같다. 플레이어가 도망칠 수 없을 정도의 굉장히 빠른 이동속력 높은 점프력(추후 업데이트 함) 굉장히 짧은 주기로 스킬 사용 강한 공격력과 약한 방어력 사망 시 시간 경과에 따른 투명 처리(에셋에 사망 애니메이션이 존재하지 않음)2. 움직임이동플레이어가 감지 되었을 때 장애물을 피해 플레이어의 위치를 추적하여 이동하는 처리는 A* Pathfinding Project 유니티 에셋을 활용했다. A* Pathfinding Project 에셋에서 제공하는 API를 활용해 움직이는 처리는 직접 코딩했다. 기본적인 아이디어와 활용법은 2D PATHFINDING - Enemy AI in Unity에서 얻었다.아래는 닌자의 이동 장면이다.위 장면을 보면 알 수 있듯이 상당히 빠른 속력으로 플레이어에게 접근한다. 플레이어는 닌자를 죽이지 않으면 사실상 벗어날 수 없다.이 때 원으로 된 파란색 영역과 빨간색 영역이 있는데, 파란색 영역은 몬스터의 이동속력이 느려지는 영역이며 빨간색 영역은 정지하는 영역이다. 참고로 이동은 등속이 아닌 가속 이동으로 구현했다.3. 점프플레이어가 감지 되었을 때 이동 과정 중에 장애물이 있거나 플레이어가 높이 점프하는 등 점프를 해야만 플레이어에게 도달할 수 있을 때 점프를 시도한다.점프를 위한 전처리점프를 무작정 해서는 안되기 때문에 제약조건을 주었다. 아래는 점프를 하기 위한 조건을 체크하는 과정이다.점프 필터링점프는 여러 필터링을 거쳐 실행이 되기 떄문에 다소 연산이 비싸다. 아래는 사용된 필터링 중 일부이다. 지면에 닿아있어야 한다. 타겟까지의 변위와 다음 경로까지의 변위의 $x$성분의 방향이 같아야 한다. 전방에 장애물이 없고 걸어갈 시 밟을 수 있는 지면이 없을 떄 다음 경로가 점프를 위한 최소 높이보다 낮게 위치하면 점프를 실행하지 않는다. 플랫폼 사이를 점프 시 천장과 충돌 위험이 있을 경우 점프를 실행하지 않는다.포물선 구성위 필터링이 끝나면 점프를 위한 포물선 궤적을 그려 포물선 궤적 사이에 장애물 존재 여부와 장애물 특성을 검사해 점프 가능 여부를 조사한다. 아래는 포물선의 구성 요소이다. 포물선의 시작점은 객체의 position이다. 포물선의 꼭짓점의 $x$성분은 다음 경로까지의 변위의 $x$성분이고, $y$성분은 점프 시 최대 높이이다. 끝점은 시작점과 포물선의 꼭짓점까지의 $x$성분의 거리의 2.5배 떨어진 포물선 위의 점이다.포물선 궤적의 레이캐스트위에서 구성한 포물선 궤적의 레이캐스트를 발사한다. 각 점에서의 레이캐스트의 방향은 포물선 위의 점에서의 접선 벡터 방향이다. 연산을 줄이기 위해 점을 최대한 적게 사용했다. 아래는 포물선 궤적의 레이캐스트를 발사한 방법이다. 포물선 위의 임의의 점에서 접선벡터 방향으로 레이캐스트를 쏘기 위해 포물선을 접선을 이용해 근사화했다. 각 점에서 접선을 그릴 시 그 접선은 인접한 점에서 그린 접선과 교차한다. 즉, 포물선은 실제 포물선 위의 점과 각 접선의 교점들로 구성된다. 점과 점사이의 레이캐스트를 사용하기 위해 Physics2D.Linecast() 메서드를 사용했다. 레이캐스트를 쏘면서 장애물이 감지 될 시 충돌한 장애물의 법선 벡터를 확인한다. 착지해도 무리가 없는 법선벡터를 가진 장애물이라면 점프를 시도 한다. 나는 Vector2.up과 법선벡터가 이루는 각이 45도 이하일 때 점프를 실행하도록 했다.점프 실행위 전처리를 거치면 실제로 점프를 하기 위해 힘을 가한다. 힘을 가하는 원리와 알고리즘은 포물선 운동(점프) 구현에서 자세히 확인할 수 있다.점프 장면위 알고리즘을 적용한 점프 장면은 아래와 같다.위 장면에서 레이캐스트를 포물선 형태로 쏘는 모습을 빨간색으로 나타냈다. 포물선 궤적으로 점프 하는 모습을 볼 수 있다. 몰론 점프 중에도 닌자는 플레이어를 따라서 $x$방향으로 이동 중이기 때문에 실제 궤적과는 약간 다른 움직임을 가진다.4. 스킬스킬은 총 3개이며 모두 원거리 공격이다. 에셋에 근접 공격과 관련한 여러 좋은 애니메이션들이 있는데 구현하기 귀찮아서 원거리 공격만 구현했다. 다만 닌자 특성 상 빠르게 플레이어에게 접근하기 때문에 시간적인 여유가 있다면 근접 공격도 구현해볼 생각이다. 추후 닌자 스킬이 추가된다면 이 포스트를 업데이트 하겠다.나는 스킬 시스템을 구축할 때 적의 모든 공격을 스킬로 취급했다. 즉 기본 공격, 근접 공격, 원거리 무기 공격, 마법 스킬 등의 공격 종류에 관계 없이 모두 스킬이다.기본 공격기본 공격은 표창 1개를 플레이어를 향해 날린다. 다만 표창은 에셋 내에 포함되어 있지 않기 때문에 직접 다른 곳에서 구해야 한다. 나는 구글링 하다가 한 사이트에서 무료 라이센스이면서 상업적 용도로도 사용 가능한 표창 이미지를 구해 이용했다. 어디서 구했는지는 잊어버려서 링크 공유는 어려울 듯 싶다. 추후 발견하면 포스트를 업데이트하겠다.이 스킬을 사용할 때 다음과 같은 특징을 고려해야 한다. 플레이어의 위치를 추적할 수 있어야 한다. 표창을 생성하고 플레이어의 위치를 향해 발사해야 한다. 표창 충돌 시 플레이어에게 데미지를 가해야 한다.위 특징에 따라 표창을 날리는 스킬 발동은 Ninja 오브젝트에서 담당하고 데미지를 가하는 처리는 포창 오브젝트에서 담당했다.아래는 기본 공격 스킬 사용 장면이다.3표창이 스킬은 기본 공격을 발전시킨 원거리 공격이다. 표창 3개를 동시에 날리는 스킬이다. 특징은 아래와 같다. 기본 공격보다 사거리가 짧고 데미지가 약간 낮다. 발사 시 플레이어를 향해 3개의 표창이 동일한 방향으로 평행하게 발사된다. 표창 생성 시 발사 방향에 수직인 방향으로 생성된다.아래는 3표창 스킬 사용 장면이다.연속 표창이 스킬은 일정 시간 동안 기를 충전한 후 표창 5개를 순차적으로 발사하는 하이 리스크 하이 리턴형 스킬이다. 특징은 아래와 같다. 기충전 시 어떤 동작도 할 수 없으며 기충전 애니메이션이 재생된다. 기충전 동안 표창을 일정 시간 간격으로 생성한다. 생성 위치는 닌자가 바라보는 방향을 기준으로 바로 앞 3개, 생성된 표창 3개의 앞에 다시 2개를 생성해 총 5개를 생성한다. 표창 5개를 일정 시간 간격으로 플레이어를 향해 날린다. 상당히 데미지가 강하고, 유효 사거리가 길다.아래는 연속 표창 스킬 사용 장면이다.5. 전투닌자 하나로도 상당히 강력하지만 닌자가 여러 명일 경우 파괴력이 어마무시하다. 닌자의 특징은 1. 개요에서 언급했듯이 이동속력이 굉장히 빠르고, 스킬 사용 텀이 상당히 짧다. 닌자는 플레이어를 향해 빠르게 접근해 여러 스킬을 지속적으로 사용하며 위력 또한 상당히 높다. 다만 그만큼 방어력은 낮다. 하지만 그럼에도 밸런스를 깨기에는 충분하기에 실제 게임 필드 내에는 적게 존재하는 것이 적절하다.아래는 플레이어가 3명의 닌자 몬스터와 전투하는 장면이다." }, { "title": "[Unity] 유니티 첫 게임 - 미니 보스", "url": "/unity/unity-first-game-mini-boss/", "categories": "Unity", "tags": "Unity, Game Development, Project, Enemy", "date": "2021-12-15 00:00:00 +0900", "snippet": "1. 개요Unity로 개발한 첫 게임에서 구현한 미니보스를 소개한다.사용한 에셋은 Bringer Of Death이다. 유니티 에셋스토어에서 무료로 구할 수 있다. 참고로 스킬, 이동, 체력 관리는 모두 직접 코딩해야 한다. 에셋에는 스프라이트 관련 파일만 존재한다.2. 스킬스킬 구성은 크게 3개로 나눴다. 더 구현하고 싶은 욕심이 있지만 미니 보스라는 점과 시간적인 문제 때문에 3개만 구현했다.기본 공격플레이어가 미니 보스의 일정 범위 안에 들어왔을 때 기본 공격을 실행한다. 나는 일정 범위를 유효 사거리로 지정했다.기본 공격과 관련해서 몇가지 처리가 필요하다. 기본 공격 애니메이션에는 스프라이트의 움직임만 적용되어있다. 무기에 콜라이더가 필요하다. 기본 공격 실행 전에는 무기의 콜라이더는 Off 상태여야 한다. 기본 공격 실행 시 무기의 콜라이더가 On 상태여야 하며 콜라이더는 무기 스프라이트의 움직임과 회전방향을 따라가야 한다.위 처리를 코드로 처리하기에는 다소 복잡하고 귀찮다. 따라서 나는 기본 공격 애니메이션에서 전부 처리했다.아래는 기본 공격 애니메이션의 구성과 콜라이더가 어떻게 동작하는지를 보여준다. Default 속도 1/3 배속 뒤치기스킬 이름을 뭘로 할까 고민했는데 뒤치기가 가장 직관적인거 같아서 조금 없어보이지만 일단 이렇게 네이밍했다.이 스킬은 플레이어의 뒤로 텔레포트 후 플레이어에게 기본 공격을 사용한다. 이 때 몇가지 처리가 필요하다. 뒤치기 스킬 사용 시 플레이어의 뒤로 텔레포트 한다. 이 때 즉시 텔레포트 하는 것이 아니라 지연시간을 가지며 미니 보스의 은신 모션을 실행 한다. 이 에셋에 은신과 관련된 직접적인 애니메이션이 없어 사망 애니메이션을 재구성했다. 마찬가지로 텔레포트 후 즉시 공격하는 것이 아니라 은신이 완전이 풀릴 때 까지의 지연 시간을 가진다. 은신이 풀릴 때는 은신 상태에 돌입할 때보다 빠르게 풀리도록 했다. 지연 시간이 너무 길면 플레이어가 쉽게 도망갈 수 있기 때문이다. 지연 시간 이후 공격을 무조건 실행한다. 나는 기본 공격을 실행하는 처리를 넣었다.아래는 뒤치기 스킬 사용 장면이다.Spell Cast역시 스킬 네이밍은 어렵다. 그냥 에셋에서 제공하는 스킬 애니메이션에서 이름을 따왔다.이 스킬은 플레이어의 근처 위치에 위에서 아래로 향하는 공격을 발사한다. 위에서 아래로 향하는 각 공격을 편의상 어둠 하강이라고 부르겠다. 이 스킬 자체에 다양성을 주기 위해 몇가지 처리를 했다. 어둠 하강의 개수를 정적으로 생성하는 것이 아닌 동적으로 랜덤하게 생성한다. 나는 1 ~ 5개 사이의 어둠 하강을 생성하도록 했다. 어둠 하강의 개수가 많아질 수록 피하기 어려우므로 데미지를 약화시킨다. 어둠 하강의 생성 방향 역시 랜덤하게 선택한다. 맵을 기준으로 왼쪽 -&gt; 오른쪽, 오른쪽 -&gt; 왼쪽 방향을 랜덤하게 선택한다.아래는 Spell Cast 스킬 사용 장면이다.3. 전투실제 전투 모습보스가 사용하는 스킬에 집중하기 위해 공격은 거의 하지 않고 일부로 피격 당했다. 각 스킬 하나 하나도 강력하지만 모였을 때는 상당히 강력해진다. 보스의 움직임과 스킬 사용을 예측하고 정밀한 움직임을 해야 피할 수 있다.전투 및 클리어아래는 클리어 장면이다. 무기는 샷건을 사용했으며 클리어 장면을 보여주기 위해 다소 난이도를 낮춰서 진행했다.클리어 시 아이템을 드랍하는데 특수무기인 미니건을 획득할 수 있다." }, { "title": "[수치해석] 뉴턴 다항식 보간법", "url": "/numerical-analysis/newton-polynomial/", "categories": "Numerical Analysis", "tags": "Interpolation, Algorithm, Unity, C#, Python", "date": "2021-12-11 00:00:00 +0900", "snippet": "Introduction당연하지만 프로그래밍에서 보간법(Interpolation)은 굉장히 중요한 영역이다.주어진 데이터는 한정적이지만 우리가 예측해야하는 범위는 너무 넓다.그래서 필요한 것이 보간법 (Interpolation)이다. 예를 들면 다음과 같은 점의 위치를 우리가 이미 알고 있거나 주어졌다고 하자.\\[(-1, 0), (0, 0), (1, 0), (2, 6)\\]위 점을 지나는 함수는 아래와 같다.\\[y = x(x - 1)(x + 1)\\]위 함수의 그래프:위 그림에서 빨간색 점이 우리가 이미 알고 있는 점이고, 위 점을 지나는 유일한 다항함수의 모습은 위와 같은 형태이다.위 그림의 예처럼 어떠한 데이터가 주어져있을 때 이 데이터들로 부드러운 곡선을 그려 데이터들의 사이값을 예측하고 싶다. 즉, 각 점을 지나는 비선형 함수를 구하고 싶다. 어떤 점들을 지나는 함수를 알 수 있다면 다양하게 활용할 수 있다. 데이터들을 비선형 함수로 보간하는 방법은 여러 개가 있겠지만 여기서는 가장 기초적인 다항식 보간에 대해 설명하려고 한다.다항식 보간에는 여러가지가 있다. 가장 간단한 방법으로는 주어진 데이터를 가지고 $p(x) = a_0 + a_1x + a_2x^2 + \\cdots$ 형태의 방정식을 푸는 게 가장 쉬운 방식이라고 할 수 있다. 행렬을 활용하면 프로그래밍으로 쉽게 각 항의 계수를 구할 수 있다. 이 방법을 Monomial basis 다항식 보간법이라고 한다. 하지만 이 방식에는 몇가지 문제점이 존재하기 때문에 (대표적으로 조건 상수 값이 너무 큼) 다른 다항식 보간법인 뉴턴 다항식 보간법(Newton Polynomial Interpolation)을 소개하려고 한다.Definition먼저 미리 말하지만 아래 정의만 보면 무슨 소리인지 이해하기 어려울 것이다. 당황해서 뒤로가기 누르지 말고 가볍게 보기를 권장한다. $n + 1$개의 Point에 대한 데이터 집합이 아래와 같이 주어져 있다고 하자.\\[(x_0, y_0), (x_1, y_1), \\cdots, (x_n, y_n)\\]위 $n + 1$개의 Point 중 $x$ 좌표가 모두 다르다면 뉴턴 다항식은 다음과 같이 정의될 수 있다.\\[N(x) = \\displaystyle\\sum_{i = 0}^{n}a_i p_i(x)\\]$p_i(x)$는 Newton basis polynomials (뉴턴 기반 다항식)이며 아래와 같이 정의된다.\\[p_i(x) = \\displaystyle\\prod_{k = 0}^{i - 1}(x - x_k)\\]다항식의 형식이 변수 $x$에서 위 Point의 $x$좌표를 뺀 항들끼리 곱해지는 형태이다.이 때 주의깊게 볼 점은 $i$번 째 뉴턴 기반 다항식에 대해서 $i - 1$번째까지의 $x$좌표를 참조한다. 이를 유의해야 한다.각 항의 계수 $a_i$는 Divided differences(분할 차분)이며 아래와 같이 정의된다.\\[f[x_k] = f(x_k)\\]\\[f[x_k, x_{k + 1}, \\cdots, x_l] = \\displaystyle\\frac{f[x_{k + 1}, \\cdots, x_l] - f[x_k, \\cdots, x_{l - 1}]}{x_l - x_k}\\]\\[f[x_k, \\cdots, x_l] = f[x_l, \\cdots, x_k]\\]\\[a_i = f[x_0, x_1, \\cdots, x_i]\\]위 정의를 종합하면 뉴턴 다항식을 조금 쉽게 풀어 쓸 수 있다.\\[\\begin{aligned}N(x) &amp;= a_0 + a_1(x - x_0) + a_2(x - x_0)(x - x_1) + \\cdots + a_n(x - x_0)(x - x_1)\\cdots(x - x_{n - 1}) \\\\&amp;= f[x_0] + f[x_0, x_1] (x - x_0) + f[x_0, x_1, x_2] (x - x_0)(x - x_1)+ \\cdots \\\\&amp;+ f[x_0, x_1, \\cdots, x_n] (x - x_0)(x - x_1)\\cdots(x - x_{n - 1})\\end{aligned}\\]How to calculate Divided Differences앞의 Definition에서는 $n + 1$개의 Point가 있다고 가정했지만 지금부터는 $n$개의 데이터가 있으며 모든 인덱스는 0부터 시작한다고 가정할 것이다. 이는 코딩의 편의성을 위해서이다. 즉, 주어진 Point는 아래와 같다고 가정한다.\\[(x_0, y_0), (x_1, y_1), \\cdots, (x_{n - 1}, y_{n - 1})\\]분할차분 정의 해석분할차분의 정의 중 아래 식에서 한가지 특징을 알 수 있다.\\[f[x_k, x_{k + 1}, \\cdots, x_l] = \\displaystyle\\frac{f[x_{k + 1}, \\cdots, x_l] - f[x_k, \\cdots, x_{l - 1}]}{x_l - x_k}\\]$l &gt; k$라고 가정 할 때 좌측 항의 분할차분의 경우 $x_k, \\cdots, x_l$까지 총 $l - k + 1$개의 $x$를 참조하지만 (설명의 편의를 위해 참조란 용어를 사용했지만 실제로 각 $x_i$의 값을 직접 참조하는 것은 아니다), 우측 항에서는 각 2개의 분할차분 모두 1개가 줄어든 $l - k$개의 $x$를 참조한다. 즉, 식을 전개할 수록 $x$를 참조하는 일종의 스케일이 줄어드는 재귀적 관계이다. 다만 아직까지 감이 안잡힐 수도 있으니 간단한 예를 들어보겠다.\\[f[x_0, x_1, x_2] = \\displaystyle\\frac{f[x_1, x_2] - f[x_0, x_1]}{x_2 - x_0}\\]위 식에서 $k = 0, l = 2$이다. 좌측항은 $x_0, x_1, x_2$를 참조하지만 우측항의 분할차분은 각각 $x_0, x_1$와 $x_1, x_2$를 참조한다. 위 식의 우측항에 있는 각각의 분할차분 식을 다시 아래에 전개해보겠다.\\[f[x_1, x_2] = \\displaystyle\\frac{f[x_2] - f[x_1]}{x_2 - x_1}\\]\\[f[x_0, x_1] = \\displaystyle\\frac{f[x_1] - f[x_0]}{x_1 - x_0}\\]위 식에서 또다시 스케일이 줄어들었다. $f[x_k] = f(x_0)$로 분할차분이 최소단위가 됬음을 확인할 수 있다.분할차분 계산이제 본격적으로 위 예를 가지고 분할차분 값을 계산해보겠다. 아래 분할차분의 정의를 활용하는 것으로부터 분할차분 계산을 시작할 수 있다.\\[f[x_k] = f(x_k)\\]위 정의를 활용하면 각각의 분할차분을 계산할 수 있다.\\[f[x_1, x_2] = \\displaystyle\\frac{f[x_2] - f[x_1]}{x_2 - x_1} = \\displaystyle\\frac{f(x_2) - f(x_1)}{x_2 - x_1}\\]\\[f[x_0, x_1] = \\displaystyle\\frac{f[x_1] - f[x_0]}{x_1 - x_0} = \\displaystyle\\frac{f(x_1) - f(x_0)}{x_1 - x_0}\\]우리는 $x$좌표와 그에 대응하는 함수값 $f(x)$또는 $y$좌표를 이미 알고 있다. 따라서 위 분할차분을 계산할 수 있다. 참고로 위 분할차분의 특징은 함수의 평균변화율 혹은 두 점 사이의 직선의 기울기이다. 필요한 2개의 분할차분 값을 구했으니 이를 활용해 다음 분할차분값을 드디어 구할 수 있다.\\[f[x_0, x_1, x_2] = \\displaystyle\\frac{f[x_1, x_2] - f[x_0, x_1]}{x_2 - x_0}\\]위 식에서 $f[x_1, x_2]$와 $f[x_0, x_1]$는 이미 구해서 알고 있고 $x$좌표 역시 원래 알고 있던 값이므로 $f[x_0, x_1, x_2]$ 값을 구하는 거는 단순 계산일 뿐이다.우리는 지금까지 분할차분을 구하는 방법을 알아보았다. 하지만 위 방식만 가지고는 구하기도 다소 어렵고, 코딩에 적용하기도 쉽지 않다. 따라서 우리는 분할차분표라는 개념을 도입해 문제를 쉽게 해결해보려 한다.Divided Differences TableDivided Differences Table은 분할차분표로 위에서 소개한 bottom-up 방식의 계산을 table 형태의 자료구조를 통해 쉽게 계산할 수 있도록 해주는 방법이다. 표의 형태는 아래와 같다.분할차분표: $x_i$ $f[x_i]$ $f[x_i, x_{i + 1}]$ $f[x_i, x_{i + 1}, x_{i + 2}]$ $f[x_i, x_{i + 1}, x_{i + 2}]$ $\\cdots$ $f[x_i, \\cdots, x_{i + (n - 1)}]$ $x_0$ $f(x_0)$ $f[x_0, x_1]$ $f[x_0, x_1, x_2]$ $f[x_0, x_1, x_2, x_3]$ $\\cdots$ $f[x_0, x_1, \\cdots, x_{n - 1}]$ $x_1$ $f(x_1)$ $f[x_1, x_2]$ $f[x_1, x_2, x_3]$ $\\vdots$     $x_2$ $f(x_2)$ $f[x_2, x_3]$ $\\vdots$       $x_3$ $f(x_3)$ $\\vdots$         $\\vdots$ $\\vdots$           $x_{n - 1}$ $f(x_{n - 1})$           $n$개의 Point(점)가 있고, 분할차분표의 행 인덱스는 $i$, 열 인덱스는 $j$이며 0부터 시작한다. 위 분할차분표의 특징은 아래와 같다. 분할차분 값에 해당되는 부분은 $n \\times n$ table로 구성된다. 각 열의 원소 개수는 $n$개에서 $j$개씩 감소한다. 즉, $n - j$개이다.분할차분표를 이용한 계산분할차분표를 활용해 분할차분을 쉽게 계산하는 방법을 소개한다. 분할차분 계산에서 들었던 예시를 분할차분표를 이용해 구해보겠다. 또한 이해를 돕기 위해 표에 행과 열에 대한 인덱스를 추가했다.가장 먼저 우리가 이미 알고 있는 값인 Point의 $x$좌표와 $y$좌표를 표에 넣는다.   Column 0 1 2 Row $x_i$ $f[x_i]$ $f[x_i, x_{i + 1}]$ $f[x_i, x_{i + 1}, x_{i + 2}]$ 0 $x_0$ $f(x_0)$     1 $x_1$ $f(x_1)$     2 $x_2$ $f(x_2)$     분할차분 계산에서 이미 봤듯이 분할차분 값을 계산하는 핵심은 이전의 분할차분 값을 이용하는 거다. 이 때 현재 열의 분할차분 값을 구하기 위해서는 직전 열의 바로 옆에 있는 값과 그 아래 값을 이용한다. 즉, $i$행 $j$열의 분할차분 값을 구하기 위해서 $i$행 $j - 1$열의 값과 $i + 1$행 $j - 1$열의 분할차분 값을 참조한다. 아래 표를 보면 조금 더 쉽게 이해할 수 있다. 참조해야하는 분할차분 값에 화살표를 그렸다.   Column 0 1 2 Row $x_i$ $f[x_i]$ $f[x_i, x_{i + 1}]$ $f[x_i, x_{i + 1}, x_{i + 2}]$ 0 $x_0$ $f(x_0)$ $\\rightarrow$ $f[x_0, x_1] = \\displaystyle\\frac{f[x_1] - f[x_0]}{x_1 - x_0}$   1 $x_1$ $f(x_1)$ $\\nearrow$     2 $x_2$ $f(x_2)$       Column 0 1 2 Row $x_i$ $f[x_i]$ $f[x_i, x_{i + 1}]$ $f[x_i, x_{i + 1}, x_{i + 2}]$ 0 $x_0$ $f(x_0)$ $f[x_0, x_1] = \\displaystyle\\frac{f[x_2] - f[x_0]}{x_1 - x_0}$   1 $x_1$ $f(x_1)$ $\\rightarrow$ $f[x_1, x_2] = \\displaystyle\\frac{f[x_2] - f[x_1]}{x_2 - x_1}$   2 $x_2$ $f(x_2)$ $\\nearrow$     이제 위에서 구한 두 분할차분 값 $f[x_0, x_1]$과 $f[x_1, x_2]$를 가지고 $f[x_0, x_1, x_2]$를 구하면 된다. 또한 분모의 경우 $f[x_0, x_1, x_2]$에서 맨 왼쪽의 $x_0$와 맨 오른쪽의 $x_2$의 값의 차인 $x_2 - x_0$로 구할 수 있다. 분모가 왜 이렇게 구해지는지 기억이 나지 않을 경우 분할차분 정의 해석을 보고 오기 바란다.   Column 0 1 2 Row $x_i$ $f[x_i]$ $f[x_i, x_{i + 1}]$ $f[x_i, x_{i + 1}, x_{i + 2}]$ 0 $x_0$ $f(x_0)$ $f[x_0, x_1]$ $\\rightarrow$ $f[x_0, x_1, x_2] = \\displaystyle\\frac{f[x_1, x_2] - f[x_0, x_1]}{x_2 - x_0}$ 1 $x_1$ $f(x_1)$ $f[x_1, x_2]$ $\\nearrow$   2 $x_2$ $f(x_2)$     이제 우리는 뉴턴 다항식 보간을 위해 필요한 모든 분할차분 값을 구할 수 있게 되었다.분할차분표를 활용해 다항식 보간하기뉴턴 다항식은 앞에서 봤지만 다시 Remind하자. 앞의 정의에서는 $n + 1$개의 Point였지만 지금은 $n$개의 Point이기 때문에 개수에 맞춰서 식을 다시 작성했다.\\[\\begin{aligned}N(x) &amp;= f[x_0] + f[x_0, x_1] (x - x_0) + f[x_0, x_1, x_2] (x - x_0)(x - x_1)+ \\cdots \\\\&amp;+ f[x_0, x_1, \\cdots, x_{n - 1}] (x - x_0)(x - x_1)\\cdots(x - x_{n - 2})\\end{aligned}\\]뉴턴 다항식에서 $k$가 $0 \\leq k \\leq n - 1$ 조건을 만족할 때, $k$번째 항의 계수는 $f[x_0, \\cdots, x_k]$이며 모든 항의 계수인 분할차분은 $x_0$부터 시작한다. 분할 차분표에서 $i$행 $j$열의 분할차분은 $f[x_i, \\cdots, x_{i + j}]$이다. 즉, 뉴턴 다항식의 $k$번째 항의 계수는 0행 $k$열의 분할차분 값이다.아래는 앞서 구한 분할차분표 예시이다.   Column 0 1 2 Row $x_i$ $f[x_i]$ $f[x_i, x_{i + 1}]$ $f[x_i, x_{i + 1}, x_{i + 2}]$ 0 $x_0$ 0번째 항의 계수: $f(x_0)$ 1번째 항의 계수: $f[x_0, x_1]$ 2번째 항의 계수: $f[x_0, x_1, x_2]$ 1 $x_1$ $f(x_1)$ $f[x_1, x_2]$   2 $x_2$ $f(x_2)$     위 분할차분표를 가지고 뉴턴 다항식을 보간하면 아래와 같다.\\[N(x) = f(x_0) + f[x_0, x_1] (x - x_0) + f[x_0, x_1, x_2] (x - x_0)(x - x_1)\\]임의의 점 3개가 주어졌을 때 분할차분표를 활용하면 위와 같이 2차함수로 보간할 수 있다.거창하게 뉴턴 다항식, 분할차분과 같은 어려운 개념을 봤지만, 결국 우리가 구한 식을 보면 단순한 다항식이다. 나름 굉장히 복잡한 원리를 통해서 다항식을 도출해지만 사실 다항식을 구하는 방법은 Introduction에서 말했듯이 단순하게 $y = a_0 + a_1x + a_2x^2 + \\cdots$ 형태의 방정식에 지나는 점을 대입해 각 항의 계수를 구하는 것이 훨씬 쉽고 간편하다. 하지만 이 방법은 코딩 시 가우스 소거법과 같은 행렬 연산을 통해 방정식을 풀어야 하는데, 데이터의 개수가 많아질 경우 오차에도 취약해지고 속도가 굉장히 느려진다. 따라서 더 나은 대안인 뉴턴 다항식 보간을 활용하는 것이다.Algorithm지금까지 분할차분의 개념, 분할차분표를 통한 분할차분의 계산, 뉴턴 다항식 보간 등을 알아보았다. 이제 뉴턴 다항식 보간에 대한 알고리즘을 소개하고 이를 코딩해보려 한다. 알고리즘은 아래와 같다. $\\text{Algorithm: Newton polynomial interpolation}$ \\(\\begin{align*}&amp; \\textstyle \\text{Input: a point set } P = (x_0, y_0), \\cdots, (x_{n-1}, y_{n-1}) \\text{ of } n \\text{ points, target } x \\\\&amp; \\textstyle \\text{Initialize a } n \\times n \\text{ table } T \\in \\mathbb{R}^{n \\times n}, \\ T_{i,0} \\leftarrow y_i \\text{ for all } i \\\\\\\\&amp; \\textstyle \\text{for } j = 1 \\text{ to } n - 1 \\text{ do} \\\\&amp; \\textstyle \\qquad \\text{for } i = 0 \\text{ to } n - 1 - j \\text{ do} \\\\&amp; \\textstyle \\qquad\\qquad T_{i,j} \\leftarrow (T_{i+1,j-1} - T_{i,j-1}) / (x_{i+j} - x_i) \\\\&amp; \\textstyle \\qquad \\text{end} \\\\&amp; \\textstyle \\text{end} \\\\\\\\&amp; \\textstyle N \\leftarrow 0 \\\\&amp; \\textstyle p \\leftarrow 1 \\\\&amp; \\textstyle \\text{for } i = 0 \\text{ to } n - 1 \\text{ do} \\\\&amp; \\textstyle \\qquad N \\leftarrow N + T_{0,i} \\times p \\\\&amp; \\textstyle \\qquad p \\leftarrow p \\times (x - x_{i}) \\\\&amp; \\text{end} \\\\\\\\&amp; \\textstyle \\text{return } N\\end{align*}\\)위 알고리즘을 활용해 실제 코드로 구현해보자.Python CodeAlgorithm의 내용을 Python 코드로 구현한다. Numpy library를 활용하였다.import numpy as np분할차분표 Method - Pythonpoint set points을 입력하면 분할차분표를 계산 후 반환한다. Numpy의 장점을 활용하기 위해 Algorithm 파트와 같이 중첩 for문을 사용하지 않고 각 열에 대해 vectorization 기법을 활용하여 계산하였다.def divided_differences_table(points: np.ndarray): assert points.ndim == 2 and points.shape[1] == 2 x = points[:, 0] y = points[:, 1] n = points.shape[0] # Initialize divided differences table T = np.zeros((n, n)) T[:, 0] = y for j in range(1, n): l = n - j # last row of a current colummn T[:l, j] = (T[1:l+1, j-1] - T[:l, j-1]) / (x[j:j+l] - x[:l]) return T위 for문에서 지역변수 l은 현재 분할차분을 계산하려는 열에 대해 마지막 행으로 i로 생각하면 사실상 수식은 동일하다.뉴턴 다항식 보간 - Pythonnewton_poly()는 입력된 point set points에 대해 뉴턴 다항식을 반환하는 method이다.def newton_poly(points: np.ndarray): assert points.ndim == 2 and points.shape[1] == 2 T = divided_differences_table(points) coef = T[0, :] # coefficients from the first row of divided differences table x_values = points[:, 0] def interpolate(x): try: iter(x) x = np.array(x) N = np.zeros_like(x) p = np.ones_like(x) except TypeError: N = 0 p = 1 for i in range(len(coef)): N += coef[i] * p p *= x - x_values[i] return N return interpolate위에서 정의한 divided_differences_table() method를 통해 분할차분표를 획득한 뒤 뉴턴 다항식 보간에 사용할 계수를 추출한다. local function interpolate()에 coef와 x_values를 capture 시킨 뒤 뉴턴 다항식을 계산 방법을 기술한다. 이 때 interpolate()의 입력 x에도 vectorization을 지원하기 위해 Numpy의 utility function들을 활용하였다. 그 후 interpolate() local function 자체를 반환하면 입력된 point set에 대한 뉴턴 다항식 자체를 획득할 수 있다.뉴턴 다항식 보간 최종 코드 - Python위 내용을 정리한 코드이다.디버깅 - Python디버깅을 위해 우리가 잘 알고 있는 3차함수인 $y = x(x - 1)(x + 1)$를 지나는 Point들을 Point2 타입의 배열을 만들고 초기화 한다. 점이 4개이기 때문에 3차함수를 보간할 수 있다.p = np.array([[-1, 0], [0, 0], [1, 0], [2, 6]])func = newton_poly(p)print(func([-2.0, 3.0]))출력된 결과는 다음과 같다.보간하고자 했던 함수에 동일한 $x$값인 -2와 3을 대입해보면 동일한 함숫값을 얻을 수 있다. 즉, 성공적으로 다항식을 보간했다. 이제 어떤 임의의 점이든지 간에 그 점들을 지나는 유일한 다항함수를 결정할 수 있다.Matplotlib로 뉴턴 다항식 그래프 출력 - Python위 보간된 뉴턴 다항식을 그래프로도 확인해보자.import matplotlib.pyplot as pltx = np.linspace(-3.0, 3.0, 100)y = func(x)plt.plot(x, y)plt.scatter(p[:,0], p[:,1], c=\"r\", marker=\"o\")plt.show()뉴턴 다항식 그래프:보간된 뉴턴 다항식은 주어진 point set p를 지나감을 확인할 수 있다.C# CodeUnity에서 사용해 보기 위해 C#으로도 구현해보았다.사용자 정의 타입 선언 - C#먼저 2차원 상의 Point의 위치를 표현할 수 있는 Point2 struct를 정의한다.struct Vector2{ public float x; public float y; public Vector2(float x, float y) { this.x = x; this.y = y; }}다음은 입력으로 $x$값을 받고, 출력으로 함숫값을 반환하는 delegate를 선언한다. delegate 타입은 InterpolatedFunction이라고 명칭하겠다. 이는 뉴턴 다항식 보간 시 보간된 다항식을 반환하기 위한 대리자이다.// 입력: x, 출력: f(x)delegate float InterpolatedFunction(float x);뉴턴 다항식 보간 메소드 선언 - C#구현하려는 뉴턴 다항식 보간 메소드의 형식은 다음과 같다. Point2 타입의 배열을 points 매개변수에 입력받는다. points 배열은 Point의 집합이다. points 배열의 각 점을 지나는 다항식 험수를 보간 후 보간된 다항식을 반환한다.위 내용을 수학적으로 정리하면 다음과 같다. 입력: points 배열$(x_0, y_0), (x_1, y_1), \\cdots$ 출력: 보간된 뉴턴 다항식$N(x) = f[x_0] + f[x_0, x_1] (x - x_0) + f[x_0, x_1, x_2] (x - x_0)(x - x_1)+ \\cdots$즉, 위 $N(x)$ 함수를 InterpolatedFunction delegate에 할당 후 반환하려는 것이다. 아래는 뉴턴 다항식 보간 메소드이다.static InterpolatedFunction NewtonPolynomial(params Vector2[] points) { }이제 위 메소드를 구현할 시간이다.분할차분표 생성 및 초기화 - C#먼저 분할차분표를 2차원 배열로 구현한다. $n$개의 Point가 주어졌다면 $n - 1$차 다항식을 보간할 수 있으며 분할차분표의 크기는 $n$행 $n$열이다. 이 때 각 행과 열의 인덱스 범위는 $0 \\leq i, j \\leq n - 1$이다. 분할차분표에는 $x$좌표는 들어가지 않으며 오직 분할차분 값만 들어간다.int n = points.Length; // Point의 개수float[,] dividedDifferenceTable = new float[n, n]; // n행 n열 크기의 분할차분표분할차분표의 0번째 열은 $f[x_i] = f(x_i)$이므로 각 Point의 $y$좌표(또는 함수값)을 대입한다.// 0번째 열에 y좌표(함수값) 대입for (int i = 0; i &lt; n; i++){ dividedDifferenceTable[i, 0] = points[i].y;}$i$행 $j$열의 분할차분 값은 $i + 1$행 $j - 1$열의 분할차분과 $i$행 $j - 1$열의 분할차분을 통해 구한다. 즉, 분할차분표의 $i$행 $j$열의 값을 $T_{i, j}$라고 할 때 $T_{i, j} = \\displaystyle\\frac{T_{i + 1, j - 1} - T_{i, j - 1}}{x_{i + j} - x_i}$이다. 또한 $i + 1$행 $j - 1$열의 분할차분 참조로 인해 각 열의 원소 개수는 $n - j$개다. 따라서 각 열에 대해 $i$는 $0$부터 $n - 1 - j$까지 참조한다. 내용이 잘 이해되지 않는다면 분할차분표를 이용한 계산을 다시 보길 권장한다.// 분할 차분 값들을 이전 분할 차분 값으로부터 순차적으로 구함// 0번째 열은 이미 구했으므로 j = 1부터 시작for (int j = 1; j &lt; n; j++){ for (int i = 0; i &lt; n - j; i++) { dividedDifferenceTable[i, j] = (dividedDifferenceTable[i + 1, j - 1] - dividedDifferenceTable[i, j - 1]) / (points[i + j].x - points[i].x); }}뉴턴 다항식 보간 - C#분할차분표를 완성했기 때문에 이제 뉴턴 다항식을 보간할 수 있다. 분할차분표의 0번째 행이 뉴턴 다항식의 각 항의 계수이다. 메모리 효율성을 위해 전처리로 분할차분표의 0번째 행과 Points의 $x$값들을 따로 복사한다. 그 이유는 delegate나 local function 사용 시 변수를 캡쳐할 때 발생하는 몇가지 문제 때문이다. 자세한 내용은 C# 클로저 개념을 참조하면 되며 이 포스트의 주 목적은 아니기 때문에 크게 신경쓰지 않아도 된다.// 보간하려는 다항함수의 계수: 분할차분표의 0번째 행float[] coef = new float[n];for (int i = 0; i &lt; n; i++){ coef[i] = dividedDifferenceTable[0, i];}// 필요한 데이터의 x값(마지막 x값은 필요 없음)float[] x_points = new float[n - 1];for (int i = 0; i &lt; n - 1; i++){ x_points[i] = points[i].x;}뉴턴 다항식을 구하는 Interpolate() local function 내부에서 뉴턴 다항식 $N(x) = f[x_0] + f[x_0, x_1] (x - x_0) + f[x_0, x_1, x_2] (x - x_0)(x - x_1) + \\cdots$ 와 같이 각 항의 차수가 순차적으로 증가하는 형태로 다항식을 보간한다. 이 때 위에서 복사한 값을 활용해 뉴턴 다항식을 보간할 수 있다.float Interpolate(float x){ float functionValue = 0f; // 반환할 함수값 float newtonBasisPoly = 1f; // 뉴턴 기반 다항식 for (int i = 0; i &lt; coef.Length; i++) { functionValue += coef[i] * newtonBasisPoly; newtonBasisPoly *= x - x_points[i]; // 누적곱을 사용함 } return functionValue;}local function Interpolate()를 InterpolatedFunction delegate 인스턴스에 할당 후 반환한다. 이렇게 처리하는 목적은 동일한 data set에 대해 중복해서 분할차분표를 구하는 낭비를 피하기 위해서이다.return new InterpolatedFunction(Interpolate);뉴턴 다항식 보간 최종 코드 - C#위 내용을 정리한 코드이다.디버깅 - C#디버깅을 위해 우리가 잘 알고 있는 3차함수인 $y = x(x - 1)(x + 1)$를 지나는 Point들을 Point2 타입의 배열을 만들고 초기화 한다. 점이 4개이기 때문에 3차함수를 보간할 수 있다.static void Main(string[] args){ Vector2[] points = new Vector2[4] { new Vector2(-1, 0), new Vector2(0, 0), new Vector2(1, 0), new Vector2(2, 6) }; var f = NewtonPolynomial(points); // 뉴턴 다항식 보간 메소드 호출 // 결과: (-2, -6), (3, 24) Console.WriteLine($\"({-2}, {f(-2)}), ({3}, {f(3)})\");}출력된 결과는 다음과 같다.\\[(-2, -6), (3, 24)\\]보간하고자 했던 함수에 동일한 $x$값인 -2와 3을 대입해보면 동일한 함숫값을 얻을 수 있다. 즉, 성공적으로 다항식을 보간했다. 이제 어떤 임의의 점이든지 간에 그 점들을 지나는 유일한 다항함수를 결정할 수 있다.Unity에서 구현한 비선형 오브젝트(함정) - C#Unity Object를 주어진 데이터에 의해 보간된 비선형 함수의 궤적을 따라 움직이도록 구현하였다.인스펙터 설정데이터 입력과 보간, 이동 기능은 아래 Non Linear Movable Object 컴포넌트에서 처리했다. 4개의 점을 입력해 각 점을 지나는 3차함수를 보간한다. 5개의 점을 입력해 각 점을 지나는 4차함수를 보간한다. 실제 이동 장면아래는 각각 3차함수와 4차함수의 궤적으로 움직이는 함정을 보여주는 장면이다.직선 방향으로만 움직이는 함정보다 위와 같이 곡선 형태의 움직임을 취하면 좀 더 다양성을 줄 수 있고 게임 난이도도 올릴 수 있다.References[1] Wikipedia. Newton polynomial.[2] INU. Jibum Kim. 수치해석 lecture." } ]
